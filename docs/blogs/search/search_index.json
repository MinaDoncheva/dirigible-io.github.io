{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Eclipse Dirigible \u2122 Blogs Hi, friends! Here, we share our vision, opinions, and experience on cloud development and, in particular, Eclipse Dirigible. We hope you'll enjoy reading these blogs. Add more content whenever you feel like it! Here's a link to our contributor guide for your reference.","title":"Blogs"},{"location":"#welcome-to-eclipse-dirigible-blogs","text":"Hi, friends! Here, we share our vision, opinions, and experience on cloud development and, in particular, Eclipse Dirigible. We hope you'll enjoy reading these blogs. Add more content whenever you feel like it! Here's a link to our contributor guide for your reference.","title":"Welcome to Eclipse Dirigible&trade; Blogs"},{"location":"2014/03/21/blogs-start-of-journey/","text":"The initial idea was to make use of the very promising Eclipse RAP technology https://eclipse.org/rap/ in our development, related to Service Adaptation (part of SOA tools). The question was: - Can we build Web-based tooling for Web Services simplification (i.e. enterprise services in SAP language)? Started as a POC and evolved to a TGiF project, the Dirigible first and foremost impressed us ourselves. Full Article Here: Start of Journey","title":"Start of Journey"},{"location":"2014/03/24/blogs-dirigible-on-sap-hana/","text":"Have you ever wondered if it is possible to develop end to end your next cloud application directly via the browser? What language should you use? What kind of other tools should you need for the database management, authorization definitions, testing, life-cycle management, monitoring\u2026? What if you can: write the whole application in JavaScript? You like Ruby? What about Groovy as well? And still in the same environment? run directly Apache Camel's Routes as simplified integration services and scheduled jobs? browse your database schema or catalog and execute your query and update scripts again in the same environment? Full Article Here: Dirigible on SAP HANA Cloud Platform","title":"Dirigible on SAP HANA Cloud Platform"},{"location":"2014/06/11/blogs-dirigible-extensions-configs/","text":"If somebody asks you just right now \" What do you require most from your business software in general? \", what will pop up in your mind first? To be ...reliable, secure, feature rich? ... Cheaper? \u2013 Noo!... What about being configurable, flexible, or extendable? Well, it is clear that for business applications, in comparison to other types of software bundles like HTTP Servers, Databases, Office Packages, etc., it is vital to be adaptable to the particular business processes within a given company. Full Article Here: Dirigible - Extensions vs Configurations","title":"Dirigible - Extensions vs Configurations"},{"location":"2014/08/04/blogs-dirigible-fast-track-to-hcp/","text":"Just recently I was on site with a customer with the task to develop an HCP extension to their Successfactors subscription. The timelines were very tight and the team had zero experience with HCP and its related toolset and entry points. One option was to block some time to learn how to make use of HCP cockpit and tools and essentially halt the development during this period. On the other hand there is Dirigible http://www.dirigible.io/ . Full Article Here: Dirigible is the fast track to your HCP HANA DB instance","title":"Dirigible is the fast track to your HCP HANA DB instance"},{"location":"2014/08/19/blogs-dirigible-to-replicate-or-not-to-replicate/","text":"The existential question, which only seems to offer two equal and yet feasible options. The first time we face such a question in a distributed environment (typical in the context of cloud applications) we\u2019ll have a hard time finding the \u201cright\u201d answer \u2026hmm the second time as well? The actual problem is that we cannot give a universal solution just by definition, but have to go through case by case each time. Such scenarios could be transferring data between two legacy or 3-thd party systems, between a legacy system and our custom extension, even between the different extension applications that we could have. There could be different boundary conditions depending mainly on the source and target systems\u2019 capabilities for external communication. Sometimes, additional intermediate component could be required to cope with the integration between the systems. Hence, we need to deeply investigate the pros & cons for the given scenario, personas and components and to take concrete conscious decisions for the architectural patterns, which can be used. We have to consider also aspects like the performance for direct synchronous calls between remote systems, scalability of the source system itself, lack of tailored interfaces for our brand new use-cases, preserving the security model e.g. identity propagation, need of preliminary cache and other non-functional requirements, we often reach to the well-known situation (after taking the red pill) when \u2013 \u201cthe choice has already been made, and now you have to understand it\u201d. Full Article Here: Dirigible - To Replicate or Not To Replicate","title":"Dirigible - To Replicate or Not To Replicate"},{"location":"2014/10/15/blogs-dirigible-terminal-services/","text":"In response of the great interest related to Shellshock , Dirigible provided several possibilities to use low level OS commands. After going to the well known trial instance at http://trial.dirigible.io from the menu, go to Window->Show View->Other open the Other tree-node and choose Terminal. Full Article Here: Dirigible - Terminal Services","title":"Dirigible - Terminal Services"},{"location":"2014/10/22/blogs-dirigible-java-runtime/","text":"In attempt to attract the 9 million happy Java developers to the Dirigible community, we have introduced Java as runtime language. Yes, finally we can run our Java code in the cloud the way we like it, with Dirigible. To explore the new \" Java Services \" features, we\u2019ll create a simple \"gamification\" project. It will contain only one service and a couple of helper classes. We\u2019ll take advantage of Java by using some of the most popular classes and capabilities in the language, such as Collections Comparable , and of course Generics . The service will be responsible for calculating and distributing the results of the gamification. We\u2019ll deliver the scores in two manners \u2013 displaying them to the user, or sending them via e-mail. Full Article Here: Dirigible enables dynamic in-memory Java runtime","title":"Dirigible enables dynamic in-memory Java runtime"},{"location":"2015/09/24/blogs-dirigible-tools-for-vertical-scenarios/","text":"What does vertical scenario mean? Why building applications covering such scenarios need special toolkit and why all these relates to Dirigible? Starting the work on your next cloud application, you have to set the key concepts, architecture, boundary conditions, dependencies, development model, language, tools and many more aspects, which can lead you up to either fast delivery or fail-fast situation. If you do not ask your-self questions about these aspects every time starting a project, in these turbulent times you will not be agile enough, hence not innovative enough, hence - dull coward. Assuming that - you are here and read these article, this means you are agile, innovative, devoted to be perfect each time, so let's continue - what do you need to start and where from? Let's distinguish four major application categories in the cloud applications domain: user interface only user interface and services user interface, services and data user interface, services, data and integration User Interface Only In this category fall the static sites, reports, analytical dash-boards and all the client side only applications, which rely on existing already available services. User Interface and Services These kind of applications require some server-side logic. Mostly transformation, simplification and adaptation use-cases, where there is already an existing service which provide some data in a given scope and format, but the user interface should use only part of these data in different format. User Interface, Services and Data Here fall the set of well known atomic applications consisting of the standard building blocks - data model and its persistence, business logic in web services and user interface on top of these services. User Interface, Services, Data and Integrations Following the natural process of application evolution, sooner or later the applications from the above category start requiring integrations with some 3td party services, regular replication of data from external sources, scheduled or real-time synchronizations of events, etc. Progress Gravity Based on the above categorization, the question is: in which category does my application fall? Depending on the use-case that you have to cover and the level of understanding of this use-case, you can choose one of four. Are you ready? Good! Which one did you choose? Next question is what happens after the initial demo with some mocked data, hard-coded configurations and dummy business logic? You have to add some real services with real end-points? You have to create a real persistence, hopefully reliable, eventually consistent and as for the cloud - scalable? Step by step shaping the product adding more and more features mostly based on the key requirements, not just nice to have, you will recognize that your tiny, simple and sweet show-case application became full-fledged yet powerful beast from the last category. How did this happen?! We call this - \"progress gravity\". No matter how do you start at the beginning, maturity process pulls your application down through the category stack and finally it ends up at the last one. End-to-End Coverage with Tools Now you have an idea what we do mean with the term \"vertical scenario\", right? Once you have a real problem you have to solve it completely - partial doesn't count. Who can help you in this situation - usually you need a single instrument for every single task in the chain. What if, all the instruments are packed in one toolkit? This would be perfect, right? You do not need to jump around every time you need to change the instrument. This very simple and naive explanation shows how do we look at the problem and how do we strive to solve it - to provide you with a full-fledged toolkit, where you can find all the single \"instruments\" you need to build and operate your application for a vertical scenario - completely . Feedback and Requests If you miss an \"instrument\" in the current version of Dirigible, you can always request such via: http://forum.dirigible.io","title":"Dirigible - Toolkit for Vertical Scenarios"},{"location":"2015/09/24/blogs-dirigible-tools-for-vertical-scenarios/#user-interface-only","text":"In this category fall the static sites, reports, analytical dash-boards and all the client side only applications, which rely on existing already available services.","title":"User Interface Only"},{"location":"2015/09/24/blogs-dirigible-tools-for-vertical-scenarios/#user-interface-and-services","text":"These kind of applications require some server-side logic. Mostly transformation, simplification and adaptation use-cases, where there is already an existing service which provide some data in a given scope and format, but the user interface should use only part of these data in different format.","title":"User Interface and Services"},{"location":"2015/09/24/blogs-dirigible-tools-for-vertical-scenarios/#user-interface-services-and-data","text":"Here fall the set of well known atomic applications consisting of the standard building blocks - data model and its persistence, business logic in web services and user interface on top of these services.","title":"User Interface, Services and Data"},{"location":"2015/09/24/blogs-dirigible-tools-for-vertical-scenarios/#user-interface-services-data-and-integrations","text":"Following the natural process of application evolution, sooner or later the applications from the above category start requiring integrations with some 3td party services, regular replication of data from external sources, scheduled or real-time synchronizations of events, etc.","title":"User Interface, Services, Data and Integrations"},{"location":"2015/09/24/blogs-dirigible-tools-for-vertical-scenarios/#progress-gravity","text":"Based on the above categorization, the question is: in which category does my application fall? Depending on the use-case that you have to cover and the level of understanding of this use-case, you can choose one of four. Are you ready? Good! Which one did you choose? Next question is what happens after the initial demo with some mocked data, hard-coded configurations and dummy business logic? You have to add some real services with real end-points? You have to create a real persistence, hopefully reliable, eventually consistent and as for the cloud - scalable? Step by step shaping the product adding more and more features mostly based on the key requirements, not just nice to have, you will recognize that your tiny, simple and sweet show-case application became full-fledged yet powerful beast from the last category. How did this happen?! We call this - \"progress gravity\". No matter how do you start at the beginning, maturity process pulls your application down through the category stack and finally it ends up at the last one.","title":"Progress Gravity"},{"location":"2015/09/24/blogs-dirigible-tools-for-vertical-scenarios/#end-to-end-coverage-with-tools","text":"Now you have an idea what we do mean with the term \"vertical scenario\", right? Once you have a real problem you have to solve it completely - partial doesn't count. Who can help you in this situation - usually you need a single instrument for every single task in the chain. What if, all the instruments are packed in one toolkit? This would be perfect, right? You do not need to jump around every time you need to change the instrument. This very simple and naive explanation shows how do we look at the problem and how do we strive to solve it - to provide you with a full-fledged toolkit, where you can find all the single \"instruments\" you need to build and operate your application for a vertical scenario - completely .","title":"End-to-End Coverage with Tools"},{"location":"2015/09/24/blogs-dirigible-tools-for-vertical-scenarios/#feedback-and-requests","text":"If you miss an \"instrument\" in the current version of Dirigible, you can always request such via: http://forum.dirigible.io","title":"Feedback and Requests"},{"location":"2015/10/21/blogs-dirigible-impl-sql-plugin/","text":"How to implement a custom plugin for Dirigible, which brings custom execution engine for a custom development language? Hmmm ... why at all you would need this? In general you do not. You can rely on the standard JavaScript, supported by default as a primary language for services in Dirigible. If you prefer Java you are still in the supported default options. But, what if you want to use your preferred language like Ruby, Groovy, Pyton, Scala and many other JVM and non-JVM modern languages? The good news - it's not so difficult. In this tutorial we will give you the major steps and directions, which can guide you throughout such integrations. Let's take something simple e.g. SQL script. The engine, which executes this language is the underlying RDBMS itself, but from Dirigible's point of view the database is abstracted via JDBC interface. So, let's create a new feature - 'SQL Services' support. It will provide the following: Editor for SQL script with highlighting of the keywords Icon in Workspace and Repository views showing that *.sql files are recognized Activator/Publisher, which will take care of the transfer of the SQL artifacts to Sandbox/Registry Runtime dispatcher, which will provide the endpoint for access for these services Runtime executor, which will take the artifact and will do the processing Infrastructure - pom.xml, config.ini, feature.xml User interface for endpoints in Registry Sample template for SQL Service If you are Eclipse RCP/RAP and OSGi developer, you can skip this blog and go directly by cloning the sources and looking for the Java and JavaScript plugins as example. Let's start... Editor for SQL Luckily we support two web editors in Dirigible - Orion and ACE. The later has good support for SQL Language, hence we can use it directly. Be sure that you enable the support of your language in the corresponding editor by adding the file extension to the editor's 'extensions' parameter in the plugin.xml. In this case in plugin org.eclipse.dirigible.ide.editor.ace , extension point org.eclipse.ui.editors , class org.eclipse.dirigible.ide.editor.ace.AceEditor . Icon for *.sql files Add an icon in the resources folder of the org.eclipse.dirigible.ide.repository.ui plugin, e.g. icon-sql.png . Add a reference of the icon and the necessary file extension in org.eclipse.dirigible.ide.repository.ui.viewer.AbstractArtifactLabelProvider similar like the other cases. Publisher adaptation There are a few adaptation that can enable *.sql artifact to be considered as supported scripting services. To do that: Add the corresponding constant for SQL extension in ARTIFACT_EXTENSION in the class org.eclipse.dirigible.repository.api.ICommonConstants e.g. public static final String SQL = \".sql\"; Add SQL_CONTAINER_MAPPING and SQL_SANDBOX_MAPPING in class org.eclipse.dirigible.ide.common.CommonParameters in similar way like the others. Add the corresponding artifact extension and mappings in the static list and maps in class org.eclipse.dirigible.ide.scripts.publish.ScriptsPublisher in similar way like the others. We are done at the IDE side! Now we go to the Runtime to implement the execution engine. Engine for SQL Create a new plugin which will contain all the execution engine related artifacts for the SQL support. As a template you can use already available for Java org.eclipse.dirigible.runtime.java - e.g. org.eclipse.dirigible.runtime.sql Add corresponding ENGINE_TYPE public static final String SQL = \"sql\"; in org.eclipse.dirigible.repository.api.ICommonConstants Do the same for ENGINE_ALIAS Modify/check: .project file MANIFEST.MF file names, dependencies, exported packages In OSGi-INF folder create a sql-executor.xml with corresponding references plugin.xml fill the corresponding servlets and filters pom.xml Add the module definition in the parent's pom.xml, e.g. < module>org.eclipse.dirigible.runtime.sql< /module> In the source folder ( src ), you should finally have at least: org.eclipse.dirigible.runtime.filter.SQLRegistrySecureFilter.java package org.eclipse.dirigible.runtime.filter ; public class SQLRegistrySecureFilter extends AbstractRegistrySecureFilter { private static final String SQL_SECURED_MAPPING = \"/services/sql-secured\" ; //$NON-NLS-1$ @Override protected String getSecuredMapping () { return SQL_SECURED_MAPPING ; } } org.eclipse.dirigible.runtime.registry.SQLRegistryServlet.java package org.eclipse.dirigible.runtime.registry ; public class SQLRegistryServlet extends AbstractRegistryServiceServlet { private static final long serialVersionUID = - 7292896045277229573L ; @Override protected String getServletMapping () { return \"/sql/\" ; } @Override protected String getFileExtension () { return \".sql\" ; } @Override protected String getRequestProcessingFailedMessage () { return Messages . getString ( \"JavascriptRegistryServlet.REQUEST_PROCESSING_FAILED_S\" ); } } org.eclipse.dirigible.runtime.sql.SQLExecutor.java package org.eclipse.dirigible.runtime.sql ; import java.io.IOException ; import java.sql.Connection ; import java.sql.PreparedStatement ; import java.sql.ResultSet ; import java.sql.ResultSetMetaData ; import java.util.ArrayList ; import java.util.HashMap ; import java.util.List ; import java.util.Map ; import javax.servlet.http.HttpServletRequest ; import javax.servlet.http.HttpServletResponse ; import javax.sql.DataSource ; import org.eclipse.dirigible.repository.api.ICommonConstants ; import org.eclipse.dirigible.repository.api.IRepository ; import org.eclipse.dirigible.repository.logging.Logger ; import org.eclipse.dirigible.runtime.repository.RepositoryFacade ; import org.eclipse.dirigible.runtime.scripting.AbstractScriptExecutor ; import com.google.gson.Gson ; import com.google.gson.JsonArray ; import com.google.gson.JsonObject ; import com.google.gson.JsonPrimitive ; public class SQLExecutor extends AbstractScriptExecutor { private static final String SQL_MODULE_NAME_CANNOT_BE_NULL = \"SQL module name cannot be null.\" ; private static final Logger logger = Logger . getLogger ( SQLExecutor . class ); private IRepository repository ; private String [] rootPaths ; private Map < String , Object > defaultVariables ; private String classpath ; public SQLExecutor ( IRepository repository , String ... rootPaths ) { this . repository = repository ; this . rootPaths = rootPaths ; this . defaultVariables = new HashMap < String , Object > (); this . classpath = classpath ; } @Override public Object executeServiceModule ( HttpServletRequest request , HttpServletResponse response , Object input , String module , Map < Object , Object > executionContext ) throws IOException { String result = null ; try { logger . debug ( \"entering: executeServiceModule()\" ); //$NON-NLS-1$ logger . debug ( \"module=\" + module ); //$NON-NLS-1$ if ( module == null ) { throw new IOException ( SQL_MODULE_NAME_CANNOT_BE_NULL ); } String sqlSource = new String ( retrieveModule ( repository , module , \"\" , rootPaths ). getContent ()); DataSource dataSource = RepositoryFacade . getInstance (). getDataSource (); Connection connection = null ; try { connection = dataSource . getConnection (); PreparedStatement pstmt = connection . prepareStatement ( sqlSource ); ResultSet rs = pstmt . executeQuery (); // get column names ResultSetMetaData rsmd = rs . getMetaData (); int columnCnt = rsmd . getColumnCount (); List < String > columnNames = new ArrayList < String > (); for ( int i = 1 ; i <= columnCnt ; i ++ ) { columnNames . add ( rsmd . getColumnName ( i ). toUpperCase ()); } JsonArray array = new JsonArray (); while ( rs . next ()) { JsonObject obj = new JsonObject (); for ( int i = 1 ; i <= columnCnt ; i ++ ) { String key = columnNames . get ( i - 1 ); String value = rs . getString ( i ); obj . add ( key , new JsonPrimitive ( value != null ? value : \"\" )); } array . add ( obj ); } result = new Gson (). toJson ( array ); rs . close (); pstmt . close (); } finally { if ( connection != null ) { connection . close (); } } } catch ( Exception e ) { logger . error ( e . getMessage (), e ); throw new IOException ( e ); } return result ; } @Override protected void registerDefaultVariable ( Object scope , String name , Object value ) { defaultVariables . put ( name , value ); } @Override protected String getModuleType ( String path ) { return ICommonConstants . ARTIFACT_TYPE . SCRIPTING_SERVICES ; } } org.eclipse.dirigible.runtime.sql.SQLServlet.java org.eclipse.dirigible.runtime.sql.SQLSandboxServlet.java org.eclipse.dirigible.runtime.sql.SQLSecuredServlet.java org.eclipse.dirigible.runtime.sql.SQLScriptExecutorProvider.java Include the Plugin as a Feature There is a feature for the runtime plugins in the project p2.runtime.feature Add the SQL plugin to the feature.xml accordingly Include the Plugin for Packaging You have to include just created plugin into the configuration files for Equinox OSGi: In the project releng/dirigible-all-tomcat > sub-folder src/main/webapp/WEB-INF/configuration > file config.ini In the project releng/dirigible-runtime-tomcat > sub-folder src/main/webapp/WEB-INF/configuration > file config.ini be very careful with the white spaces in the beginning of the line Security Constrains in web.xml In the project releng/dirigible-all-tomcat > sub-folder src/web/ > all files web.xml excluding trial In the project releng/dirigible-runtime-tomcat > sub-folder src/web/ > all files web.xml excluding trial Flows and Jobs Integration Luckily we have already implemented the extensibility in a way that SQLScriptExecutorProvider from above is automatically registered and can be used in Flows. Registry Section for SQL Services The plugin containing the registry user interface is org.eclipse.dirigible.runtime.ui Create a file sql.html in the sub-folder resources/ui/templates/scripting/sql < div id = \"content\" ng-include = \"'templates/default.html'\" ></ div > Adapt $routeProvider in the app.js file by adding routing for sql pages In the same file add a corresponding section in $scope.homeData Add the controller itself in default.js defaultControllers.controller('SQLCtrl', function($scope, $resource) { $scope.restService = $resource('../scripting/sql'); }); Do not forget to add some image file also Add a manu item in the main menu - menu.json Template for SQL Scripting Service To complete the SQL support we can add at least one template to be available in the New->ScriptinService wizard. To do that, in the plugin org.eclipse.dirigible.ide.template.ui.js Create file sql-service.sql under the folder src/org/eclipse/dirigible/ide/template/ui/js/templates SELECT * FROM DGB_FILES Add the definition in the plugin.xml accordingly <template category= \"ScriptingServices\" image= \"/icons/sql-service.png\" location= \"/org/eclipse/dirigible/ide/template/ui/js/templates/sql-service.sql\" text= \"SQL Sample Query Service\" > </template> Do not forget the icon as well Add the default extension file recognition in org.eclipse.dirigible.ide.template.ui.js.wizard.JavascriptServiceTemplateTargetLocationPage ... } else if ( \"/org/eclipse/dirigible/ide/template/ui/js/templates/sql-service.sql\" //$NON-NLS-1$ . equals ( model . getTemplate (). getLocation ())) { jsOrLibExt = \"sql\" ; //$NON-NLS-1$ } ... Congratulations! If you managed to follow all this - you are a hero! The git commit for reference is here","title":"Tutorial - How to implement a plugin for SQL language support"},{"location":"2015/10/21/blogs-dirigible-impl-sql-plugin/#editor-for-sql","text":"Luckily we support two web editors in Dirigible - Orion and ACE. The later has good support for SQL Language, hence we can use it directly. Be sure that you enable the support of your language in the corresponding editor by adding the file extension to the editor's 'extensions' parameter in the plugin.xml. In this case in plugin org.eclipse.dirigible.ide.editor.ace , extension point org.eclipse.ui.editors , class org.eclipse.dirigible.ide.editor.ace.AceEditor .","title":"Editor for SQL"},{"location":"2015/10/21/blogs-dirigible-impl-sql-plugin/#icon-for-sql-files","text":"Add an icon in the resources folder of the org.eclipse.dirigible.ide.repository.ui plugin, e.g. icon-sql.png . Add a reference of the icon and the necessary file extension in org.eclipse.dirigible.ide.repository.ui.viewer.AbstractArtifactLabelProvider similar like the other cases.","title":"Icon for *.sql files"},{"location":"2015/10/21/blogs-dirigible-impl-sql-plugin/#publisher-adaptation","text":"There are a few adaptation that can enable *.sql artifact to be considered as supported scripting services. To do that: Add the corresponding constant for SQL extension in ARTIFACT_EXTENSION in the class org.eclipse.dirigible.repository.api.ICommonConstants e.g. public static final String SQL = \".sql\"; Add SQL_CONTAINER_MAPPING and SQL_SANDBOX_MAPPING in class org.eclipse.dirigible.ide.common.CommonParameters in similar way like the others. Add the corresponding artifact extension and mappings in the static list and maps in class org.eclipse.dirigible.ide.scripts.publish.ScriptsPublisher in similar way like the others. We are done at the IDE side! Now we go to the Runtime to implement the execution engine.","title":"Publisher adaptation"},{"location":"2015/10/21/blogs-dirigible-impl-sql-plugin/#engine-for-sql","text":"Create a new plugin which will contain all the execution engine related artifacts for the SQL support. As a template you can use already available for Java org.eclipse.dirigible.runtime.java - e.g. org.eclipse.dirigible.runtime.sql Add corresponding ENGINE_TYPE public static final String SQL = \"sql\"; in org.eclipse.dirigible.repository.api.ICommonConstants Do the same for ENGINE_ALIAS Modify/check: .project file MANIFEST.MF file names, dependencies, exported packages In OSGi-INF folder create a sql-executor.xml with corresponding references plugin.xml fill the corresponding servlets and filters pom.xml Add the module definition in the parent's pom.xml, e.g. < module>org.eclipse.dirigible.runtime.sql< /module> In the source folder ( src ), you should finally have at least: org.eclipse.dirigible.runtime.filter.SQLRegistrySecureFilter.java package org.eclipse.dirigible.runtime.filter ; public class SQLRegistrySecureFilter extends AbstractRegistrySecureFilter { private static final String SQL_SECURED_MAPPING = \"/services/sql-secured\" ; //$NON-NLS-1$ @Override protected String getSecuredMapping () { return SQL_SECURED_MAPPING ; } } org.eclipse.dirigible.runtime.registry.SQLRegistryServlet.java package org.eclipse.dirigible.runtime.registry ; public class SQLRegistryServlet extends AbstractRegistryServiceServlet { private static final long serialVersionUID = - 7292896045277229573L ; @Override protected String getServletMapping () { return \"/sql/\" ; } @Override protected String getFileExtension () { return \".sql\" ; } @Override protected String getRequestProcessingFailedMessage () { return Messages . getString ( \"JavascriptRegistryServlet.REQUEST_PROCESSING_FAILED_S\" ); } } org.eclipse.dirigible.runtime.sql.SQLExecutor.java package org.eclipse.dirigible.runtime.sql ; import java.io.IOException ; import java.sql.Connection ; import java.sql.PreparedStatement ; import java.sql.ResultSet ; import java.sql.ResultSetMetaData ; import java.util.ArrayList ; import java.util.HashMap ; import java.util.List ; import java.util.Map ; import javax.servlet.http.HttpServletRequest ; import javax.servlet.http.HttpServletResponse ; import javax.sql.DataSource ; import org.eclipse.dirigible.repository.api.ICommonConstants ; import org.eclipse.dirigible.repository.api.IRepository ; import org.eclipse.dirigible.repository.logging.Logger ; import org.eclipse.dirigible.runtime.repository.RepositoryFacade ; import org.eclipse.dirigible.runtime.scripting.AbstractScriptExecutor ; import com.google.gson.Gson ; import com.google.gson.JsonArray ; import com.google.gson.JsonObject ; import com.google.gson.JsonPrimitive ; public class SQLExecutor extends AbstractScriptExecutor { private static final String SQL_MODULE_NAME_CANNOT_BE_NULL = \"SQL module name cannot be null.\" ; private static final Logger logger = Logger . getLogger ( SQLExecutor . class ); private IRepository repository ; private String [] rootPaths ; private Map < String , Object > defaultVariables ; private String classpath ; public SQLExecutor ( IRepository repository , String ... rootPaths ) { this . repository = repository ; this . rootPaths = rootPaths ; this . defaultVariables = new HashMap < String , Object > (); this . classpath = classpath ; } @Override public Object executeServiceModule ( HttpServletRequest request , HttpServletResponse response , Object input , String module , Map < Object , Object > executionContext ) throws IOException { String result = null ; try { logger . debug ( \"entering: executeServiceModule()\" ); //$NON-NLS-1$ logger . debug ( \"module=\" + module ); //$NON-NLS-1$ if ( module == null ) { throw new IOException ( SQL_MODULE_NAME_CANNOT_BE_NULL ); } String sqlSource = new String ( retrieveModule ( repository , module , \"\" , rootPaths ). getContent ()); DataSource dataSource = RepositoryFacade . getInstance (). getDataSource (); Connection connection = null ; try { connection = dataSource . getConnection (); PreparedStatement pstmt = connection . prepareStatement ( sqlSource ); ResultSet rs = pstmt . executeQuery (); // get column names ResultSetMetaData rsmd = rs . getMetaData (); int columnCnt = rsmd . getColumnCount (); List < String > columnNames = new ArrayList < String > (); for ( int i = 1 ; i <= columnCnt ; i ++ ) { columnNames . add ( rsmd . getColumnName ( i ). toUpperCase ()); } JsonArray array = new JsonArray (); while ( rs . next ()) { JsonObject obj = new JsonObject (); for ( int i = 1 ; i <= columnCnt ; i ++ ) { String key = columnNames . get ( i - 1 ); String value = rs . getString ( i ); obj . add ( key , new JsonPrimitive ( value != null ? value : \"\" )); } array . add ( obj ); } result = new Gson (). toJson ( array ); rs . close (); pstmt . close (); } finally { if ( connection != null ) { connection . close (); } } } catch ( Exception e ) { logger . error ( e . getMessage (), e ); throw new IOException ( e ); } return result ; } @Override protected void registerDefaultVariable ( Object scope , String name , Object value ) { defaultVariables . put ( name , value ); } @Override protected String getModuleType ( String path ) { return ICommonConstants . ARTIFACT_TYPE . SCRIPTING_SERVICES ; } } org.eclipse.dirigible.runtime.sql.SQLServlet.java org.eclipse.dirigible.runtime.sql.SQLSandboxServlet.java org.eclipse.dirigible.runtime.sql.SQLSecuredServlet.java org.eclipse.dirigible.runtime.sql.SQLScriptExecutorProvider.java","title":"Engine for SQL"},{"location":"2015/10/21/blogs-dirigible-impl-sql-plugin/#include-the-plugin-as-a-feature","text":"There is a feature for the runtime plugins in the project p2.runtime.feature Add the SQL plugin to the feature.xml accordingly","title":"Include the Plugin as a Feature"},{"location":"2015/10/21/blogs-dirigible-impl-sql-plugin/#include-the-plugin-for-packaging","text":"You have to include just created plugin into the configuration files for Equinox OSGi: In the project releng/dirigible-all-tomcat > sub-folder src/main/webapp/WEB-INF/configuration > file config.ini In the project releng/dirigible-runtime-tomcat > sub-folder src/main/webapp/WEB-INF/configuration > file config.ini be very careful with the white spaces in the beginning of the line","title":"Include the Plugin for Packaging"},{"location":"2015/10/21/blogs-dirigible-impl-sql-plugin/#security-constrains-in-webxml","text":"In the project releng/dirigible-all-tomcat > sub-folder src/web/ > all files web.xml excluding trial In the project releng/dirigible-runtime-tomcat > sub-folder src/web/ > all files web.xml excluding trial","title":"Security Constrains in web.xml"},{"location":"2015/10/21/blogs-dirigible-impl-sql-plugin/#flows-and-jobs-integration","text":"Luckily we have already implemented the extensibility in a way that SQLScriptExecutorProvider from above is automatically registered and can be used in Flows.","title":"Flows and Jobs Integration"},{"location":"2015/10/21/blogs-dirigible-impl-sql-plugin/#registry-section-for-sql-services","text":"The plugin containing the registry user interface is org.eclipse.dirigible.runtime.ui Create a file sql.html in the sub-folder resources/ui/templates/scripting/sql < div id = \"content\" ng-include = \"'templates/default.html'\" ></ div > Adapt $routeProvider in the app.js file by adding routing for sql pages In the same file add a corresponding section in $scope.homeData Add the controller itself in default.js defaultControllers.controller('SQLCtrl', function($scope, $resource) { $scope.restService = $resource('../scripting/sql'); }); Do not forget to add some image file also Add a manu item in the main menu - menu.json","title":"Registry Section for SQL Services"},{"location":"2015/10/21/blogs-dirigible-impl-sql-plugin/#template-for-sql-scripting-service","text":"To complete the SQL support we can add at least one template to be available in the New->ScriptinService wizard. To do that, in the plugin org.eclipse.dirigible.ide.template.ui.js Create file sql-service.sql under the folder src/org/eclipse/dirigible/ide/template/ui/js/templates SELECT * FROM DGB_FILES Add the definition in the plugin.xml accordingly <template category= \"ScriptingServices\" image= \"/icons/sql-service.png\" location= \"/org/eclipse/dirigible/ide/template/ui/js/templates/sql-service.sql\" text= \"SQL Sample Query Service\" > </template> Do not forget the icon as well Add the default extension file recognition in org.eclipse.dirigible.ide.template.ui.js.wizard.JavascriptServiceTemplateTargetLocationPage ... } else if ( \"/org/eclipse/dirigible/ide/template/ui/js/templates/sql-service.sql\" //$NON-NLS-1$ . equals ( model . getTemplate (). getLocation ())) { jsOrLibExt = \"sql\" ; //$NON-NLS-1$ } ... Congratulations! If you managed to follow all this - you are a hero! The git commit for reference is here","title":"Template for SQL Scripting Service"},{"location":"2015/10/28/blogs-dirigible-branding/","text":"Being a cloud platform provider or development tools provider company, most probably you would like to have your own logo and a name following your products naming convention instead of Dirigible's ones. It is very easy following the Eclipse RAP Branding approach. Create a plugin for your theme You can use the existing plugin org.eclipse.rap.design.example as a template: https://github.com/eclipse/rap/tree/master/examples/org.eclipse.rap.design.example . Create an entry point Let's assume that we just use the existing example plugin with the existing sample theme with id org.eclipse.rap.design.example.business.branding or org.eclipse.rap.design.example.fancy.branding . The new entrypoint declaration in the plugin.xml in the project org.eclipse.dirigible.ide.ui.rap should look like: ... <extension point= \"org.eclipse.rap.ui.entrypoint\" > <entrypoint brandingId= \"org.eclipse.rap.design.example.business.branding\" class= \"org.eclipse.dirigible.ide.ui.rap.entry.DirigibleWorkbench\" id= \"org.eclipse.dirigible.ide.ui.rap.entry.DefaultEntrypoint\" path= \"/business\" > </entrypoint> </extension> ... Add the branding plugin to parent's pom.xml Do not forget to add the branding plugin as a module definition in the parent's pom.xml Include the branding plugin as a feature There is a feature for the ide plugins in the project p2.ide.feature Add your branding plugin to the feature.xml accordingly Include the branding plugin for packaging You have to include branding plugin into the configuration files for Equinox OSGi: In the project releng/dirigible-all-tomcat > sub-folder src/main/webapp/WEB-INF/configuration > file config.ini be very careful with the white spaces in the beginning of the line Below you can get an impression what is achievable. Business Theme - /business Fancy Theme - /fancy Enjoy Branding!","title":"Tutorial - How to re-brand Dirigible workbench"},{"location":"2015/10/28/blogs-dirigible-branding/#create-a-plugin-for-your-theme","text":"You can use the existing plugin org.eclipse.rap.design.example as a template: https://github.com/eclipse/rap/tree/master/examples/org.eclipse.rap.design.example .","title":"Create a plugin for your theme"},{"location":"2015/10/28/blogs-dirigible-branding/#create-an-entry-point","text":"Let's assume that we just use the existing example plugin with the existing sample theme with id org.eclipse.rap.design.example.business.branding or org.eclipse.rap.design.example.fancy.branding . The new entrypoint declaration in the plugin.xml in the project org.eclipse.dirigible.ide.ui.rap should look like: ... <extension point= \"org.eclipse.rap.ui.entrypoint\" > <entrypoint brandingId= \"org.eclipse.rap.design.example.business.branding\" class= \"org.eclipse.dirigible.ide.ui.rap.entry.DirigibleWorkbench\" id= \"org.eclipse.dirigible.ide.ui.rap.entry.DefaultEntrypoint\" path= \"/business\" > </entrypoint> </extension> ...","title":"Create an entry point"},{"location":"2015/10/28/blogs-dirigible-branding/#add-the-branding-plugin-to-parents-pomxml","text":"Do not forget to add the branding plugin as a module definition in the parent's pom.xml","title":"Add the branding plugin to parent's pom.xml"},{"location":"2015/10/28/blogs-dirigible-branding/#include-the-branding-plugin-as-a-feature","text":"There is a feature for the ide plugins in the project p2.ide.feature Add your branding plugin to the feature.xml accordingly","title":"Include the branding plugin as a feature"},{"location":"2015/10/28/blogs-dirigible-branding/#include-the-branding-plugin-for-packaging","text":"You have to include branding plugin into the configuration files for Equinox OSGi: In the project releng/dirigible-all-tomcat > sub-folder src/main/webapp/WEB-INF/configuration > file config.ini be very careful with the white spaces in the beginning of the line Below you can get an impression what is achievable.","title":"Include the branding plugin for packaging"},{"location":"2015/10/28/blogs-dirigible-branding/#business-theme-business","text":"","title":"Business Theme - /business"},{"location":"2015/10/28/blogs-dirigible-branding/#fancy-theme-fancy","text":"Enjoy Branding!","title":"Fancy Theme - /fancy"},{"location":"2015/10/28/blogs-dirigible-orion-editor/","text":"Why Orion? How the code-completion is achieved? How the Orion editor is integrated with RAP? Why Orion? Our choice for using the Orion editor as the primary editor in Dirigible is bases on that it is has the best support and tooling for JavaScript. Also JavaScript is the language of choice for writing services with Dirigible. Beyond these arguments, Dirigible and Orion are part of the Eclipse Cloud Development iniciative , that strives to set up the standarts and the best practices for \"developing in the cloud for the cloud\" . Taking advantage of the open source eco system is key mindset, layed in the foundations of the project. Why Tern.js? Tern.js is a code-analysis and code-completion library for JavaScript. It can run both on client-side and on server-side. In order to achive real time proposals and to remove the overhead from server communication, in Dirigible we use Tern.js as a client-side library. In addition to the JavaScript code-completion, Tern.js allows to introduce custom suggestions - the way to integrate and allow code-completion for Dirigible API . How is the Injected API integrated in Orion? We use the standard Tern.js approach leveraged by Orion, by declaring objects and functions for code-completion as a JavaScript plugin. You can find the plugin here . After the build of Orion itself, there is a generated dirigible.json file out of the declarations. To use and package the embedded Orion editor in Dirigible we need to go over the following steps: Build the json definition with: npm install tern git clone orion.client git reset --hard origin/stable_20150817 Add your declaration file in ternWorkerCore.js orion.client/bundles/org.eclipse.orion.client.javascript/web/node_modules/tern/bin/condense --name dirigible --no-spans --plugin doc_comment --def ecma5 --def browser dirigible.js > orion.client/bundles/org.eclipse.orion.client.javascript/web/tern/defs/dirigible.json mvn clean install copy orion.client/build-js/codeEdit > resources How it is integrated with RAP? By using RAP scripting capabilities for callbacks, we have client and backend sides communicating via the standard RAP chanel. Thanks to functions like getText() , setText() , setDirty() , setDebugRow() , etc., we are on the half of the way. To glue to whole thing to works as one, we have in the backend EditorWidget.java and its coresponding client-side controller editor.html . What about Debugging? Last but not least, here comes the integrated debuggier in Dirigible. This was not so easy and trivial part, but finally the Dirigible's debugger uses the Orion editor. Client-side integration ... function getBreakpointsEnabled () { return breakpointsEnabled ; } function setBreakpointsEnabled ( status ) { breakpointsEnabled = status ; } function loadBreakpoint ( breakpoint ) { handleAddRemoveBreakpoint ( breakpoint ); } function setDebugRow ( row ) { editor . setCaretOffset ( editor . getLineStart ( row )); } function handleAddRemoveBreakpoint ( lineIndex ) { if ( typeof ( Storage ) === \"undefined\" ) { alert ( \"Session storage is not available!\" ) } else if ( getBreakpointsEnabled ()) { var breakpointsArray ; if ( sessionStorage . breakpoints ) { breakpointsArray = JSON . parse ( sessionStorage . breakpoints ); var index = breakpointsArray . indexOf ( lineIndex ); if ( index > - 1 ) { breakpointsArray . splice ( index , 1 ); clearBreakpoint ( lineIndex ); } else { breakpointsArray . push ( lineIndex ); setBreakpoint ( lineIndex ); } } else { breakpointsArray = [] ; breakpointsArray . push ( lineIndex ); setBreakpoint ( lineIndex ); } sessionStorage . breakpoints = JSON . stringify ( breakpointsArray ); } } ... The whole file can be found here . Server-side ... new BrowserFunction ( browser , \"setBreakpoint\" ) { @Override public Object function ( final Object [] arguments ) { if (( listener != null ) && ( arguments [ 0 ] != null ) && ( arguments [ 0 ] instanceof Number )) { listener . setBreakpoint ((( Number ) arguments [ 0 ] ). intValue ()); } return null ; } }; new BrowserFunction ( browser , \"clearBreakpoint\" ) { @Override public Object function ( final Object [] arguments ) { if (( listener != null ) && ( arguments [ 0 ] != null ) && ( arguments [ 0 ] instanceof Number )) { listener . clearBreakpoint ((( Number ) arguments [ 0 ] ). intValue ()); } return null ; } }; ... public void setDebugRow ( final int row ) { execute ( \"setDebugRow\" , row ); } public void loadBreakpoints ( final int [] breakpoints ) { for ( final int breakpoint : breakpoints ) { execute ( \"loadBreakpoint\" , breakpoint ); } } private void execute ( final String function , final Object ... arguments ) { browser . execute ( buildFunctionCall ( function , arguments )); } ... The whole file can be found here . Special thanks to Libing Wang and the orion-dev team for helping us with the integration between the debugger and the Orion editor. You can find the whole conversation here .","title":"How the Orion editor is integrated in Dirigible"},{"location":"2015/10/28/blogs-dirigible-orion-editor/#why-orion","text":"Our choice for using the Orion editor as the primary editor in Dirigible is bases on that it is has the best support and tooling for JavaScript. Also JavaScript is the language of choice for writing services with Dirigible. Beyond these arguments, Dirigible and Orion are part of the Eclipse Cloud Development iniciative , that strives to set up the standarts and the best practices for \"developing in the cloud for the cloud\" . Taking advantage of the open source eco system is key mindset, layed in the foundations of the project.","title":"Why Orion?"},{"location":"2015/10/28/blogs-dirigible-orion-editor/#why-ternjs","text":"Tern.js is a code-analysis and code-completion library for JavaScript. It can run both on client-side and on server-side. In order to achive real time proposals and to remove the overhead from server communication, in Dirigible we use Tern.js as a client-side library. In addition to the JavaScript code-completion, Tern.js allows to introduce custom suggestions - the way to integrate and allow code-completion for Dirigible API .","title":"Why Tern.js?"},{"location":"2015/10/28/blogs-dirigible-orion-editor/#how-is-the-injected-api-integrated-in-orion","text":"We use the standard Tern.js approach leveraged by Orion, by declaring objects and functions for code-completion as a JavaScript plugin. You can find the plugin here . After the build of Orion itself, there is a generated dirigible.json file out of the declarations. To use and package the embedded Orion editor in Dirigible we need to go over the following steps:","title":"How is the Injected API integrated in Orion?"},{"location":"2015/10/28/blogs-dirigible-orion-editor/#build-the-json-definition-with","text":"npm install tern git clone orion.client git reset --hard origin/stable_20150817","title":"Build the json definition with:"},{"location":"2015/10/28/blogs-dirigible-orion-editor/#add-your-declaration-file-in-ternworkercorejs","text":"orion.client/bundles/org.eclipse.orion.client.javascript/web/node_modules/tern/bin/condense --name dirigible --no-spans --plugin doc_comment --def ecma5 --def browser dirigible.js > orion.client/bundles/org.eclipse.orion.client.javascript/web/tern/defs/dirigible.json mvn clean install copy orion.client/build-js/codeEdit > resources","title":"Add your declaration file in ternWorkerCore.js"},{"location":"2015/10/28/blogs-dirigible-orion-editor/#how-it-is-integrated-with-rap","text":"By using RAP scripting capabilities for callbacks, we have client and backend sides communicating via the standard RAP chanel. Thanks to functions like getText() , setText() , setDirty() , setDebugRow() , etc., we are on the half of the way. To glue to whole thing to works as one, we have in the backend EditorWidget.java and its coresponding client-side controller editor.html .","title":"How it is integrated with RAP?"},{"location":"2015/10/28/blogs-dirigible-orion-editor/#what-about-debugging","text":"Last but not least, here comes the integrated debuggier in Dirigible. This was not so easy and trivial part, but finally the Dirigible's debugger uses the Orion editor.","title":"What about Debugging?"},{"location":"2015/10/28/blogs-dirigible-orion-editor/#client-side-integration","text":"... function getBreakpointsEnabled () { return breakpointsEnabled ; } function setBreakpointsEnabled ( status ) { breakpointsEnabled = status ; } function loadBreakpoint ( breakpoint ) { handleAddRemoveBreakpoint ( breakpoint ); } function setDebugRow ( row ) { editor . setCaretOffset ( editor . getLineStart ( row )); } function handleAddRemoveBreakpoint ( lineIndex ) { if ( typeof ( Storage ) === \"undefined\" ) { alert ( \"Session storage is not available!\" ) } else if ( getBreakpointsEnabled ()) { var breakpointsArray ; if ( sessionStorage . breakpoints ) { breakpointsArray = JSON . parse ( sessionStorage . breakpoints ); var index = breakpointsArray . indexOf ( lineIndex ); if ( index > - 1 ) { breakpointsArray . splice ( index , 1 ); clearBreakpoint ( lineIndex ); } else { breakpointsArray . push ( lineIndex ); setBreakpoint ( lineIndex ); } } else { breakpointsArray = [] ; breakpointsArray . push ( lineIndex ); setBreakpoint ( lineIndex ); } sessionStorage . breakpoints = JSON . stringify ( breakpointsArray ); } } ... The whole file can be found here .","title":"Client-side integration"},{"location":"2015/10/28/blogs-dirigible-orion-editor/#server-side","text":"... new BrowserFunction ( browser , \"setBreakpoint\" ) { @Override public Object function ( final Object [] arguments ) { if (( listener != null ) && ( arguments [ 0 ] != null ) && ( arguments [ 0 ] instanceof Number )) { listener . setBreakpoint ((( Number ) arguments [ 0 ] ). intValue ()); } return null ; } }; new BrowserFunction ( browser , \"clearBreakpoint\" ) { @Override public Object function ( final Object [] arguments ) { if (( listener != null ) && ( arguments [ 0 ] != null ) && ( arguments [ 0 ] instanceof Number )) { listener . clearBreakpoint ((( Number ) arguments [ 0 ] ). intValue ()); } return null ; } }; ... public void setDebugRow ( final int row ) { execute ( \"setDebugRow\" , row ); } public void loadBreakpoints ( final int [] breakpoints ) { for ( final int breakpoint : breakpoints ) { execute ( \"loadBreakpoint\" , breakpoint ); } } private void execute ( final String function , final Object ... arguments ) { browser . execute ( buildFunctionCall ( function , arguments )); } ... The whole file can be found here . Special thanks to Libing Wang and the orion-dev team for helping us with the integration between the debugger and the Orion editor. You can find the whole conversation here .","title":"Server-side"},{"location":"2015/12/10/blogs-dirigible-remote-debugging/","text":"While a significant part of the Dirigible development can be conveniently supported by in-Eclipse debugging using the generated 'dirigible-local' OSGi Framework launch configuration, there is also a couple of use cases that cannot be implemented using this approach. More specifically, this is the case when Dirigible is deployed in a web container and you need to debug it remotely e.g. from Eclipse. This very much affects both supportability but also partially development process too. Wrt supportability , being able to debug remotely is very important so that issues can be inspected directly in the defective deployments. Wrt development process , it is the only valid way to debug components that rely on external components and their execution environment. One such example is the configurable database access in the repository services. To set the scene in brief, suppose that one needs to integrate yet another database and debug end-to-end if it worked out (or why exactly it didn't work) in a real setup with Dirigible deployed in Tomcat with the DB drivers provisioned by Tomcat and exposed as a JNDI-bound DataSource in web.xml. Currently, you can't do this in OSGi-only environment. You could think that setting up remote debugging is as trivial as with any other web application and mostly it is with a few caveats that can ruin your day. The two important specific steps that i needed to perform before i had remote debugging working for me were: 1.Build Dirigible with debug info <plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-compiler-plugin </artifactId> <version> 2.4 </version> <configuration> <source> 1.7 </source> <target> 1.7 </target> <debug> true </debug> <debuglevel> lines,vars,source </debuglevel> </configuration> </plugin> 2.Clear the work directory in Tomcat (!) ...because in my case I had previously deployed Dirigible. OSGi is using it for its bundles and it will interfere with fresh deployments Summing up the steps: 1.Build Dirigible with debug info 2.Clear previous deployments if any and make sure the work directory is clear (very important!), and deploy 3.Start Tomcat in debug mode: catalina jpda start 4.In Eclipse, launch a Debug launch configuration with default settings. Set breakpoints and debug happily. It would be lovely to have some shortcut for the first two steps (and particularly the second) but I can't figure out anything more suitable than manual work for now. Enjoy!","title":"Developer - Remote debugging Dirigible source code"},{"location":"2015/12/10/blogs-dirigible-remote-debugging/#1build-dirigible-with-debug-info","text":"<plugin> <groupId> org.apache.maven.plugins </groupId> <artifactId> maven-compiler-plugin </artifactId> <version> 2.4 </version> <configuration> <source> 1.7 </source> <target> 1.7 </target> <debug> true </debug> <debuglevel> lines,vars,source </debuglevel> </configuration> </plugin>","title":"1.Build Dirigible with debug info"},{"location":"2015/12/10/blogs-dirigible-remote-debugging/#2clear-the-work-directory-in-tomcat","text":"...because in my case I had previously deployed Dirigible. OSGi is using it for its bundles and it will interfere with fresh deployments","title":"2.Clear the work directory in Tomcat (!)"},{"location":"2015/12/10/blogs-dirigible-remote-debugging/#summing-up-the-steps","text":"1.Build Dirigible with debug info 2.Clear previous deployments if any and make sure the work directory is clear (very important!), and deploy 3.Start Tomcat in debug mode: catalina jpda start 4.In Eclipse, launch a Debug launch configuration with default settings. Set breakpoints and debug happily. It would be lovely to have some shortcut for the first two steps (and particularly the second) but I can't figure out anything more suitable than manual work for now. Enjoy!","title":"Summing up the steps:"},{"location":"2015/12/17/blogs-how-to-install-dirigible-on-sap-hana-cloud-platform/","text":"You can try HTML, CSS, Java Script, Java, and SQL without installing anything on your computer. Just start it in HCP (SAP Hana Cloud Platform) and access from anywhere and create your own project. It could be very useful for learners, or for teachers as an online educational tool for full stack web development... Full Article Here: How to install Dirigible on SAP HANA Cloud Platform","title":"Tutorial - How to install Dirigible on SAP HANA Cloud Platform"},{"location":"2015/12/21/blogs-groovy-is-back/","text":"Groovy Dev Platform powered by Eclipse Dirigible is back! The fork of the Eclipse Dirigible project with the add-ons for Groovy Developers can be found at EclipseLabs Disclaimer This is not an official Eclipse project, but rather an example how to build your own Dev Platform based on Eclipse Dirigible. Download There is also a download section, where you can found the pre-built artifacts: Trial self executable Jar file War file for Apache Tomcat","title":"Developer - Groovy is back"},{"location":"2015/12/21/blogs-groovy-is-back/#disclaimer","text":"This is not an official Eclipse project, but rather an example how to build your own Dev Platform based on Eclipse Dirigible.","title":"Disclaimer"},{"location":"2015/12/21/blogs-groovy-is-back/#download","text":"There is also a download section, where you can found the pre-built artifacts: Trial self executable Jar file War file for Apache Tomcat","title":"Download"},{"location":"2016/01/07/blogs-dirigible-custom-ds-1/","text":"Starting with version Dirigible 2.2 M3, it is possible to register multiple custom data sources alongside with the default, system one. This feature allows keeping Dirigible system data completely separate from application data. And now application developers can create applications that span across multiple data sources. Both of these bring much more sense of production-readiness than ever before. In this Part I of our series dedicated to the new feature, we explore in details how a Service Provider would setup Dirigible for multiple data sources and how Operators and App Developers would benefit using Dirigible\u2019s database related tools and programming model. In the next parts of the series, we are going to see more topics related to custom data sources in Dirigible, such as how to extend the set of supported data sources, which currently consists of MySQL, PostgreSQL, Derby, SAPDB, SAP HANA DB and MongoDB, and more. Part I: Custom Data Sources setup A zero-to-hero, complete setup of a new Data Source in Dirigible consists of the following simple steps: Provision JDBC driver and Data Source classes to the runtime Bind a configured Data Source to JNDI Configure a reference to the JNDI-bound Data Source resource in Dirigible Register the Data Source with Dirigible IDE to make it available to its features This is the short story. Let us follow this process in details step-by-step and in more detail. Our setup will be a Dirigible web application deployed in a Tomcat web container and a PostgreSQL data source that will be provisioned along with the default Derby data source. Step 1: Provision JDBC drivers classes Supply a copy of PostgreSQL JDBC drivers jar into the <TOMCAT_HOME>/lib directory of the Tomcat hosting Dirigible. On this stage : The PostgreSQL JDBC driver classes can be loaded by web applications, including Dirigible deployment on this Tomcat instance. Step 2: Bind a Data Source to JNDI This is a web container and JDBC driver specific step. Modify <TOMCAT_HOME>/conf/context.xml to add a Resource tag: <Context> \u2026 <Resource name= \"jdbc/PostgreSQL\" auth= \"Container\" type= \"javax.sql.DataSource\" driverClassName= \"org.postgresql.Driver\" url= \"jdbc:postgresql://127.0.0.1:5432/<DB_NAME_HERE>\" username= \"<YOUR_USER_HERE>\" password= \"<YOUR_PASSWORD_HERE>\" /> \u2026 </Context> Note : Remember to change the placeholders in this example with actual values. must be changed to a valid database in your PostgreSQL instance at this URL. and are respectively the user name and password for a valid user of the database. Consult with the Postgre JDBC driver documentation for further details including on setting up for Tomcat . Tomcat\u2019s documentation also has a dedicated section on setting up a JDNI javax.sql.DataSource with PostgreSQL. On this stage : We have setup a JNDI javax.sql.DataSource instance named jdbc/PostgreSQL that can be looked up and is fully capable of producing connections to a PostgreSQL database as defined by its parameters. Step 3: Configure an application reference to the JNDI-bound Data Source resource There are two approaches at this goal serving different use cases. One of the use cases is when Dirigible\u2019s runtime environment is not managed. In that case, the required configuration details are entered in Dirigible\u2019s web.xml and this is the case that we shall review in more detail here. The other case is when the runtime environment is managed for e.g. automatic system provisioning. In that case the required configuration can be provided with environment variables (and of course this approach can be used also for unmanaged environments). We shall dedicate a blog on this in future. Locate Dirigible\u2019s web.xml descriptor file and open it for edit. In web.xml, locate the section with the tag <servlet id=\"bridge\"> and scroll down to its set of init-param tags. Add the following block: <init-param> <param-name> jndiCustomDataSource-postgre </param-name> <param-value> java:comp/env/jdbc/PostgreSQL </param-value> </init-param> Make sure to use the prefix \u201cjndiCustomDataSource-\u201c in param-name exactly as is, case sensitive. The suffix postgre will be used as identity for this resource in the next step. Notice, the construction of the string in the param-value. The pattern is to add the prefix java:comp/env to the name of the JNDI javax.sql.DataSource resource we defined in the previous step in context.xml . In our case this is the string jdbc/PostgreSQL . On this stage : We have setup Dirigible to lookup a named javax.sql.DataSource from JDNI and make it available to its features. Step 4: Register the Data Source in Dirigible injected API Note : Before proceeding, make sure that Tomcat is restarted if it was online when previous steps were accomplished, or start it now if it was offline. Open Dirigible IDE and select Window > Preferences from its menu: In the dialog that pops up, locate Data Sources in the list on the left, and then click the button New\u2026 Fill in the pop up as suggested by the screenshot below: Finally, confirm all dialogs. On this stage : The PostgreSQL javax.sql.DataSource is available for the Dirigible Injected API and Database tools. Verify results It is time to reap what we sow now. We shall now step in the shoes of an Operator or a Developer and use Dirigible\u2019s Database perspective tools with our new data source. Click to expand the dropdown in the Database Browser view and voil\u00e0, we\u2019ve got a brand new additional data source called postgre : Let\u2019s explore it like we do with the default one. Drill down its contents and select the table information_schema.sql_languages . Right-click on it and choose Open Definition from the context menu: Find the result in Table Details : Now right-click on the table again and select Show Content from the context menu. Find the result in SQL Console : Now, let\u2019s try how we can benefit from the new data source programmatically. Follow the implementation steps described in this dirigible sample to create a scripting service that uses the InjectedAPI to get a reference to our custom data source and print some results. Use the following source for the service: /* globals $ */ /* eslint-env node, dirigible */ $ . getResponse (). setContentType ( \"text/html; charset=UTF-8\" ); $ . getResponse (). setCharacterEncoding ( \"UTF-8\" ); var ds = $ . getNamedDatasources (). get ( \"postgre\" ); var conn = ds . getConnection (); try { var stmt = conn . createStatement (); var rs = stmt . executeQuery ( \"select * from \\\"information_schema\\\".\\\"sql_languages\\\"\" ); $ . getResponse (). getWriter (). println ( 'SQL language variants<br>' ); while ( rs . next ()) { $ . getResponse (). getWriter (). println ( rs . getString ( 1 ) + '-' + rs . getString ( 2 ) + '<br>' ); } } finally { conn . close (); } $ . getResponse (). getWriter (). flush (); $ . getResponse (). getWriter (). close (); The printed results look like that: What is next? Now that you know how to quickly onboard a data source for a database supported by Dirigible out-of-the-box, you might want learn how to approach the rest that are available out there. That is the topic of the next blog in the series.","title":"BYODS (Bring Your Own Data Source) in Dirigible - Part I: Custom Data Sources setup"},{"location":"2016/01/07/blogs-dirigible-custom-ds-1/#part-i-custom-data-sources-setup","text":"A zero-to-hero, complete setup of a new Data Source in Dirigible consists of the following simple steps: Provision JDBC driver and Data Source classes to the runtime Bind a configured Data Source to JNDI Configure a reference to the JNDI-bound Data Source resource in Dirigible Register the Data Source with Dirigible IDE to make it available to its features This is the short story. Let us follow this process in details step-by-step and in more detail. Our setup will be a Dirigible web application deployed in a Tomcat web container and a PostgreSQL data source that will be provisioned along with the default Derby data source.","title":"Part I: Custom Data Sources setup"},{"location":"2016/01/07/blogs-dirigible-custom-ds-1/#step-1-provision-jdbc-drivers-classes","text":"Supply a copy of PostgreSQL JDBC drivers jar into the <TOMCAT_HOME>/lib directory of the Tomcat hosting Dirigible. On this stage : The PostgreSQL JDBC driver classes can be loaded by web applications, including Dirigible deployment on this Tomcat instance.","title":"Step 1: Provision JDBC drivers classes"},{"location":"2016/01/07/blogs-dirigible-custom-ds-1/#step-2-bind-a-data-source-to-jndi","text":"This is a web container and JDBC driver specific step. Modify <TOMCAT_HOME>/conf/context.xml to add a Resource tag: <Context> \u2026 <Resource name= \"jdbc/PostgreSQL\" auth= \"Container\" type= \"javax.sql.DataSource\" driverClassName= \"org.postgresql.Driver\" url= \"jdbc:postgresql://127.0.0.1:5432/<DB_NAME_HERE>\" username= \"<YOUR_USER_HERE>\" password= \"<YOUR_PASSWORD_HERE>\" /> \u2026 </Context> Note : Remember to change the placeholders in this example with actual values. must be changed to a valid database in your PostgreSQL instance at this URL. and are respectively the user name and password for a valid user of the database. Consult with the Postgre JDBC driver documentation for further details including on setting up for Tomcat . Tomcat\u2019s documentation also has a dedicated section on setting up a JDNI javax.sql.DataSource with PostgreSQL. On this stage : We have setup a JNDI javax.sql.DataSource instance named jdbc/PostgreSQL that can be looked up and is fully capable of producing connections to a PostgreSQL database as defined by its parameters.","title":"Step 2: Bind a Data Source to JNDI"},{"location":"2016/01/07/blogs-dirigible-custom-ds-1/#step-3-configure-an-application-reference-to-the-jndi-bound-data-source-resource","text":"There are two approaches at this goal serving different use cases. One of the use cases is when Dirigible\u2019s runtime environment is not managed. In that case, the required configuration details are entered in Dirigible\u2019s web.xml and this is the case that we shall review in more detail here. The other case is when the runtime environment is managed for e.g. automatic system provisioning. In that case the required configuration can be provided with environment variables (and of course this approach can be used also for unmanaged environments). We shall dedicate a blog on this in future. Locate Dirigible\u2019s web.xml descriptor file and open it for edit. In web.xml, locate the section with the tag <servlet id=\"bridge\"> and scroll down to its set of init-param tags. Add the following block: <init-param> <param-name> jndiCustomDataSource-postgre </param-name> <param-value> java:comp/env/jdbc/PostgreSQL </param-value> </init-param> Make sure to use the prefix \u201cjndiCustomDataSource-\u201c in param-name exactly as is, case sensitive. The suffix postgre will be used as identity for this resource in the next step. Notice, the construction of the string in the param-value. The pattern is to add the prefix java:comp/env to the name of the JNDI javax.sql.DataSource resource we defined in the previous step in context.xml . In our case this is the string jdbc/PostgreSQL . On this stage : We have setup Dirigible to lookup a named javax.sql.DataSource from JDNI and make it available to its features.","title":"Step 3: Configure an application reference to the JNDI-bound Data Source resource"},{"location":"2016/01/07/blogs-dirigible-custom-ds-1/#step-4-register-the-data-source-in-dirigible-injected-api","text":"Note : Before proceeding, make sure that Tomcat is restarted if it was online when previous steps were accomplished, or start it now if it was offline. Open Dirigible IDE and select Window > Preferences from its menu: In the dialog that pops up, locate Data Sources in the list on the left, and then click the button New\u2026 Fill in the pop up as suggested by the screenshot below: Finally, confirm all dialogs. On this stage : The PostgreSQL javax.sql.DataSource is available for the Dirigible Injected API and Database tools.","title":"Step 4: Register the Data Source in Dirigible injected API"},{"location":"2016/01/07/blogs-dirigible-custom-ds-1/#verify-results","text":"It is time to reap what we sow now. We shall now step in the shoes of an Operator or a Developer and use Dirigible\u2019s Database perspective tools with our new data source. Click to expand the dropdown in the Database Browser view and voil\u00e0, we\u2019ve got a brand new additional data source called postgre : Let\u2019s explore it like we do with the default one. Drill down its contents and select the table information_schema.sql_languages . Right-click on it and choose Open Definition from the context menu: Find the result in Table Details : Now right-click on the table again and select Show Content from the context menu. Find the result in SQL Console : Now, let\u2019s try how we can benefit from the new data source programmatically. Follow the implementation steps described in this dirigible sample to create a scripting service that uses the InjectedAPI to get a reference to our custom data source and print some results. Use the following source for the service: /* globals $ */ /* eslint-env node, dirigible */ $ . getResponse (). setContentType ( \"text/html; charset=UTF-8\" ); $ . getResponse (). setCharacterEncoding ( \"UTF-8\" ); var ds = $ . getNamedDatasources (). get ( \"postgre\" ); var conn = ds . getConnection (); try { var stmt = conn . createStatement (); var rs = stmt . executeQuery ( \"select * from \\\"information_schema\\\".\\\"sql_languages\\\"\" ); $ . getResponse (). getWriter (). println ( 'SQL language variants<br>' ); while ( rs . next ()) { $ . getResponse (). getWriter (). println ( rs . getString ( 1 ) + '-' + rs . getString ( 2 ) + '<br>' ); } } finally { conn . close (); } $ . getResponse (). getWriter (). flush (); $ . getResponse (). getWriter (). close (); The printed results look like that:","title":"Verify results"},{"location":"2016/01/07/blogs-dirigible-custom-ds-1/#what-is-next","text":"Now that you know how to quickly onboard a data source for a database supported by Dirigible out-of-the-box, you might want learn how to approach the rest that are available out there. That is the topic of the next blog in the series.","title":"What is next?"},{"location":"2016/01/11/blogs-dirigible-custom-ds-2/","text":"Dirigible supports multiple database products by means of dialect adapters that can be used to extend the support to new ones In the previous Part I of our series, dedicated to the new multiple custom data sources feature, we introduced you to the routines required to setup a new custom data source representing one of the database brands that Dirigible supports out-of-the-box, and use it. In this Part II of the series we are going to explore what it takes to onboard a new, not yet supported database and make use of data sources configured for it, as discussed in Part I . One of the setup steps requires a minimal development and integration effort and in this Part II, we explore in details this particular task that is necessary to accomplish the integration of a new data source kind in Dirigible. It is a one-time job per database product that can then be reused for any concrete instance. Part II: Extending supported databases for custom data sources The relational database world enjoy the standard query language SQL for ages. However, database systems are often not entirely compliant with the standard. For example, it happens that they implement subset or extensions of it and ultimately end up with variants of SQL. We call these variants database (SQL) dialects . An optimal and correct use of a database requires to take this into account. This is why Dirigible and alike tools need to \u2018know\u2019 dialects to be able to truly support the corresponding database. And since the list of databases and applicable dialects out there is quite big, and it grows, it is reasonable to support some sensible, popular minimum of these and provide a mechanism to extend the support. The databases that are currently supported in Dirigible (in version 2.2 M3) are MySQL, PostgreSQL, Derby, SAPDB, SAP HANA DB, Sybase and MongoDB. Dirigible speaks their dialects already and you can create custom data sources configured for running instances of these databases as discussed in Part I . Let us now explore what is how to extend this list to support also H2 database and be able to create custom data sources for it too. Hitting the wall Let us first try to employ the routine from Part I with H2 and see what happens. Step 1: Provision the drivers Supply a copy of the database JDBC drivers in Tomcat\u2019s lib directory. H2 JDBC drivers are bundled together with the DB code so this means the database jar needs to be put there. Step 2: Bind to JNDI Edit Tomcat\u2019s conf/context.xml to add a resource: <Resource name= \"jdbc/H2\" auth= \"Container\" type= \"javax.sql.DataSource\" username= \"sa\" password= \"\" driverClassName= \"org.h2.Driver\" url= \"jdbc:h2:mem: \" /> Step 3: Configure application reference Add the following init parameter to the bridge servlet in the web.xml <init-param> <param-name> jndiCustomDataSource-h2 </param-name> <param-value> java:comp/env/jdbc/H2 </param-value> </init-param> Step 4: Register the data source Verify results Let\u2019s go now and check our H2 database in the Database perspective in Dirigible\u2019s IDE. Ooops: What happened? Yep! Dirigible clearly doesn\u2019t speak H2 dialect. Let\u2019s see what we can do to teach it. A new dialect onboard We need to accomplish the following steps in order to achieve our goal: Provide a class implementing the IDialectSpecifier interface Include the class in an OSGi bundle Declare an OSGi Service in XML descriptor with the new class and the IDialectSpecifier interface Register the XML descriptor in its container bundle MANIFEST.MF Except for the first task that is purely development and requires mostly domain knowledge for Dirigible APIs and H2 database, the rest of the tasks are a standard wiring mechanism and component model in OSGi. Let\u2019s focus on each part now. Implementation Technologies such as Dirigible delegate to concrete dialects the handling of database-specific statements and the interface IDialectSpecifier defines this contract. In addition, the interface also specifies some more generic characteristics of a database product kind, such as if it is a schemaless database or not (Yes, we look at you NoSQL! But more on that in a future blog). To make things easier and reduce redundant code to the minimum, Dirigible provides an out-of-the-box, convenience, common implementation for relational databases called RDBGenericDialectSpecifier . An absolutely minimal implementation of a dialect is the following public class H2DBSpecifier extends RDBGenericDialectSpecifier { private static final String PRODUCT_NAME = \"H2\" ; @Override public boolean isDialectForName ( String productName ) { return PRODUCT_NAME . equalsIgnoreCase ( productName ); } } It doesn\u2019t do much but is just enough to get us going. We will leave it as it is for now and proceed with some plumbing. Later, we shall come back to the class for a more elaborate insight and implementation. Bundling What we need to achieve on this stage is to declare a new OSGi (declarative) service so that Dirigible can find and use it. Each out-of-the-box dialect is declared as a service component, with its service interface ( IDialectSpecifier ) physically residing in its own bundle (org.eclipse.dirigible.repository.datasource), and an implementation class in another (org.eclipse.dirigible.repository.datasource.dialects). Detaching the interface and its implementations allows seamless, dynamic discovery of available dialects at runtime without disruption when new dialects are onboarded. Let\u2019s get down to it. First, we need to declare our service component in a XML descriptor file. Normally, such XMLs reside in an OSGI-INF directory. For example, OSGI-INF/h2-dialect.xml: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <scr:component xmlns:scr= \"http://www.osgi.org/xmlns/scr/v1.1.0\" immediate= \"true\" name= \"H2Dialect\" > <service> <provide interface= \"org.eclipse.dirigible.repository.datasource.db.dialect.IDialectSpecifier\" /> </service> <implementation class= \"org.eclipse.dirigible.repository.datasource.db.dialect.H2DBSpecifier\" /> </scr:component> Here the important variables are the component name and the implementation class. See, the OSGI-INF directory in org.eclipse.dirigible.repository.datasource.dialects for other examples. Next, we need to register the new service component in its bundle MANIFETS.MF in a Service-Component header. Another option that comes in handy is to use a pattern instead (e.g. Service-Component: OSGi-INF/*.xml). See for example the MANIFEST.MF in the bundle with the out-of-the-box dialects. And that\u2019s all folks! If you follow this routine, rebuild Dirigible with a bundle that contains your correctly registered dialect and deploy it, you will be able to happily explore the H2 database: Dissecting IDialectSpecifier Now, as promised, let\u2019s get back to the main interface for dialects. Its methods can be grouped according to their purpose. We shall review the more important of each one here. SQL statement construction callbacks There are a number of methods that are invoked during the construction of statements (queries and updates) by the Dirigible database related tools: String specify(String sql); String createLimitAndOffset(int limit, int offset); String createTopAndStart(int limit, int offset); String getAlterAddOpen(); String getAlterAddOpenEach(); String getAlterAddClose(); String getAlterAddCloseEach(); The specify method is concerned with transforming SQL statements into dialect-specific strings, by replacing a set of predefined variables from the input string with database-specific values. The variables are defined as constants in IDIalectSpecifier : public static final String DIALECT_TIMESTAMP = \"$TIMESTAMP$\"; public static final String DIALECT_BLOB = \"$BLOB$\"; public static final String DIALECT_CLOB = \"$CLOB$\"; public static final String DIALECT_CURRENT_TIMESTAMP = \"$CURRENT_TIMESTAMP$\"; public static final String DIALECT_KEY_VARCHAR = \"$KEY_VARCHAR$\"; public static final String DIALECT_BIG_VARCHAR = \"$BIG_VARCHAR$\"; The createLimitAndOffset and createTopAndStart methods are concerned with two alternative SQL syntax constructs that \"page\" results (return a subset form specified cursor position) The set of getAlter* methods are handling the ALTER <table-name> TABLE ADD construct in dialect specific manner. There are actually two pairs of methods, each pair concerned with a variant of handling the column ADD syntax. In each pair there is a method handling the opening part of the construction and there is one for the closing part. ResultSet iteration callbacks The following methods are used by Dirigible while iterating a query ResultSet: InputStream getBinaryStream(ResultSet resultSet, String columnName) throws SQLException; Common data type model translation String getSpecificType(String commonType); The getSpecificType method is responsible to translate between the common types used in Dirigible and database-specific ones. Query templates Dirigible tools such as the SQLConsole and the Database browser collaborate with the action Show Content to set a generic query, listing the contents a table and execute it. It is the following method that is invoked to provision that generic query: String getContentQueryScript(String catalogName, String schemaName, String tableName); Another one is concerned with provisioning a query that will perform database specific filtering of schemas and show only the applicable for Dirigible: String getSchemaFilterScript(); Database Metadata There are also methods concerned with the general description of the data base: boolean isSchemaFilterSupported(); boolean isCatalogForSchema(); boolean isSchemaless(); isDialectForName(productName); These methods are mostly used to decide on the composition of UI or process. For example, isSchemaless is used to determine whether the Open Definition action in the Database Browser context menu for tables is presented to end user or not. Obviously, it doesn\u2019t make a lot of sense for schemaless databases, at least not with the current view that deals with it. Similarly isSchemaFilterSupported is used by the Database Browser to invoke upon availability the getSchemaFilter method (discussed in previsous section) and reduce the schemas exhibited in the view to the applicable ones. And isCatalogForSchema instructs the UI how to handle database layouts of database products that have specific, non-standard handling of catalogs and schemas. But above all it's worth mentioning here the isDialectForName method. As you probably noted, this was the only one that was part of the minimal implementation of a dialect. What it does essentially is to assess the dialect where it is declared is applicable for the database product name supplied as argument for the productName parameter of the method. The value of the productName parameter is the string supplied by JDBC drivers implementation of DatabaseMetaData#getDatabaseProductName API. Dirigible uses this to determine, which of the available service implementations of IDialectSpecifier is applicable for a given database. Wrapping up Summing up what we already know about the IDialectSpecifier interface, here is a slightly more elaborated variant of the minimal dialect implementation that we started with: public class H2DBSpecifier extends RDBGenericDialectSpecifier { private static final String PRODUCT_NAME = \"H2\" ; private static final String H2_TIMESTAMP = \"TIMESTAMP\" ; private static final String H2_CLOB = \"CLOB\" ; private static final String H2_BLOB = \"BLOB\" ; private static final String H2_CURRENT_TIMESTAMP = \"CURRENT_TIMESTAMP\" ; private static final String H2_BIG_VARCHAR = \"VARCHAR(1000)\" ; private static final String H2_KEY_VARCHAR = \"VARCHAR(4000)\" ; private static final String LIMIT_D_D = \"LIMIT %d OFFSET %d\" ; @Override public boolean isDialectForName ( String productName ) { return PRODUCT_NAME . equalsIgnoreCase ( productName ); } @Override public String specify ( String sql ) { if ( sql == null || sql . length () < 1 ) return sql ; return sql . replace ( DIALECT_TIMESTAMP , H2_TIMESTAMP ) . replace ( DIALECT_CLOB , H2_CLOB ) . replace ( DIALECT_BLOB , H2_BLOB ) . replace ( DIALECT_CURRENT_TIMESTAMP , H2_CURRENT_TIMESTAMP ) . replace ( DIALECT_BIG_VARCHAR , H2_BIG_VARCHAR ) . replace ( DIALECT_KEY_VARCHAR , H2_KEY_VARCHAR ); } @Override public String createLimitAndOffset ( int limit , int offset ) { return String . format ( LIMIT_D_D , offset , limit ); } @Override public String getAlterAddOpen () { return \" ADD( \" ; } @Override public String getAlterAddClose () { return \")\" ; } }","title":"BYODS (Bring Your Own Data Source) in Dirigible - Part II: Extending supported databases for custom data sources"},{"location":"2016/01/11/blogs-dirigible-custom-ds-2/#part-ii-extending-supported-databases-for-custom-data-sources","text":"The relational database world enjoy the standard query language SQL for ages. However, database systems are often not entirely compliant with the standard. For example, it happens that they implement subset or extensions of it and ultimately end up with variants of SQL. We call these variants database (SQL) dialects . An optimal and correct use of a database requires to take this into account. This is why Dirigible and alike tools need to \u2018know\u2019 dialects to be able to truly support the corresponding database. And since the list of databases and applicable dialects out there is quite big, and it grows, it is reasonable to support some sensible, popular minimum of these and provide a mechanism to extend the support. The databases that are currently supported in Dirigible (in version 2.2 M3) are MySQL, PostgreSQL, Derby, SAPDB, SAP HANA DB, Sybase and MongoDB. Dirigible speaks their dialects already and you can create custom data sources configured for running instances of these databases as discussed in Part I . Let us now explore what is how to extend this list to support also H2 database and be able to create custom data sources for it too.","title":"Part II: Extending supported databases for custom data sources"},{"location":"2016/01/11/blogs-dirigible-custom-ds-2/#hitting-the-wall","text":"Let us first try to employ the routine from Part I with H2 and see what happens.","title":"Hitting the wall"},{"location":"2016/01/11/blogs-dirigible-custom-ds-2/#step-1-provision-the-drivers","text":"Supply a copy of the database JDBC drivers in Tomcat\u2019s lib directory. H2 JDBC drivers are bundled together with the DB code so this means the database jar needs to be put there.","title":"Step 1: Provision the drivers"},{"location":"2016/01/11/blogs-dirigible-custom-ds-2/#step-2-bind-to-jndi","text":"Edit Tomcat\u2019s conf/context.xml to add a resource: <Resource name= \"jdbc/H2\" auth= \"Container\" type= \"javax.sql.DataSource\" username= \"sa\" password= \"\" driverClassName= \"org.h2.Driver\" url= \"jdbc:h2:mem: \" />","title":"Step 2: Bind to JNDI"},{"location":"2016/01/11/blogs-dirigible-custom-ds-2/#step-3-configure-application-reference","text":"Add the following init parameter to the bridge servlet in the web.xml <init-param> <param-name> jndiCustomDataSource-h2 </param-name> <param-value> java:comp/env/jdbc/H2 </param-value> </init-param>","title":"Step 3: Configure application reference"},{"location":"2016/01/11/blogs-dirigible-custom-ds-2/#step-4-register-the-data-source","text":"","title":"Step 4: Register the data source"},{"location":"2016/01/11/blogs-dirigible-custom-ds-2/#verify-results","text":"Let\u2019s go now and check our H2 database in the Database perspective in Dirigible\u2019s IDE. Ooops: What happened? Yep! Dirigible clearly doesn\u2019t speak H2 dialect. Let\u2019s see what we can do to teach it.","title":"Verify results"},{"location":"2016/01/11/blogs-dirigible-custom-ds-2/#a-new-dialect-onboard","text":"We need to accomplish the following steps in order to achieve our goal: Provide a class implementing the IDialectSpecifier interface Include the class in an OSGi bundle Declare an OSGi Service in XML descriptor with the new class and the IDialectSpecifier interface Register the XML descriptor in its container bundle MANIFEST.MF Except for the first task that is purely development and requires mostly domain knowledge for Dirigible APIs and H2 database, the rest of the tasks are a standard wiring mechanism and component model in OSGi. Let\u2019s focus on each part now.","title":"A new dialect onboard"},{"location":"2016/01/11/blogs-dirigible-custom-ds-2/#implementation","text":"Technologies such as Dirigible delegate to concrete dialects the handling of database-specific statements and the interface IDialectSpecifier defines this contract. In addition, the interface also specifies some more generic characteristics of a database product kind, such as if it is a schemaless database or not (Yes, we look at you NoSQL! But more on that in a future blog). To make things easier and reduce redundant code to the minimum, Dirigible provides an out-of-the-box, convenience, common implementation for relational databases called RDBGenericDialectSpecifier . An absolutely minimal implementation of a dialect is the following public class H2DBSpecifier extends RDBGenericDialectSpecifier { private static final String PRODUCT_NAME = \"H2\" ; @Override public boolean isDialectForName ( String productName ) { return PRODUCT_NAME . equalsIgnoreCase ( productName ); } } It doesn\u2019t do much but is just enough to get us going. We will leave it as it is for now and proceed with some plumbing. Later, we shall come back to the class for a more elaborate insight and implementation.","title":"Implementation"},{"location":"2016/01/11/blogs-dirigible-custom-ds-2/#bundling","text":"What we need to achieve on this stage is to declare a new OSGi (declarative) service so that Dirigible can find and use it. Each out-of-the-box dialect is declared as a service component, with its service interface ( IDialectSpecifier ) physically residing in its own bundle (org.eclipse.dirigible.repository.datasource), and an implementation class in another (org.eclipse.dirigible.repository.datasource.dialects). Detaching the interface and its implementations allows seamless, dynamic discovery of available dialects at runtime without disruption when new dialects are onboarded. Let\u2019s get down to it. First, we need to declare our service component in a XML descriptor file. Normally, such XMLs reside in an OSGI-INF directory. For example, OSGI-INF/h2-dialect.xml: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <scr:component xmlns:scr= \"http://www.osgi.org/xmlns/scr/v1.1.0\" immediate= \"true\" name= \"H2Dialect\" > <service> <provide interface= \"org.eclipse.dirigible.repository.datasource.db.dialect.IDialectSpecifier\" /> </service> <implementation class= \"org.eclipse.dirigible.repository.datasource.db.dialect.H2DBSpecifier\" /> </scr:component> Here the important variables are the component name and the implementation class. See, the OSGI-INF directory in org.eclipse.dirigible.repository.datasource.dialects for other examples. Next, we need to register the new service component in its bundle MANIFETS.MF in a Service-Component header. Another option that comes in handy is to use a pattern instead (e.g. Service-Component: OSGi-INF/*.xml). See for example the MANIFEST.MF in the bundle with the out-of-the-box dialects. And that\u2019s all folks! If you follow this routine, rebuild Dirigible with a bundle that contains your correctly registered dialect and deploy it, you will be able to happily explore the H2 database:","title":"Bundling"},{"location":"2016/01/11/blogs-dirigible-custom-ds-2/#dissecting-idialectspecifier","text":"Now, as promised, let\u2019s get back to the main interface for dialects. Its methods can be grouped according to their purpose. We shall review the more important of each one here.","title":"Dissecting IDialectSpecifier"},{"location":"2016/01/11/blogs-dirigible-custom-ds-2/#sql-statement-construction-callbacks","text":"There are a number of methods that are invoked during the construction of statements (queries and updates) by the Dirigible database related tools: String specify(String sql); String createLimitAndOffset(int limit, int offset); String createTopAndStart(int limit, int offset); String getAlterAddOpen(); String getAlterAddOpenEach(); String getAlterAddClose(); String getAlterAddCloseEach(); The specify method is concerned with transforming SQL statements into dialect-specific strings, by replacing a set of predefined variables from the input string with database-specific values. The variables are defined as constants in IDIalectSpecifier : public static final String DIALECT_TIMESTAMP = \"$TIMESTAMP$\"; public static final String DIALECT_BLOB = \"$BLOB$\"; public static final String DIALECT_CLOB = \"$CLOB$\"; public static final String DIALECT_CURRENT_TIMESTAMP = \"$CURRENT_TIMESTAMP$\"; public static final String DIALECT_KEY_VARCHAR = \"$KEY_VARCHAR$\"; public static final String DIALECT_BIG_VARCHAR = \"$BIG_VARCHAR$\"; The createLimitAndOffset and createTopAndStart methods are concerned with two alternative SQL syntax constructs that \"page\" results (return a subset form specified cursor position) The set of getAlter* methods are handling the ALTER <table-name> TABLE ADD construct in dialect specific manner. There are actually two pairs of methods, each pair concerned with a variant of handling the column ADD syntax. In each pair there is a method handling the opening part of the construction and there is one for the closing part.","title":"SQL statement construction callbacks"},{"location":"2016/01/11/blogs-dirigible-custom-ds-2/#resultset-iteration-callbacks","text":"The following methods are used by Dirigible while iterating a query ResultSet: InputStream getBinaryStream(ResultSet resultSet, String columnName) throws SQLException;","title":"ResultSet iteration callbacks"},{"location":"2016/01/11/blogs-dirigible-custom-ds-2/#common-data-type-model-translation","text":"String getSpecificType(String commonType); The getSpecificType method is responsible to translate between the common types used in Dirigible and database-specific ones.","title":"Common data type model translation"},{"location":"2016/01/11/blogs-dirigible-custom-ds-2/#query-templates","text":"Dirigible tools such as the SQLConsole and the Database browser collaborate with the action Show Content to set a generic query, listing the contents a table and execute it. It is the following method that is invoked to provision that generic query: String getContentQueryScript(String catalogName, String schemaName, String tableName); Another one is concerned with provisioning a query that will perform database specific filtering of schemas and show only the applicable for Dirigible: String getSchemaFilterScript();","title":"Query templates"},{"location":"2016/01/11/blogs-dirigible-custom-ds-2/#database-metadata","text":"There are also methods concerned with the general description of the data base: boolean isSchemaFilterSupported(); boolean isCatalogForSchema(); boolean isSchemaless(); isDialectForName(productName); These methods are mostly used to decide on the composition of UI or process. For example, isSchemaless is used to determine whether the Open Definition action in the Database Browser context menu for tables is presented to end user or not. Obviously, it doesn\u2019t make a lot of sense for schemaless databases, at least not with the current view that deals with it. Similarly isSchemaFilterSupported is used by the Database Browser to invoke upon availability the getSchemaFilter method (discussed in previsous section) and reduce the schemas exhibited in the view to the applicable ones. And isCatalogForSchema instructs the UI how to handle database layouts of database products that have specific, non-standard handling of catalogs and schemas. But above all it's worth mentioning here the isDialectForName method. As you probably noted, this was the only one that was part of the minimal implementation of a dialect. What it does essentially is to assess the dialect where it is declared is applicable for the database product name supplied as argument for the productName parameter of the method. The value of the productName parameter is the string supplied by JDBC drivers implementation of DatabaseMetaData#getDatabaseProductName API. Dirigible uses this to determine, which of the available service implementations of IDialectSpecifier is applicable for a given database.","title":"Database Metadata"},{"location":"2016/01/11/blogs-dirigible-custom-ds-2/#wrapping-up","text":"Summing up what we already know about the IDialectSpecifier interface, here is a slightly more elaborated variant of the minimal dialect implementation that we started with: public class H2DBSpecifier extends RDBGenericDialectSpecifier { private static final String PRODUCT_NAME = \"H2\" ; private static final String H2_TIMESTAMP = \"TIMESTAMP\" ; private static final String H2_CLOB = \"CLOB\" ; private static final String H2_BLOB = \"BLOB\" ; private static final String H2_CURRENT_TIMESTAMP = \"CURRENT_TIMESTAMP\" ; private static final String H2_BIG_VARCHAR = \"VARCHAR(1000)\" ; private static final String H2_KEY_VARCHAR = \"VARCHAR(4000)\" ; private static final String LIMIT_D_D = \"LIMIT %d OFFSET %d\" ; @Override public boolean isDialectForName ( String productName ) { return PRODUCT_NAME . equalsIgnoreCase ( productName ); } @Override public String specify ( String sql ) { if ( sql == null || sql . length () < 1 ) return sql ; return sql . replace ( DIALECT_TIMESTAMP , H2_TIMESTAMP ) . replace ( DIALECT_CLOB , H2_CLOB ) . replace ( DIALECT_BLOB , H2_BLOB ) . replace ( DIALECT_CURRENT_TIMESTAMP , H2_CURRENT_TIMESTAMP ) . replace ( DIALECT_BIG_VARCHAR , H2_BIG_VARCHAR ) . replace ( DIALECT_KEY_VARCHAR , H2_KEY_VARCHAR ); } @Override public String createLimitAndOffset ( int limit , int offset ) { return String . format ( LIMIT_D_D , offset , limit ); } @Override public String getAlterAddOpen () { return \" ADD( \" ; } @Override public String getAlterAddClose () { return \")\" ; } }","title":"Wrapping up"},{"location":"2016/01/21/blogs-dirigible-custom-ds-3/","text":"Dirigible welcomes Mongo DB onboard! Starting with version 2.2 Mongo DB is supported out-of-the-box In previous blogs in the \"BYODS in Dirigible\" series we explored how data sources are integrated in general with most examples focusing on relational databases as options. But what about NoSQL ? Dirigible welcomes Mongo DB onboard! Starting with version 2.2 Mongo DB is supported out-of-the-box. You can explore it in the IDE and develop scripting services for it. Taking advantage of NoSQL document storage is now an entirely viable option. This is the first stage of our roadmap for onboarding NoSQL development. It uses JDBC as standard communication protocol and API. We are well aware that it is not native to NoSql development and is rather a \u201cquick way in\u201d. On next stage, we plan to explore the options to provision Apache TinkerPop as a well-recognized standard Graph API (to which Mongo DB also complies) to scripting services via the Injected API . We shall seek also for convenient ways to provide access to native Graph APIs of NoSql data stores with all pros and cons that go along with that. But first things first. We shall now explore what we\u2019ve got for Mongo DB developers in Dirigible 2.2. Part III: MongoDB custom data source What\u2019s in the box? With a Mongo DB custom data source integrated in Dirigible Database tools, you can explore the related database instance list of collections and examine collection documents: <img src=\"/img/posts/20160121-0/3-0.png\"/> As with any other relational data source, you can also execute queries and updates but hence, using Mongo\u2019s native BSON-based query language: <img src=\"/img/posts/20160121-0/3-1.png\"/> It is integrated also into the InjectedAPI and therefore in your scripting service you can request the data source by its name, get a connection and execute a statement using Mongo\u2019s native query language, and iterate the result set (here, using the JDBC API. Read below for more options): var ds = $ . getNamedDatasources (). get ( \u2018 mongodb \u2019 ); var conn = ds . getConnection (); try { var stmt = conn . createStatement (); var rs = stmt . executeQuery ( \u2018 { \u201c find \u201d : \u201d testCollection \u201d } \u2019 ); while ( rs . next ()) { $ . getResponse (). getWriter (). println ( rs . getString ( 1 ) + ':' + rs . getString ( \u201c name \u201d ) + '<br>' ); } } finally { conn . close (); } Key design notes Onboarding a Mongo DB data source leverages exactly the same integration mechanism in Dirigible as any other (relational) data source. This feature has been discussed in detail in the previous BYODS blog series. The obvious advantage of this approach is that it follows an established path. That simplicity comes at the cost of a few reasonable prerequisites listed below. JDBC API JDBC is the standard API used by Dirigible internally to integrate data sources and by developers to use them. Therefore, you will need a JDBC compliant driver to provision access to a Mongo database. Its role is to reconcile the conceptual differences between the relational model centric JDBC API and the NoSQL document store world. In Mongo, despite the name \u201c Java driver \u201d that you will find on Mongo DB\u2019s site concerning Java clients, this has nothing to do with JDBC drivers. It is a Java client API. If you look around for available JDBC drivers for Mongo DB, they are not exactly abundant either. What\u2019s more troublesome here is that virtually all available drivers actually try to translate between Mongo DB\u2019s native query language and SQL. While this works perfectly well for us in terms of technical integration, it does not comply with our goal to make Mongo DB\u2019s developers feel at home in Dirigible, because it would be fairly weird for them to write SQL to query a document database. To fill this gap and for the sake of this example we\u2019ve prototyped a driver that can send native queries encoded as BSON to Mongo DB. It is available on Github . The fine print? Once again, this driver is a prototype and as of the time of this writing there\u2019s still nothing comparable (Meaning happily abusing the JDBC API as a standard protocol for Mongo DB but reusing its own query language). Show some love for it and we will further enhance it. The rest of the drivers out there translate to/from SQL, which will work for the InjectedAPI if you are happy with this approach, but not with the Dirigible database tools in the IDE\u2019s Database perspective. Query language In order to execute query or update statements from Dirigible, your back-end needs to be able to interpret a formal language that can be encoded in strings because that\u2019s the input it will get. There are options here, but it would be best to re-use a query language if your database already has one. Developers who are already used to it will feel at home and make the best use of the database capabilities. Other options, but less desirable for the same reasons, are to translate to and from SQL or other suitable language. MongoDB has a concept of query language. Queries are BSON encoded documents that are input to the operation find . Our JDBC driver takes documents in that format as string input to its query operations in the JDBC API, converts internally to BSON documents and invokes the operation find on the Mongo DB Java client. The JDBC query operations string input therefore needs to be compliant with Mongo's find operation input parameter specification . Result sets Results are returned as JDBC ResultSet , i.e. in a table form. The driver of choice should be capable of transforming internally to this form of results presentation from Mongo's documents format. Row data. Here is the nice part. The Mongo DB JDBC driver we introduced above is capable of a nice trick that can limit greatly your relation to JDBC and keep you more in the real Mongo world. While iterating the result set, and quite frankly slightly abusing the API for java.sql.ResultSet#getObject(int) method, if you pass -100 as argument, you will get the native Mongo document for the current iteration, the result of the MongoCursor #next() method. Standard? No. Convenient? Yes. Of course, the JDBC driver still provides you with the option to stick to the JDBC API to explore a row contents if you wanted that. So you have these two options here to choose from. ResultSetMetadata. A major difference between Mongo DB and relational databases is that Mongo DB is schemaless. Although it is not encouraged, the documents that constitute the model do not necessarily follow the same scheme and their properties may vary. One consequence is that the ResultSetMetadata should be handled with care. Since it is the content that defines the schema, first it is possible to deliver some insight on the ResultSetMetadata only if there are some documents stored, and second the metadata concerning the schema change with every new document so it is completely known only at the end of the iteration of the result set and is valid only until the data changes. The get (int index) methods. Our JDBC driver makes a best effort to return stable value by index, relying on the ordering of the results as provided by Mongo DB. Note that Mongo's documentation states the following: \" Starting in version 2.6, MongoDB actively attempts to preserve the field order in a document. Before version 2.6, MongoDB did not actively preserve the order of the fields in a document. \". The bottom line is that index-based value extraction from a ResultSet row is not working for Mongo DB versions earlier than 2.6, and for 2.6 and newer, the document fields will be as reliably ordered as Mongo DB can do it. These are all important considerations when implementing and using the result sets returned by queries. Provisioning The setup of a Mongo DB data source is no different from what we already did in Part I , so here we shall cut short and focus only on the details that you need to provide. Step 1: Provision JDBC drivers classes Get the JDBC driver source from Github and use Maven to build. Copy the build result in Tomcat\u2019s lib directory. Step 2: Bind a Data Source to JNDI Edit Tomcat\u2019s conf/context.xml to add a resource: <Resource name= \"jdbc/MongoDB\" auth= \"Container\" type= \"javax.sql.DataSource\" driverClassName= \"io.dirigible.mongodb.jdbcMongodbJdbcDriver\" url= \"jdbc:mongodb://127.0.0.1:5432\" username= \"<YOUR_USER_HERE>\" password= \"<YOUR_PASSWORD_HERE>\" /> Note: Remember to change the placeholders in this example with actual values. and are respectively the user name and password for a valid user of the database. Step 3: Configure application reference Add the following init parameter to the bridge servlet in the web.xml <init-param> <param-name> jndiCustomDataSource-mongodb </param-name> <param-value> java:comp/env/jdbc/MongoDB </param-value> </init-param> Step 4: Register the data source Go to Dirigible IDE Preferences, locate Data Sources and create a new one. Fill in the following details in dialog that pops up: Id: mongodb Name: MongoDB Type: JNDI Location: java:comp/env/jdbc/MongoDB Finally, confirm all dialogs. And that\u2019s pretty much it. You should have a new data source by the name mongodb by now. Putting it to use Now that we\u2019ve got a Mongo DB data source in Dirigible, put it to some good use. /* globals $ */ /* eslint-env node, dirigible */ $ . getResponse (). setContentType ( \"text/html; charset=UTF-8\" ); $ . getResponse (). setCharacterEncoding ( \"UTF-8\" ); var out = $ . getResponse (). getWriter (); var ds = $ . getNamedDatasources (). get ( \"mongodb\" ); var conn = ds . getConnection (); try { var stmt = conn . createStatement (); var rs = stmt . executeQuery ( \u2018 { find : \"testCollection\" } \u2019 ); while ( rs . next ()) { var rsDoc = rs . getObject ( - 100 ); for ( var prop in rsDoc ){ out . println ( prop + ': ' + rsDoc [ prop ] + '<br>' ); } } } finally { conn . close (); } out . flush (); out . close (); In this code snippet we have several semantic blocks. First we open a writer to output some data from the service: $ . getResponse (). setContentType ( \"text/html; charset=UTF-8\" ); $ . getResponse (). setCharacterEncoding ( \"UTF-8\" ); var out = $ . getResponse (). getWriter (); Next, we get a connection to the Mongo DB data source that we setup on previous stage: var ds = $ . getNamedDatasources (). get ( \"mongodb\" ); var conn = ds . getConnection (); Then, we create a statement and execute it using the standard JDBC API but the native Mongo DB query language: var stmt = conn . createStatement (); var rs = stmt . executeQuery ( \u2018 { find : \"testCollection\" } \u2019 ); Now we are ready to iterate on the result set and output some results. Note how we use the standard JDBC API for iteration and the little trick that our Mongo DB JDBC driver is capable of with the rs.getObject(-100); statement. Once we get hold of the JSON document for the current iteration we use pure JavaScript and no JDBC to make some use of it: while ( rs . next ()) { var rsDoc = rs . getObject ( - 100 ); for ( var prop in rsDoc ){ out . println ( prop + ': ' + rsDoc [ prop ] + '<br>' ); } } Finally, as good citizens we close all open resource streams.","title":"BYODS (Bring Your Own Data Source) in Dirigible - Part III: MongoDB custom data source"},{"location":"2016/01/21/blogs-dirigible-custom-ds-3/#part-iii-mongodb-custom-data-source","text":"What\u2019s in the box? With a Mongo DB custom data source integrated in Dirigible Database tools, you can explore the related database instance list of collections and examine collection documents: <img src=\"/img/posts/20160121-0/3-0.png\"/> As with any other relational data source, you can also execute queries and updates but hence, using Mongo\u2019s native BSON-based query language: <img src=\"/img/posts/20160121-0/3-1.png\"/> It is integrated also into the InjectedAPI and therefore in your scripting service you can request the data source by its name, get a connection and execute a statement using Mongo\u2019s native query language, and iterate the result set (here, using the JDBC API. Read below for more options): var ds = $ . getNamedDatasources (). get ( \u2018 mongodb \u2019 ); var conn = ds . getConnection (); try { var stmt = conn . createStatement (); var rs = stmt . executeQuery ( \u2018 { \u201c find \u201d : \u201d testCollection \u201d } \u2019 ); while ( rs . next ()) { $ . getResponse (). getWriter (). println ( rs . getString ( 1 ) + ':' + rs . getString ( \u201c name \u201d ) + '<br>' ); } } finally { conn . close (); }","title":"Part III: MongoDB custom data source"},{"location":"2016/01/21/blogs-dirigible-custom-ds-3/#key-design-notes","text":"Onboarding a Mongo DB data source leverages exactly the same integration mechanism in Dirigible as any other (relational) data source. This feature has been discussed in detail in the previous BYODS blog series. The obvious advantage of this approach is that it follows an established path. That simplicity comes at the cost of a few reasonable prerequisites listed below.","title":"Key design notes"},{"location":"2016/01/21/blogs-dirigible-custom-ds-3/#jdbc-api","text":"JDBC is the standard API used by Dirigible internally to integrate data sources and by developers to use them. Therefore, you will need a JDBC compliant driver to provision access to a Mongo database. Its role is to reconcile the conceptual differences between the relational model centric JDBC API and the NoSQL document store world. In Mongo, despite the name \u201c Java driver \u201d that you will find on Mongo DB\u2019s site concerning Java clients, this has nothing to do with JDBC drivers. It is a Java client API. If you look around for available JDBC drivers for Mongo DB, they are not exactly abundant either. What\u2019s more troublesome here is that virtually all available drivers actually try to translate between Mongo DB\u2019s native query language and SQL. While this works perfectly well for us in terms of technical integration, it does not comply with our goal to make Mongo DB\u2019s developers feel at home in Dirigible, because it would be fairly weird for them to write SQL to query a document database. To fill this gap and for the sake of this example we\u2019ve prototyped a driver that can send native queries encoded as BSON to Mongo DB. It is available on Github . The fine print? Once again, this driver is a prototype and as of the time of this writing there\u2019s still nothing comparable (Meaning happily abusing the JDBC API as a standard protocol for Mongo DB but reusing its own query language). Show some love for it and we will further enhance it. The rest of the drivers out there translate to/from SQL, which will work for the InjectedAPI if you are happy with this approach, but not with the Dirigible database tools in the IDE\u2019s Database perspective.","title":"JDBC API"},{"location":"2016/01/21/blogs-dirigible-custom-ds-3/#query-language","text":"In order to execute query or update statements from Dirigible, your back-end needs to be able to interpret a formal language that can be encoded in strings because that\u2019s the input it will get. There are options here, but it would be best to re-use a query language if your database already has one. Developers who are already used to it will feel at home and make the best use of the database capabilities. Other options, but less desirable for the same reasons, are to translate to and from SQL or other suitable language. MongoDB has a concept of query language. Queries are BSON encoded documents that are input to the operation find . Our JDBC driver takes documents in that format as string input to its query operations in the JDBC API, converts internally to BSON documents and invokes the operation find on the Mongo DB Java client. The JDBC query operations string input therefore needs to be compliant with Mongo's find operation input parameter specification .","title":"Query language"},{"location":"2016/01/21/blogs-dirigible-custom-ds-3/#result-sets","text":"Results are returned as JDBC ResultSet , i.e. in a table form. The driver of choice should be capable of transforming internally to this form of results presentation from Mongo's documents format. Row data. Here is the nice part. The Mongo DB JDBC driver we introduced above is capable of a nice trick that can limit greatly your relation to JDBC and keep you more in the real Mongo world. While iterating the result set, and quite frankly slightly abusing the API for java.sql.ResultSet#getObject(int) method, if you pass -100 as argument, you will get the native Mongo document for the current iteration, the result of the MongoCursor #next() method. Standard? No. Convenient? Yes. Of course, the JDBC driver still provides you with the option to stick to the JDBC API to explore a row contents if you wanted that. So you have these two options here to choose from. ResultSetMetadata. A major difference between Mongo DB and relational databases is that Mongo DB is schemaless. Although it is not encouraged, the documents that constitute the model do not necessarily follow the same scheme and their properties may vary. One consequence is that the ResultSetMetadata should be handled with care. Since it is the content that defines the schema, first it is possible to deliver some insight on the ResultSetMetadata only if there are some documents stored, and second the metadata concerning the schema change with every new document so it is completely known only at the end of the iteration of the result set and is valid only until the data changes. The get (int index) methods. Our JDBC driver makes a best effort to return stable value by index, relying on the ordering of the results as provided by Mongo DB. Note that Mongo's documentation states the following: \" Starting in version 2.6, MongoDB actively attempts to preserve the field order in a document. Before version 2.6, MongoDB did not actively preserve the order of the fields in a document. \". The bottom line is that index-based value extraction from a ResultSet row is not working for Mongo DB versions earlier than 2.6, and for 2.6 and newer, the document fields will be as reliably ordered as Mongo DB can do it. These are all important considerations when implementing and using the result sets returned by queries.","title":"Result sets"},{"location":"2016/01/21/blogs-dirigible-custom-ds-3/#provisioning","text":"The setup of a Mongo DB data source is no different from what we already did in Part I , so here we shall cut short and focus only on the details that you need to provide.","title":"Provisioning"},{"location":"2016/01/21/blogs-dirigible-custom-ds-3/#step-1-provision-jdbc-drivers-classes","text":"Get the JDBC driver source from Github and use Maven to build. Copy the build result in Tomcat\u2019s lib directory.","title":"Step 1: Provision JDBC drivers classes"},{"location":"2016/01/21/blogs-dirigible-custom-ds-3/#step-2-bind-a-data-source-to-jndi","text":"Edit Tomcat\u2019s conf/context.xml to add a resource: <Resource name= \"jdbc/MongoDB\" auth= \"Container\" type= \"javax.sql.DataSource\" driverClassName= \"io.dirigible.mongodb.jdbcMongodbJdbcDriver\" url= \"jdbc:mongodb://127.0.0.1:5432\" username= \"<YOUR_USER_HERE>\" password= \"<YOUR_PASSWORD_HERE>\" /> Note: Remember to change the placeholders in this example with actual values. and are respectively the user name and password for a valid user of the database.","title":"Step 2: Bind a Data Source to JNDI"},{"location":"2016/01/21/blogs-dirigible-custom-ds-3/#step-3-configure-application-reference","text":"Add the following init parameter to the bridge servlet in the web.xml <init-param> <param-name> jndiCustomDataSource-mongodb </param-name> <param-value> java:comp/env/jdbc/MongoDB </param-value> </init-param>","title":"Step 3: Configure application reference"},{"location":"2016/01/21/blogs-dirigible-custom-ds-3/#step-4-register-the-data-source","text":"Go to Dirigible IDE Preferences, locate Data Sources and create a new one. Fill in the following details in dialog that pops up: Id: mongodb Name: MongoDB Type: JNDI Location: java:comp/env/jdbc/MongoDB Finally, confirm all dialogs. And that\u2019s pretty much it. You should have a new data source by the name mongodb by now.","title":"Step 4: Register the data source"},{"location":"2016/01/21/blogs-dirigible-custom-ds-3/#putting-it-to-use","text":"Now that we\u2019ve got a Mongo DB data source in Dirigible, put it to some good use. /* globals $ */ /* eslint-env node, dirigible */ $ . getResponse (). setContentType ( \"text/html; charset=UTF-8\" ); $ . getResponse (). setCharacterEncoding ( \"UTF-8\" ); var out = $ . getResponse (). getWriter (); var ds = $ . getNamedDatasources (). get ( \"mongodb\" ); var conn = ds . getConnection (); try { var stmt = conn . createStatement (); var rs = stmt . executeQuery ( \u2018 { find : \"testCollection\" } \u2019 ); while ( rs . next ()) { var rsDoc = rs . getObject ( - 100 ); for ( var prop in rsDoc ){ out . println ( prop + ': ' + rsDoc [ prop ] + '<br>' ); } } } finally { conn . close (); } out . flush (); out . close (); In this code snippet we have several semantic blocks. First we open a writer to output some data from the service: $ . getResponse (). setContentType ( \"text/html; charset=UTF-8\" ); $ . getResponse (). setCharacterEncoding ( \"UTF-8\" ); var out = $ . getResponse (). getWriter (); Next, we get a connection to the Mongo DB data source that we setup on previous stage: var ds = $ . getNamedDatasources (). get ( \"mongodb\" ); var conn = ds . getConnection (); Then, we create a statement and execute it using the standard JDBC API but the native Mongo DB query language: var stmt = conn . createStatement (); var rs = stmt . executeQuery ( \u2018 { find : \"testCollection\" } \u2019 ); Now we are ready to iterate on the result set and output some results. Note how we use the standard JDBC API for iteration and the little trick that our Mongo DB JDBC driver is capable of with the rs.getObject(-100); statement. Once we get hold of the JSON document for the current iteration we use pure JavaScript and no JDBC to make some use of it: while ( rs . next ()) { var rsDoc = rs . getObject ( - 100 ); for ( var prop in rsDoc ){ out . println ( prop + ': ' + rsDoc [ prop ] + '<br>' ); } } Finally, as good citizens we close all open resource streams.","title":"Putting it to use"},{"location":"2016/01/21/blogs-repositories/","text":"What exactly the term Repository means in the context of Dirigible? How it is related to my projects' life-cycle management? Is there a benefit to have the whole content in a single place? What is the difference between the Local and Master Repositories, and when to use them? The Repository In Dirigible all projects artefacts, including the configurations are stored in a single file system like structure. This content can be easily transported from one instance to another as an archive file in ZIP format, using import/export capabilities. In addition there is a secured RESTful service, which can be used for external updates of the repository content from outside remotely via HTTP. Local Repository The Repository running in a given instance of Dirigible is often referred as 'Local Repository' and plays the role of an operational source code storage. The analogy is the workspace directory in many desktop IDEs, such as Eclipse. When you create a project with files and folders inside, they are immediately reflected in the Local Repository. Having in mind this, the Local Repository has to be above all - fast. The default implementation used in Dirigible is File System based one. It means that Dirigible stores the projects artefacts in some root directory (default or pre-configured) on the very same instance where it runs. There is also a relational database backed implementation for a Local Repository, which can be useful in one of the deployment options described later. Master Repository Another kind of Repository is the so called Master Repository. This is an immutable (read-only) content provider compliant with the IRepository API. The role of this repository is to supply the initial content comprising configurations and public registry (and also workspaces if any) during the bootstrap step of Dirigble's instance. This is actually a pull transfer, where the Dirigible is the active party. This approach gives the flexibility the Master Repository itself (as a passive party) to be implemented based on file system, Git repository or any relational database. Deployment Options Single Persistent Instance This is the simplest option from the operations point of view deployment option. It includes a VM with persistent state, Java Web Container (e.g. Tomcat) and Dirigible \"All-In-One\" product (WAR file) deployed on it. It uses a file system based Local Repository and no Master Repository is configured in this case. This option applies to trial and local development scenarios. Still interaction with Git, as well as import/export functions are available and can be used. Single non-Persistence Instance In this case we have a VM which doesn't keep its state after restart. We either have to mount a persistent file system to the image and configure a Local Repository, or we have to use a RDBMS based Local Repository connected to a remote (most likely \"managed\") relational database. Multiple non-Persistence Instances In this case we have multiple transient VMs connected via the RDBMS based Local Repository to a single database schema. Changes to the content (e.g. during development) can be made from any of the VM instances. After the start of a new VM image it gets the most recent content like all the others in the pool and can be used right away for development or production. The load optimisation here is based only on the in-memory cache built-in the Local Repository. Single Master Instance and Multiple Slave Instances If you want to have immutable production instances and a single or a few instances for development or support, you can choose this option. In this case the \"development/support\" instance(s) have direct connection (their Local Repository) to the \"master\" database schema or the \"master\" root directory (in case of a shared file system). All the other \"production\" instances have configured a Master Repository to the \"master\" source on one hand and a Local Repository configured as local file system based one on the other hand. This option gives the flexibility to have secure \"production\" instances where nobody has even theoretical possibility to break the \"master\" code base. At the same time to have a special instance(s) can be still accessed by different network access rules (e.g. internal access) for quick debugging and bug-fixing on the fly and on the very same environment. Multiple Instances with Git Another option similar to the previous one, is that you can configure the Master Repository to retrieve the content from a Git repository. This can be useful for \"production-only\u201d instances, which have to be immutable on one hand, and versioned on the other. Configurations Configurations parameters for the Repository components can be provided either as initial parameters for the DirigibleBridge servlet in the web.xml or as environment variables. For example to enable the default file-based Local Repository you can use the following snippet: <!-- Default Repository Provider --> <init-param> <param-name> repositoryProvider </param-name> <param-value> org.eclipse.dirigible.repository.local.LocalRepositoryProvider </param-value> </init-param> In case you want to use the database Local Repository you can use the following provider instead: org . eclipse . dirigible . repository . db . DBRepositoryProvider The corresponding database Master Repository can be enabled by: <!-- Default Repository Provider Master (used for Initial Load or Reset) --> <init-param> <param-name> repositoryProviderMaster </param-name> <param-value> org.eclipse.dirigible.repository.db.DBMasterRepositoryProvider </param-value> </init-param> and with Git-based Mater Repository: <!-- Default Repository Provider Master (used for Initial Load or Reset) --> <init-param> <param-name> repositoryProviderMaster </param-name> <param-value> org.eclipse.dirigible.repository.db.GitMasterRepositoryProvider </param-value> </init-param> <!-- Master Repository parameters - Git based --> <init-param> <param-name> masterRepositoryGitTarget </param-name> <param-value> master_git_repository </param-value> </init-param> <init-param> <param-name> masterRepositoryGitLocation </param-name> <param-value> https://xxx </param-value> </init-param> <init-param> <param-name> masterRepositoryGitUser </param-name> <param-value> {git.user} </param-value> </init-param> <init-param> <param-name> masterRepositoryGitPassword </param-name> <param-value> {git.password} </param-value> </init-param> <init-param> <param-name> masterRepositoryGitBranch </param-name> <param-value> {git.branch} </param-value> </init-param> Outlook Following the concepts of Repositories - Local and Master ones, the obvious path ahead is implementation of more production-ready connectors to different data storages e.g. NoSQL such as MongoDB or OrientDB, Cloud storage services e.g. Amazon S3, Google Cloud Storage, etc. Of course, this leaves somehow the responsibility of the security, integrity, high-availability, disaster recovery and the other important capabilities of the content Repository to the low level implementation of the data storage, but at the end this is how it should be, isn't it?","title":"Developer - Repositories, repositories, repositories..."},{"location":"2016/01/21/blogs-repositories/#the-repository","text":"In Dirigible all projects artefacts, including the configurations are stored in a single file system like structure. This content can be easily transported from one instance to another as an archive file in ZIP format, using import/export capabilities. In addition there is a secured RESTful service, which can be used for external updates of the repository content from outside remotely via HTTP.","title":"The Repository"},{"location":"2016/01/21/blogs-repositories/#local-repository","text":"The Repository running in a given instance of Dirigible is often referred as 'Local Repository' and plays the role of an operational source code storage. The analogy is the workspace directory in many desktop IDEs, such as Eclipse. When you create a project with files and folders inside, they are immediately reflected in the Local Repository. Having in mind this, the Local Repository has to be above all - fast. The default implementation used in Dirigible is File System based one. It means that Dirigible stores the projects artefacts in some root directory (default or pre-configured) on the very same instance where it runs. There is also a relational database backed implementation for a Local Repository, which can be useful in one of the deployment options described later.","title":"Local Repository"},{"location":"2016/01/21/blogs-repositories/#master-repository","text":"Another kind of Repository is the so called Master Repository. This is an immutable (read-only) content provider compliant with the IRepository API. The role of this repository is to supply the initial content comprising configurations and public registry (and also workspaces if any) during the bootstrap step of Dirigble's instance. This is actually a pull transfer, where the Dirigible is the active party. This approach gives the flexibility the Master Repository itself (as a passive party) to be implemented based on file system, Git repository or any relational database.","title":"Master Repository"},{"location":"2016/01/21/blogs-repositories/#deployment-options","text":"","title":"Deployment Options"},{"location":"2016/01/21/blogs-repositories/#single-persistent-instance","text":"This is the simplest option from the operations point of view deployment option. It includes a VM with persistent state, Java Web Container (e.g. Tomcat) and Dirigible \"All-In-One\" product (WAR file) deployed on it. It uses a file system based Local Repository and no Master Repository is configured in this case. This option applies to trial and local development scenarios. Still interaction with Git, as well as import/export functions are available and can be used.","title":"Single Persistent Instance"},{"location":"2016/01/21/blogs-repositories/#single-non-persistence-instance","text":"In this case we have a VM which doesn't keep its state after restart. We either have to mount a persistent file system to the image and configure a Local Repository, or we have to use a RDBMS based Local Repository connected to a remote (most likely \"managed\") relational database.","title":"Single non-Persistence Instance"},{"location":"2016/01/21/blogs-repositories/#multiple-non-persistence-instances","text":"In this case we have multiple transient VMs connected via the RDBMS based Local Repository to a single database schema. Changes to the content (e.g. during development) can be made from any of the VM instances. After the start of a new VM image it gets the most recent content like all the others in the pool and can be used right away for development or production. The load optimisation here is based only on the in-memory cache built-in the Local Repository.","title":"Multiple non-Persistence Instances"},{"location":"2016/01/21/blogs-repositories/#single-master-instance-and-multiple-slave-instances","text":"If you want to have immutable production instances and a single or a few instances for development or support, you can choose this option. In this case the \"development/support\" instance(s) have direct connection (their Local Repository) to the \"master\" database schema or the \"master\" root directory (in case of a shared file system). All the other \"production\" instances have configured a Master Repository to the \"master\" source on one hand and a Local Repository configured as local file system based one on the other hand. This option gives the flexibility to have secure \"production\" instances where nobody has even theoretical possibility to break the \"master\" code base. At the same time to have a special instance(s) can be still accessed by different network access rules (e.g. internal access) for quick debugging and bug-fixing on the fly and on the very same environment.","title":"Single Master Instance and Multiple Slave Instances"},{"location":"2016/01/21/blogs-repositories/#multiple-instances-with-git","text":"Another option similar to the previous one, is that you can configure the Master Repository to retrieve the content from a Git repository. This can be useful for \"production-only\u201d instances, which have to be immutable on one hand, and versioned on the other.","title":"Multiple Instances with Git"},{"location":"2016/01/21/blogs-repositories/#configurations","text":"Configurations parameters for the Repository components can be provided either as initial parameters for the DirigibleBridge servlet in the web.xml or as environment variables. For example to enable the default file-based Local Repository you can use the following snippet: <!-- Default Repository Provider --> <init-param> <param-name> repositoryProvider </param-name> <param-value> org.eclipse.dirigible.repository.local.LocalRepositoryProvider </param-value> </init-param> In case you want to use the database Local Repository you can use the following provider instead: org . eclipse . dirigible . repository . db . DBRepositoryProvider The corresponding database Master Repository can be enabled by: <!-- Default Repository Provider Master (used for Initial Load or Reset) --> <init-param> <param-name> repositoryProviderMaster </param-name> <param-value> org.eclipse.dirigible.repository.db.DBMasterRepositoryProvider </param-value> </init-param> and with Git-based Mater Repository: <!-- Default Repository Provider Master (used for Initial Load or Reset) --> <init-param> <param-name> repositoryProviderMaster </param-name> <param-value> org.eclipse.dirigible.repository.db.GitMasterRepositoryProvider </param-value> </init-param> <!-- Master Repository parameters - Git based --> <init-param> <param-name> masterRepositoryGitTarget </param-name> <param-value> master_git_repository </param-value> </init-param> <init-param> <param-name> masterRepositoryGitLocation </param-name> <param-value> https://xxx </param-value> </init-param> <init-param> <param-name> masterRepositoryGitUser </param-name> <param-value> {git.user} </param-value> </init-param> <init-param> <param-name> masterRepositoryGitPassword </param-name> <param-value> {git.password} </param-value> </init-param> <init-param> <param-name> masterRepositoryGitBranch </param-name> <param-value> {git.branch} </param-value> </init-param>","title":"Configurations"},{"location":"2016/01/21/blogs-repositories/#outlook","text":"Following the concepts of Repositories - Local and Master ones, the obvious path ahead is implementation of more production-ready connectors to different data storages e.g. NoSQL such as MongoDB or OrientDB, Cloud storage services e.g. Amazon S3, Google Cloud Storage, etc. Of course, this leaves somehow the responsibility of the security, integrity, high-availability, disaster recovery and the other important capabilities of the content Repository to the low level implementation of the data storage, but at the end this is how it should be, isn't it?","title":"Outlook"},{"location":"2016/02/05/blogs-develop-from-mobile-for-mobile/","text":"or what will be the next big breakthrough in the way native mobile applications are developed. Innovations Before starting with the \"Develop from Mobile for Mobile\" topic, let's say few words about the innovations. There are two types of innovations: Sustaining innovation Disruptive innovation Sustaining innovation is innovation that leads to improvement of an existing technology, product or service. Example of sustaining innovation is the electrical bulb - the evolution from the incandescent bulb to an energy saving bulb and then to the LED bulb. Through that evolution process the function of the electrical bulb didn't change, it became more energy efficient, emitting more light and having longer life. This sustaining innovation improved the existing \"bulb business\", but didn't create new business opportunities. The second type of innovations - the disruptive innovation , is so powerful that it can develop new markets, re-shape existing ones, create industries that did not exist before and have an enormous effect over the \"way the things are done\" . Examples for innovations of such scale are: Ford model T - the first serial produced automobile for mass consumption Cellular phones - Nokia gives to the market the cellular phones that soon replaced the fixed line telephones iPhone - wipe out the regular cellular phones from the market and open the doors for the smart phones Netflix and iTunes - drives out of the business most video and audio stores around the globe airbnb and booking.com - re-shapes the market for the tourist and the hotel industries Almost all industries and lines of business have suffered disruptive innovations based on software through the last 10 years. Overview It is time to go back to our topic and \"Develop from Mobile for Mobile\" . What do we mean by this and how the native mobile application development is related in the context of the Dirigible? First of all, let's introduce Tabris.js . It is a mobile framework that allows you to develop native iOS and Android mobile applications, written entirely in JavaScript. This framework is the right choice when native performance, native look and feel and single code-base (JavaScript) is wanted. Last but not least, it is possible to use existing JavaScript libraries and native extensions to extend the core functionality, when needed. Unlike other frameworks that use webviews or cross-platform intermediate runtimes, Tabris.js executes the JavaScript directly on the device and renders everything using native widgets. Thanks to the framework capabilities, the developers now can focus more on the mobile application development and less on the platform specifics (iOS and Android). Demo Now let's see how you can \"Develop from Mobile for Mobile\" with Dirigible. Prerequisites Have installed the tabris mobile client on a iOS or Android device from here Have account at https://tabrisjs.com (not mandatory) Have deployed your own instance of Dirigible as described here (not mandatory) Steps Launch your own Dirigible instance or use the free trial . From the home screen click the \"Develop\" tile and then launch the IDE from the \"Web IDE\" tile. Close the \"Get Started\" wizard if you don't have projects in the Dirigible instance. Create new project. Right click->New->Project . Give it some name (in the demo, for name is used \"tabris\"). From the list of available project templates select \"Blank Application\" . Create new \"Hello World\" native mobile application. Right click on the project->New->Mobile App . From the list of available templates select \"Tabris.js Hello World\" . Expand your project and navigate to the \"package.json\" file under the \"MobileApplications\" folder. Select the file and open the \"Preview\" tab. Copy from the \"Preview\" tab the URL to the application. (Optional) Login into your https://tabrisjs.com account and select the \"My Scripts\" tab. From there \"Link Script\" that we've created with the Dirigible. Open the \"Tabris.js\" mobile client from your device. If you've linked the script from your tabris.js account, the application can be found under the \"MY SCRIPTS\" tab, if not, then type the URL in the \"URL\" tab. Now let's take the most from the \"In-System Development\" concept and apply it on the native mobile application. Switch back to the Dirigible IDE and update the application. Back to the device, it is time to refresh the content of the application. Whoah, that is a real \"zero time to market\" . The changes were applied immediately and the content of application was updated. But what abot the \"Develop from Mobile for Mobile\" concept and more precisely the first part of the moto \"Develop from Mobile...\" ? While you are on the device, luanch the web browser and open the \"Dirigible Registry\" (the home screen). Click on the \"Develop\" tile, on the next page select the \"Light IDE\" and you are ready to go. Navigate down to the application sources and apply some changes. Hit the \"Publish\" button, so the applied changes will be available immediately. Convince yourself, that the \"Develop from Mobile for Mobile\" is real, available right now, applicable and easy to use. Conclusion We've seen that a cross platform (iOS and Android) native mobile applications can be developed only with the help of a web browser and nothing more. The integration between Dirigible and Tabris.js enables the developers to reach all their customers and make changes to the product with a \"zero time to market\" speed. The bottom line is this: \"By teaching ourselves to look through the lens of the theory to the future, we can actually see the future very clearly!\" Additional Resources What is a \"Disruptive Innovation\" by Clayton Christensen","title":"Develop from Mobile for Mobile"},{"location":"2016/02/05/blogs-develop-from-mobile-for-mobile/#innovations","text":"Before starting with the \"Develop from Mobile for Mobile\" topic, let's say few words about the innovations. There are two types of innovations: Sustaining innovation Disruptive innovation Sustaining innovation is innovation that leads to improvement of an existing technology, product or service. Example of sustaining innovation is the electrical bulb - the evolution from the incandescent bulb to an energy saving bulb and then to the LED bulb. Through that evolution process the function of the electrical bulb didn't change, it became more energy efficient, emitting more light and having longer life. This sustaining innovation improved the existing \"bulb business\", but didn't create new business opportunities. The second type of innovations - the disruptive innovation , is so powerful that it can develop new markets, re-shape existing ones, create industries that did not exist before and have an enormous effect over the \"way the things are done\" . Examples for innovations of such scale are: Ford model T - the first serial produced automobile for mass consumption Cellular phones - Nokia gives to the market the cellular phones that soon replaced the fixed line telephones iPhone - wipe out the regular cellular phones from the market and open the doors for the smart phones Netflix and iTunes - drives out of the business most video and audio stores around the globe airbnb and booking.com - re-shapes the market for the tourist and the hotel industries Almost all industries and lines of business have suffered disruptive innovations based on software through the last 10 years.","title":"Innovations"},{"location":"2016/02/05/blogs-develop-from-mobile-for-mobile/#overview","text":"It is time to go back to our topic and \"Develop from Mobile for Mobile\" . What do we mean by this and how the native mobile application development is related in the context of the Dirigible? First of all, let's introduce Tabris.js . It is a mobile framework that allows you to develop native iOS and Android mobile applications, written entirely in JavaScript. This framework is the right choice when native performance, native look and feel and single code-base (JavaScript) is wanted. Last but not least, it is possible to use existing JavaScript libraries and native extensions to extend the core functionality, when needed. Unlike other frameworks that use webviews or cross-platform intermediate runtimes, Tabris.js executes the JavaScript directly on the device and renders everything using native widgets. Thanks to the framework capabilities, the developers now can focus more on the mobile application development and less on the platform specifics (iOS and Android).","title":"Overview"},{"location":"2016/02/05/blogs-develop-from-mobile-for-mobile/#demo","text":"Now let's see how you can \"Develop from Mobile for Mobile\" with Dirigible.","title":"Demo"},{"location":"2016/02/05/blogs-develop-from-mobile-for-mobile/#prerequisites","text":"Have installed the tabris mobile client on a iOS or Android device from here Have account at https://tabrisjs.com (not mandatory) Have deployed your own instance of Dirigible as described here (not mandatory)","title":"Prerequisites"},{"location":"2016/02/05/blogs-develop-from-mobile-for-mobile/#steps","text":"Launch your own Dirigible instance or use the free trial . From the home screen click the \"Develop\" tile and then launch the IDE from the \"Web IDE\" tile. Close the \"Get Started\" wizard if you don't have projects in the Dirigible instance. Create new project. Right click->New->Project . Give it some name (in the demo, for name is used \"tabris\"). From the list of available project templates select \"Blank Application\" . Create new \"Hello World\" native mobile application. Right click on the project->New->Mobile App . From the list of available templates select \"Tabris.js Hello World\" . Expand your project and navigate to the \"package.json\" file under the \"MobileApplications\" folder. Select the file and open the \"Preview\" tab. Copy from the \"Preview\" tab the URL to the application. (Optional) Login into your https://tabrisjs.com account and select the \"My Scripts\" tab. From there \"Link Script\" that we've created with the Dirigible. Open the \"Tabris.js\" mobile client from your device. If you've linked the script from your tabris.js account, the application can be found under the \"MY SCRIPTS\" tab, if not, then type the URL in the \"URL\" tab. Now let's take the most from the \"In-System Development\" concept and apply it on the native mobile application. Switch back to the Dirigible IDE and update the application. Back to the device, it is time to refresh the content of the application. Whoah, that is a real \"zero time to market\" . The changes were applied immediately and the content of application was updated. But what abot the \"Develop from Mobile for Mobile\" concept and more precisely the first part of the moto \"Develop from Mobile...\" ? While you are on the device, luanch the web browser and open the \"Dirigible Registry\" (the home screen). Click on the \"Develop\" tile, on the next page select the \"Light IDE\" and you are ready to go. Navigate down to the application sources and apply some changes. Hit the \"Publish\" button, so the applied changes will be available immediately. Convince yourself, that the \"Develop from Mobile for Mobile\" is real, available right now, applicable and easy to use.","title":"Steps"},{"location":"2016/02/05/blogs-develop-from-mobile-for-mobile/#conclusion","text":"We've seen that a cross platform (iOS and Android) native mobile applications can be developed only with the help of a web browser and nothing more. The integration between Dirigible and Tabris.js enables the developers to reach all their customers and make changes to the product with a \"zero time to market\" speed. The bottom line is this: \"By teaching ourselves to look through the lens of the theory to the future, we can actually see the future very clearly!\"","title":"Conclusion"},{"location":"2016/02/05/blogs-develop-from-mobile-for-mobile/#additional-resources","text":"What is a \"Disruptive Innovation\" by Clayton Christensen","title":"Additional Resources"},{"location":"2016/02/26/blogs-understanding-dirigible/","text":"During the past couple of years Dirigible evolved from an RAP based Web IDE for simplification and adaptation of SOAP based Web services to a full fledged Dev Platform with its own yet unique to some extents architecture and features. Besides the main driving principles like In-System Development Model, Dynamic Alteration, Content Centric LM, Toolset Completeness, Vertical Scenarios Coverage and the other concepts that can be found easily at the front pages at the Web site, here we will give you more detailed insights of how Dirigible compares to the other similar frameworks and platforms. We will explain the current focus, priorities and future vision. Blocking vs. non-Blocking There are tons of discussions about the significant improvements in the performance, using the non-blocking a.k.a asynchronous programming model. It is quite successfully used in Node.js, Netty and other frameworks. We believe it is useful for many specific scenarios, especially for long running tasks and complex event processing cases. On the other hand, the main target application archetype for Dirigible is the one, that consists of a database backed RESTful services exposed to the end user via HTML5 interface. This type of applications aim at managing the entities from a business derived domain model. In this case the \"traditional\" i.e. synchronous structuring of the source code is much easier for Java, PHP, ABAP programmers and the all others with the similar background. Following this de-facto standard development style of writing business applications, we decided not to teach you how to develop in a new way, but simply to stick to the synchronous model - no matter that the default language in Dirigible is JavaScript. It could be strange to write JavaScript services in the \"Java\" way, but this is the case in Dirigible and will remain in the future as a primary target. All the current Injected APIs are synchronous. Events and Flows Following the above statements, in Dirigible we highly encourage the you to use the declarative Flows services to achieve the non-blocking processing of the long running tasks. The underlying flow engine will be responsible for the distribution and parallelization of the execution of the given task in the best possible way. Of course, you can code such asynchronous algorithms in JavaScript by using callbacks, or any other scripting language by using its own capabilities, but in this case the responsibility of the optimization, debugging and bug-fixing remains entirely yours. Dependency Management There are plenty of package management tools and dependency management descriptor files out there. Starting with the fact that the major Linux distributions use their own package managers such as APT, RPM, YUM, Zipper, throughout the language specific \"default\" descriptors such as MANIFEST.MF, package.json, gemspec, etc. and even the build tools dependency descriptors such as pom.xml, build.xml, bower.json, etc. it seems that there is no a silver bullet solution for this problem... obviously. What is for sure - the problem is complex and as much as one tries to fix it completely with very descriptive manifest files and complicated algorithms for strict dependency resolutions, the outcome is that the manifest files become too hard to be maintained. This leads to incorrect content and at the end leads to the fact that it is quite difficult to have such systems, based on such strict dependencies up and running at all. The \"global install\" has its drawbacks e.g. with version conflicts via transitive dependencies, but to go to the 'npm' style that leads to duplications is simply unacceptable. Another unacceptable decision is the strongest OSGi approach, which could prevent a plugin or even the whole application from starting, if there is an unsatisfied dependencies issues. The standard Java way so far, with no explicit dependencies check at all at run time, beats them all. Yes, you ever face such a problem, it will happen at the worst possible time - when somebody actually start using the chain, which has a dependency issue - Class Not Found case. In the same time you can work with all the other chains, which do not have any issue. In theory this is wrong because it is not perfect, but in the reality this is the only working model for the large scale applications combining huge set of components contributed by the teams that are distributed and diverse. Hence, in Dirigible we choose the \"non-blocking\" approach - the dependencies declarations are only to help you to navigate and pull the right components, without stopping you if a single dependency is not present at the moment. JavaScript vs. Java vs. ? The scripting languages are the ones chosen in Dirigible by several reasons. The project's ultimate driving force - shortest development turn-around time, requires the time between \"coding\" and \"testing\" to be zero or near to zero. Complex and time consuming build and deploy processes are just unacceptable. Fact. Scripting languages perfectly matches in this case and the whole architecture of Dirigible is built around this concept. Why at the same time do we support Java? The simple answer is - as an arbitrary scripting language. The detailed one - with in-memory compilation. Why JavaScript is the \"chosen\" one? First class citizen in the scripting languages group, widely used already for client and server side components, etc. Also important thing, which to some extents depends on the same factors, there is already available comprehensive source code web based editors, with highlighting, code analysis and code completion - such as Orion. Domain Driven Design vs. Model Driven Architecture Domain Driven Design (DDD) is the natural choice of what Dirigible aims to provide - the dev platform for business services. The starting point of the development of a business application is the definition of the domain model entities. At this phase nothing else matters - only the players and theirs interactions. The ultimate goal of any toolkit is after this design and definition phase to generate and run a full-fledged application auto-magically. We are not there yet, although you can in just a couple of seconds expose your entity from a database table, thru the RESTful service with pattern-based HTML5 user interface. But the important point here is that we know this kind of automatic generation is just to have a scaffold as a preview quickly. In this way we can have an idea how the real application will look like eventually, when it is ready. The real work on customizations based on the consumer's requirements just begins. What happens if you have to change the model, but you have already made lots of changes in the generated code? Bad news, you have to do them again after the generation of the new artefacts or you can skip the generation step and go and apply the needed modifications derived from the model changes on your own. Can Model Driven Architecture (MDA) help here? If yes, do we see MDA as an essential part of Dirigible? In short - yes for both. Sometimes the \"preview\" state is good enough to be used \"in the time being\". But we have to be clear here, such a \"magic\" that can solve this problem completely - does not exist. MDA approach comes with performance degradation it is never optimized enough for your specific case. User interface is never fancy enough nor extensible enough. The behavior that comes from the MDA framework doesn't necessarily match your need, but it is hard or impossible to change. Hence, in Dirigible we see DDD with one-time generation as a primary approach and MDA as just an option. Microservices vs. Monoliths There is a big noise related to the Microservices concepts although they are neither new nor unknown in the technology space until now. How do they reflect on Dirigible? How can we build a business application in the Cloud following the Microservices architecture? First of all it is possible to do this. What\u2019s more, in Dirigible this style of componentization is even kind of enforced. But here comes the major difference - in Dirigible we leverage an unified platform, which the services can run on - the Dev Platform. Hence, whether you decide to divide your components to run on separate instances (to scale separately) or to have them all in a single instance - this is entirely your choice. We keep the unified approach, because in the most of the cases the performance of the local communication channel between the components, for example, is the only acceptable choice. To be fair here, the Microservices architecture does not come because it is the best option - that is because this is an approach to solve the current situation, where many different components are written in different languages, run on different platforms, hence the unification for the deployment of all these can be done only at a very low level - OS, VM, containers and the communication channel can be established on a very high (and expensive) level - TCP/HTTP/File System. Roles Separation vs. One Man Army Depending on the project scale, there is a common suggestion, which constantly appears - separation of roles. This leads to the implied conclusion that the different roles (personas) mean different persons. And this, on the other hand, means that the different persons can use different tools during the development process of a single solution, doesn't it? In Dirigible we take this very seriously - we strive to provide the full set of tools as well as runtime foundation required by all the roles concerning a given project. Whether you will decide to bring the whole crew to work on the project or you will do it alone - it is your decision - you can do both. Dirigible claims to cover all your needs, with the appropriate tooling and runtime engines to develop your next generation business application. Open Source vs. Proprietary Dirigible is an open source project. It is based on a huge set of the open source frameworks. If there weren't such open source methodologies and initiatives, our world would never be the same, that's for sure. Dirigible would have never appeared. The collective intelligence - the major benefit of the open source, proved already many times that it can beat any other proprietary yet closed way of innovations. This leads us to the natural evolution of the Dev Platform concept - Dirigible to be used as the unified foundation for open source business services and utilities. Stay tuned for the exiting news in the next couple of weeks. Enjoy!","title":"Understanding Dirigible"},{"location":"2016/02/26/blogs-understanding-dirigible/#blocking-vs-non-blocking","text":"There are tons of discussions about the significant improvements in the performance, using the non-blocking a.k.a asynchronous programming model. It is quite successfully used in Node.js, Netty and other frameworks. We believe it is useful for many specific scenarios, especially for long running tasks and complex event processing cases. On the other hand, the main target application archetype for Dirigible is the one, that consists of a database backed RESTful services exposed to the end user via HTML5 interface. This type of applications aim at managing the entities from a business derived domain model. In this case the \"traditional\" i.e. synchronous structuring of the source code is much easier for Java, PHP, ABAP programmers and the all others with the similar background. Following this de-facto standard development style of writing business applications, we decided not to teach you how to develop in a new way, but simply to stick to the synchronous model - no matter that the default language in Dirigible is JavaScript. It could be strange to write JavaScript services in the \"Java\" way, but this is the case in Dirigible and will remain in the future as a primary target. All the current Injected APIs are synchronous.","title":"Blocking vs. non-Blocking"},{"location":"2016/02/26/blogs-understanding-dirigible/#events-and-flows","text":"Following the above statements, in Dirigible we highly encourage the you to use the declarative Flows services to achieve the non-blocking processing of the long running tasks. The underlying flow engine will be responsible for the distribution and parallelization of the execution of the given task in the best possible way. Of course, you can code such asynchronous algorithms in JavaScript by using callbacks, or any other scripting language by using its own capabilities, but in this case the responsibility of the optimization, debugging and bug-fixing remains entirely yours.","title":"Events and Flows"},{"location":"2016/02/26/blogs-understanding-dirigible/#dependency-management","text":"There are plenty of package management tools and dependency management descriptor files out there. Starting with the fact that the major Linux distributions use their own package managers such as APT, RPM, YUM, Zipper, throughout the language specific \"default\" descriptors such as MANIFEST.MF, package.json, gemspec, etc. and even the build tools dependency descriptors such as pom.xml, build.xml, bower.json, etc. it seems that there is no a silver bullet solution for this problem... obviously. What is for sure - the problem is complex and as much as one tries to fix it completely with very descriptive manifest files and complicated algorithms for strict dependency resolutions, the outcome is that the manifest files become too hard to be maintained. This leads to incorrect content and at the end leads to the fact that it is quite difficult to have such systems, based on such strict dependencies up and running at all. The \"global install\" has its drawbacks e.g. with version conflicts via transitive dependencies, but to go to the 'npm' style that leads to duplications is simply unacceptable. Another unacceptable decision is the strongest OSGi approach, which could prevent a plugin or even the whole application from starting, if there is an unsatisfied dependencies issues. The standard Java way so far, with no explicit dependencies check at all at run time, beats them all. Yes, you ever face such a problem, it will happen at the worst possible time - when somebody actually start using the chain, which has a dependency issue - Class Not Found case. In the same time you can work with all the other chains, which do not have any issue. In theory this is wrong because it is not perfect, but in the reality this is the only working model for the large scale applications combining huge set of components contributed by the teams that are distributed and diverse. Hence, in Dirigible we choose the \"non-blocking\" approach - the dependencies declarations are only to help you to navigate and pull the right components, without stopping you if a single dependency is not present at the moment.","title":"Dependency Management"},{"location":"2016/02/26/blogs-understanding-dirigible/#javascript-vs-java-vs","text":"The scripting languages are the ones chosen in Dirigible by several reasons. The project's ultimate driving force - shortest development turn-around time, requires the time between \"coding\" and \"testing\" to be zero or near to zero. Complex and time consuming build and deploy processes are just unacceptable. Fact. Scripting languages perfectly matches in this case and the whole architecture of Dirigible is built around this concept. Why at the same time do we support Java? The simple answer is - as an arbitrary scripting language. The detailed one - with in-memory compilation. Why JavaScript is the \"chosen\" one? First class citizen in the scripting languages group, widely used already for client and server side components, etc. Also important thing, which to some extents depends on the same factors, there is already available comprehensive source code web based editors, with highlighting, code analysis and code completion - such as Orion.","title":"JavaScript vs. Java vs. ?"},{"location":"2016/02/26/blogs-understanding-dirigible/#domain-driven-design-vs-model-driven-architecture","text":"Domain Driven Design (DDD) is the natural choice of what Dirigible aims to provide - the dev platform for business services. The starting point of the development of a business application is the definition of the domain model entities. At this phase nothing else matters - only the players and theirs interactions. The ultimate goal of any toolkit is after this design and definition phase to generate and run a full-fledged application auto-magically. We are not there yet, although you can in just a couple of seconds expose your entity from a database table, thru the RESTful service with pattern-based HTML5 user interface. But the important point here is that we know this kind of automatic generation is just to have a scaffold as a preview quickly. In this way we can have an idea how the real application will look like eventually, when it is ready. The real work on customizations based on the consumer's requirements just begins. What happens if you have to change the model, but you have already made lots of changes in the generated code? Bad news, you have to do them again after the generation of the new artefacts or you can skip the generation step and go and apply the needed modifications derived from the model changes on your own. Can Model Driven Architecture (MDA) help here? If yes, do we see MDA as an essential part of Dirigible? In short - yes for both. Sometimes the \"preview\" state is good enough to be used \"in the time being\". But we have to be clear here, such a \"magic\" that can solve this problem completely - does not exist. MDA approach comes with performance degradation it is never optimized enough for your specific case. User interface is never fancy enough nor extensible enough. The behavior that comes from the MDA framework doesn't necessarily match your need, but it is hard or impossible to change. Hence, in Dirigible we see DDD with one-time generation as a primary approach and MDA as just an option.","title":"Domain Driven Design vs. Model Driven Architecture"},{"location":"2016/02/26/blogs-understanding-dirigible/#microservices-vs-monoliths","text":"There is a big noise related to the Microservices concepts although they are neither new nor unknown in the technology space until now. How do they reflect on Dirigible? How can we build a business application in the Cloud following the Microservices architecture? First of all it is possible to do this. What\u2019s more, in Dirigible this style of componentization is even kind of enforced. But here comes the major difference - in Dirigible we leverage an unified platform, which the services can run on - the Dev Platform. Hence, whether you decide to divide your components to run on separate instances (to scale separately) or to have them all in a single instance - this is entirely your choice. We keep the unified approach, because in the most of the cases the performance of the local communication channel between the components, for example, is the only acceptable choice. To be fair here, the Microservices architecture does not come because it is the best option - that is because this is an approach to solve the current situation, where many different components are written in different languages, run on different platforms, hence the unification for the deployment of all these can be done only at a very low level - OS, VM, containers and the communication channel can be established on a very high (and expensive) level - TCP/HTTP/File System.","title":"Microservices vs. Monoliths"},{"location":"2016/02/26/blogs-understanding-dirigible/#roles-separation-vs-one-man-army","text":"Depending on the project scale, there is a common suggestion, which constantly appears - separation of roles. This leads to the implied conclusion that the different roles (personas) mean different persons. And this, on the other hand, means that the different persons can use different tools during the development process of a single solution, doesn't it? In Dirigible we take this very seriously - we strive to provide the full set of tools as well as runtime foundation required by all the roles concerning a given project. Whether you will decide to bring the whole crew to work on the project or you will do it alone - it is your decision - you can do both. Dirigible claims to cover all your needs, with the appropriate tooling and runtime engines to develop your next generation business application.","title":"Roles Separation vs. One Man Army"},{"location":"2016/02/26/blogs-understanding-dirigible/#open-source-vs-proprietary","text":"Dirigible is an open source project. It is based on a huge set of the open source frameworks. If there weren't such open source methodologies and initiatives, our world would never be the same, that's for sure. Dirigible would have never appeared. The collective intelligence - the major benefit of the open source, proved already many times that it can beat any other proprietary yet closed way of innovations. This leads us to the natural evolution of the Dev Platform concept - Dirigible to be used as the unified foundation for open source business services and utilities. Stay tuned for the exiting news in the next couple of weeks. Enjoy!","title":"Open Source vs. Proprietary"},{"location":"2016/05/19/blogs-web-sockets-and-osgi-in-servlet-container/","text":"How to use WebSockets, coming as a standard feature with the modern Servlet Containers (e.g. Tomcat 7.x) from within the embedded Equinox OSGi environment deployed as a WAR application archive? If you haven't asked yourself such a question so far, just forget it and live in peace... But in case you have already quite serious reasons to separate the functionality of your huge and complex application to plugins to be manageable and already have chosen the OSGi way with the Eclipse Equinox implementation and in the same time you want your application in the Web, most probably you already know the nasty issues that appear ones you try something aside from the standard \"ServletBridge\" scenario. Background OK, if you are not aware about the above use-case, but still want to learn what it is about let's start with some prerequisites: What is WebSockets by Matt West: http://blog.teamtreehouse.com/an-introduction-to-websockets What is OSGi in Servlet Container series by Angelo Zerr: https://angelozerr.wordpress.com/2010/08/31/osgi-equinox-in-a-servlet-container-step0/ The Problem Now, assuming you got the idea how the architecture looks like and you are convinced it worths the effort - what is the problem? On one side we have the web application environment, which is as standard as any other web application running on the Tomcat server. You can have there Servelts, WebSockets, etc. You have access to the shared libraries within the Tomcat/lib folder as any other application has. The Solution ClassLoaders visibility The first problem is how to make the WebSockets API classes to be visible at runtime by the OSGi environment? This is configured in the launch.ini file as: 1. OSGi's parent and context class-loaders have to be set to fwk 2. the transitive packages are listed in the extra property osgi.*=@null org.osgi.*=@null eclipse.*=@null osgi.parentClassloader=fwk osgi.contextClassLoaderParent=fwk org.osgi.framework.system.packages.extra=javax.websocket,javax.websocket.server,javax.mail,javax.mail.internet,org.eclipse.dirigible.ide.bridge The actual file can be found here Configure Dependency During the development you will need to include the WebSockets API in the target platform. This is required to develop your server side logic in a plugin against the WebSockets API. You can refer the already available artifact in the Orbit repository here In your *.target file add the following: <location includeAllPlatforms= \"false\" includeConfigurePhase= \"true\" includeMode= \"slicer\" includeSource= \"true\" type= \"InstallableUnit\" > ... <unit id= \"javax.websocket\" version= \"1.0.0.v20140310-1603\" /> <repository location= \"http://download.eclipse.org/tools/orbit/downloads/drops/R20150519210750/repository\" /> </location> The target platform file of Dirigible can be found here After the reloading of the target platform, javax.websocket package is available and can be added to a manifest file of the plugin you want to use for the server-side implementation: Import-Package: ... javax.websocket, javax.websocket.server ... and the corresponding sample from the Dirigible code-base here Important - use package import not plugin dependency as soon as at the runtime the classes will be exposed by the application class-loader not by the OSGi parent class-loader itself. WebSocket Proxy (outside OSGi) We made the necessary configurations, now we can start with the implementation of our WebSocket servlet. Let's create a real-time logging servlet, which can send immediately the log messages to all the clients currently connected to it. First of all we need the standard implementation of a WebSocket outside of the OSGi environment. It will accept the connections from the clients and will play a role of a bridge to the OSGi environment. We can use the standard annotations @ServerEndpoint, @onOpen, @onMessage, @onError and @onClose ... @ServerEndpoint ( \"/log\" ) public class WebSocketLogBridgeServlet { private static final Logger logger = LoggerFactory . getLogger ( WebSocketLogBridgeServlet . class ); private static Map < String , Session > openSessions = new ConcurrentHashMap < String , Session > (); @OnOpen public void onOpen ( Session session ) throws IOException { openSessions . put ( session . getId (), session ); callInternal ( \"onOpen\" , session , null ); } protected void callInternal ( String methodName , Session session , String message ) { logger . debug ( \"Getting internal pair...\" ); Object logInternal = DirigibleBridge . BRIDGES . get ( \"websocket_log_channel_internal\" ); logger . debug ( \"Getting internal pair passed: \" + ( logInternal != null )); if ( logInternal == null ) { String peerError = \"Internal WebSocket peer for Log Service is null.\" ; logger . error ( peerError ); try { session . getBasicRemote (). sendText ( peerError ); } catch ( IOException e ) { logger . error ( e . getMessage (), e ); } return ; } try { Method method = null ; if ( message == null ) { method = logInternal . getClass (). getMethod ( methodName , Session . class ); method . invoke ( logInternal , session ); } else { method = logInternal . getClass (). getMethod ( methodName , String . class , Session . class ); method . invoke ( logInternal , message , session ); } } catch ( NoSuchMethodException e ) { logger . error ( e . getMessage (), e ); } catch ( SecurityException e ) { logger . error ( e . getMessage (), e ); } catch ( IllegalAccessException e ) { logger . error ( e . getMessage (), e ); } catch ( IllegalArgumentException e ) { logger . error ( e . getMessage (), e ); } catch ( InvocationTargetException e ) { logger . error ( e . getMessage (), e ); } } @OnMessage public void onMessage ( String message , Session session ) { callInternal ( \"onMessage\" , session , message ); } @OnError public void onError ( Session session , Throwable t ) { callInternal ( \"onError\" , session , t . getMessage ()); logger . error ( t . getMessage (), t ); } @OnClose public void onClose ( Session session ) { openSessions . remove ( session . getId ()); callInternal ( \"onClose\" , session , null ); } ... full source code here The interesting part here is the BRIDGES map, which contains the already registered bridges coming from the OSGi environment. Object logInternal = DirigibleBridge . BRIDGES . get ( \"websocket_log_channel_internal\" ); The source code of the DirigibleBridge can be found here WebSocket Bridge (inside OSGi) We have already the WebSocket end-point, which will accept the connections and will redirect the corresponding calls to the internal \"bridge\" object. Let's have a look at the bridge implementation itself: ... private static Map < String , Session > openSessions = new ConcurrentHashMap < String , Session > (); @OnOpen public void onOpen ( Session session ) throws IOException { openSessions . put ( session . getId (), session ); session . getBasicRemote (). sendText ( \"[log] open: \" + session . getId ()); logger . debug ( \"[ws:log] onOpen: \" + session . getId ()); } @OnMessage public void onMessage ( String message , Session session ) { logger . debug ( \"[ws:log] onMessage: \" + message ); } @OnError public void onError ( Session session , String error ) { logger . debug ( \"[ws:log] onError: \" + error ); } @OnClose public void onClose ( Session session ) { openSessions . remove ( session . getId ()); logger . debug ( \"[ws:log] onClose: Session \" + session . getId () + \" has ended\" ); } public static void sendText ( String sessionId , String message ) { try { if ( sessionId == null ) { for ( Object element : openSessions . values ()) { Session session = ( Session ) element ; session . getBasicRemote (). sendText ( message ); } } else { openSessions . get ( sessionId ). getBasicRemote (). sendText ( message ); } } catch ( IOException e ) { logger . error ( e . getMessage (), e ); } } @Override public void log ( String level , String message ) { for ( Session session : openSessions . values ()) { try { synchronized ( session ) { session . getBasicRemote (). sendText ( String . format ( \"[%s] %s\" , level , message )); } } catch ( Throwable e ) { // do not log it with the Logger e . printStackTrace (); } } } ... source code here We can use the same WebSockets API classes like javax.websocket.Session and even the annotations for the methods (of course the annotations in the above example are just for clarity as soon as there is no actual processor for them within the OSGi environment). Ones we have the implementation of the internal (bridge) part of the pair, we have to add a registration logic to the plugin activator: public class MetricsActivator implements BundleActivator { private static final Logger logger = Logger . getLogger ( MetricsActivator . class ); WebSocketLogBridgeServletInternal webSocketLogBridgeServletInternal ; @Override public void start ( BundleContext context ) throws Exception { ... setupLogChannel (); } protected void setupLogChannel () { logger . debug ( \"Setting log channel internal ...\" ); webSocketLogBridgeServletInternal = new WebSocketLogBridgeServletInternal (); DirigibleBridge . BRIDGES . put ( \"websocket_log_channel_internal\" , webSocketLogBridgeServletInternal ); Logger . addListener ( webSocketLogBridgeServletInternal ); logger . debug ( \"Log channel internal has been set.\" ); } @Override public void stop ( BundleContext context ) throws Exception { webSocketLogBridgeServletInternal . closeAll (); Logger . removeListener ( webSocketLogBridgeServletInternal ); } } source code here WebSocket Client At this step we are ready with the server side implementation. Let's create a simple user interface in HTML and client-side JavaScript, which will connect to the Log WebSocket Service and will print every single log message to the body of the page: ... < body onload = \"connectToLog()\" > < script > var connectToLog = function () { try { var logSocket = new WebSocket ((( location . protocol === 'https:' ) ? \"wss://\" : \"ws://\" ) + window . location . host + \"/log\" ); } catch ( e ) { document . writeln ( \"<div style='background-color: black; font-family: monospace; color: red'>[\" + new Date (). toISOString () + \"][error]\" + e . message + \"</div>\" ); } logSocket . onmessage = function ( message ) { var color = \"#44EE44\" ; if ( message . data . startsWith ( \"[error]\" )) { color = \"red\" ; } var date = new Date (); var id = date . getTime (); document . writeln ( \"<div id='\" + id + \"' style='background-color: black; font-family: monospace; color: \" + color + \"'>[\" + date . toISOString () + \"]\" + message . data + \"</div>\" ); window . location . hash = \"#\" + id ; }; setInterval ( clear , 60000 ); } var clear = function () { document . body . innerHTML = '' ; document . writeln ( \"<div style='background-color: black; font-family: monospace; color: gray'>[\" + new Date (). toISOString () + \"][clear]...</div>\" ); } </ script > ... source code here The assumption here is that the protocol of the WebSocket connection has the same security level as the page itself http->ws https->wss. On receiving a log message the \"logSocket.onmessage\" function is called. The other function \"clear\" is added just for usability and performance reasons. The above user interface can be used stand-alone or can be embedded in the Registry portal or in the WebIDE. Can it be easier? Yes - with Eclipse Dirigible! The support of WebSockets in Dirigible's Scripting Services is coming with release 2.4 - today! 2.0 compliant API is on place and sample will be provided shortly. Enjoy!","title":"WebSockets and Equinox OSGi in a Servlet Container"},{"location":"2016/05/19/blogs-web-sockets-and-osgi-in-servlet-container/#background","text":"OK, if you are not aware about the above use-case, but still want to learn what it is about let's start with some prerequisites: What is WebSockets by Matt West: http://blog.teamtreehouse.com/an-introduction-to-websockets What is OSGi in Servlet Container series by Angelo Zerr: https://angelozerr.wordpress.com/2010/08/31/osgi-equinox-in-a-servlet-container-step0/","title":"Background"},{"location":"2016/05/19/blogs-web-sockets-and-osgi-in-servlet-container/#the-problem","text":"Now, assuming you got the idea how the architecture looks like and you are convinced it worths the effort - what is the problem? On one side we have the web application environment, which is as standard as any other web application running on the Tomcat server. You can have there Servelts, WebSockets, etc. You have access to the shared libraries within the Tomcat/lib folder as any other application has.","title":"The Problem"},{"location":"2016/05/19/blogs-web-sockets-and-osgi-in-servlet-container/#the-solution","text":"","title":"The Solution"},{"location":"2016/05/19/blogs-web-sockets-and-osgi-in-servlet-container/#classloaders-visibility","text":"The first problem is how to make the WebSockets API classes to be visible at runtime by the OSGi environment? This is configured in the launch.ini file as: 1. OSGi's parent and context class-loaders have to be set to fwk 2. the transitive packages are listed in the extra property osgi.*=@null org.osgi.*=@null eclipse.*=@null osgi.parentClassloader=fwk osgi.contextClassLoaderParent=fwk org.osgi.framework.system.packages.extra=javax.websocket,javax.websocket.server,javax.mail,javax.mail.internet,org.eclipse.dirigible.ide.bridge The actual file can be found here","title":"ClassLoaders visibility"},{"location":"2016/05/19/blogs-web-sockets-and-osgi-in-servlet-container/#configure-dependency","text":"During the development you will need to include the WebSockets API in the target platform. This is required to develop your server side logic in a plugin against the WebSockets API. You can refer the already available artifact in the Orbit repository here In your *.target file add the following: <location includeAllPlatforms= \"false\" includeConfigurePhase= \"true\" includeMode= \"slicer\" includeSource= \"true\" type= \"InstallableUnit\" > ... <unit id= \"javax.websocket\" version= \"1.0.0.v20140310-1603\" /> <repository location= \"http://download.eclipse.org/tools/orbit/downloads/drops/R20150519210750/repository\" /> </location> The target platform file of Dirigible can be found here After the reloading of the target platform, javax.websocket package is available and can be added to a manifest file of the plugin you want to use for the server-side implementation: Import-Package: ... javax.websocket, javax.websocket.server ... and the corresponding sample from the Dirigible code-base here Important - use package import not plugin dependency as soon as at the runtime the classes will be exposed by the application class-loader not by the OSGi parent class-loader itself.","title":"Configure Dependency"},{"location":"2016/05/19/blogs-web-sockets-and-osgi-in-servlet-container/#websocket-proxy-outside-osgi","text":"We made the necessary configurations, now we can start with the implementation of our WebSocket servlet. Let's create a real-time logging servlet, which can send immediately the log messages to all the clients currently connected to it. First of all we need the standard implementation of a WebSocket outside of the OSGi environment. It will accept the connections from the clients and will play a role of a bridge to the OSGi environment. We can use the standard annotations @ServerEndpoint, @onOpen, @onMessage, @onError and @onClose ... @ServerEndpoint ( \"/log\" ) public class WebSocketLogBridgeServlet { private static final Logger logger = LoggerFactory . getLogger ( WebSocketLogBridgeServlet . class ); private static Map < String , Session > openSessions = new ConcurrentHashMap < String , Session > (); @OnOpen public void onOpen ( Session session ) throws IOException { openSessions . put ( session . getId (), session ); callInternal ( \"onOpen\" , session , null ); } protected void callInternal ( String methodName , Session session , String message ) { logger . debug ( \"Getting internal pair...\" ); Object logInternal = DirigibleBridge . BRIDGES . get ( \"websocket_log_channel_internal\" ); logger . debug ( \"Getting internal pair passed: \" + ( logInternal != null )); if ( logInternal == null ) { String peerError = \"Internal WebSocket peer for Log Service is null.\" ; logger . error ( peerError ); try { session . getBasicRemote (). sendText ( peerError ); } catch ( IOException e ) { logger . error ( e . getMessage (), e ); } return ; } try { Method method = null ; if ( message == null ) { method = logInternal . getClass (). getMethod ( methodName , Session . class ); method . invoke ( logInternal , session ); } else { method = logInternal . getClass (). getMethod ( methodName , String . class , Session . class ); method . invoke ( logInternal , message , session ); } } catch ( NoSuchMethodException e ) { logger . error ( e . getMessage (), e ); } catch ( SecurityException e ) { logger . error ( e . getMessage (), e ); } catch ( IllegalAccessException e ) { logger . error ( e . getMessage (), e ); } catch ( IllegalArgumentException e ) { logger . error ( e . getMessage (), e ); } catch ( InvocationTargetException e ) { logger . error ( e . getMessage (), e ); } } @OnMessage public void onMessage ( String message , Session session ) { callInternal ( \"onMessage\" , session , message ); } @OnError public void onError ( Session session , Throwable t ) { callInternal ( \"onError\" , session , t . getMessage ()); logger . error ( t . getMessage (), t ); } @OnClose public void onClose ( Session session ) { openSessions . remove ( session . getId ()); callInternal ( \"onClose\" , session , null ); } ... full source code here The interesting part here is the BRIDGES map, which contains the already registered bridges coming from the OSGi environment. Object logInternal = DirigibleBridge . BRIDGES . get ( \"websocket_log_channel_internal\" ); The source code of the DirigibleBridge can be found here","title":"WebSocket Proxy (outside OSGi)"},{"location":"2016/05/19/blogs-web-sockets-and-osgi-in-servlet-container/#websocket-bridge-inside-osgi","text":"We have already the WebSocket end-point, which will accept the connections and will redirect the corresponding calls to the internal \"bridge\" object. Let's have a look at the bridge implementation itself: ... private static Map < String , Session > openSessions = new ConcurrentHashMap < String , Session > (); @OnOpen public void onOpen ( Session session ) throws IOException { openSessions . put ( session . getId (), session ); session . getBasicRemote (). sendText ( \"[log] open: \" + session . getId ()); logger . debug ( \"[ws:log] onOpen: \" + session . getId ()); } @OnMessage public void onMessage ( String message , Session session ) { logger . debug ( \"[ws:log] onMessage: \" + message ); } @OnError public void onError ( Session session , String error ) { logger . debug ( \"[ws:log] onError: \" + error ); } @OnClose public void onClose ( Session session ) { openSessions . remove ( session . getId ()); logger . debug ( \"[ws:log] onClose: Session \" + session . getId () + \" has ended\" ); } public static void sendText ( String sessionId , String message ) { try { if ( sessionId == null ) { for ( Object element : openSessions . values ()) { Session session = ( Session ) element ; session . getBasicRemote (). sendText ( message ); } } else { openSessions . get ( sessionId ). getBasicRemote (). sendText ( message ); } } catch ( IOException e ) { logger . error ( e . getMessage (), e ); } } @Override public void log ( String level , String message ) { for ( Session session : openSessions . values ()) { try { synchronized ( session ) { session . getBasicRemote (). sendText ( String . format ( \"[%s] %s\" , level , message )); } } catch ( Throwable e ) { // do not log it with the Logger e . printStackTrace (); } } } ... source code here We can use the same WebSockets API classes like javax.websocket.Session and even the annotations for the methods (of course the annotations in the above example are just for clarity as soon as there is no actual processor for them within the OSGi environment). Ones we have the implementation of the internal (bridge) part of the pair, we have to add a registration logic to the plugin activator: public class MetricsActivator implements BundleActivator { private static final Logger logger = Logger . getLogger ( MetricsActivator . class ); WebSocketLogBridgeServletInternal webSocketLogBridgeServletInternal ; @Override public void start ( BundleContext context ) throws Exception { ... setupLogChannel (); } protected void setupLogChannel () { logger . debug ( \"Setting log channel internal ...\" ); webSocketLogBridgeServletInternal = new WebSocketLogBridgeServletInternal (); DirigibleBridge . BRIDGES . put ( \"websocket_log_channel_internal\" , webSocketLogBridgeServletInternal ); Logger . addListener ( webSocketLogBridgeServletInternal ); logger . debug ( \"Log channel internal has been set.\" ); } @Override public void stop ( BundleContext context ) throws Exception { webSocketLogBridgeServletInternal . closeAll (); Logger . removeListener ( webSocketLogBridgeServletInternal ); } } source code here","title":"WebSocket Bridge (inside OSGi)"},{"location":"2016/05/19/blogs-web-sockets-and-osgi-in-servlet-container/#websocket-client","text":"At this step we are ready with the server side implementation. Let's create a simple user interface in HTML and client-side JavaScript, which will connect to the Log WebSocket Service and will print every single log message to the body of the page: ... < body onload = \"connectToLog()\" > < script > var connectToLog = function () { try { var logSocket = new WebSocket ((( location . protocol === 'https:' ) ? \"wss://\" : \"ws://\" ) + window . location . host + \"/log\" ); } catch ( e ) { document . writeln ( \"<div style='background-color: black; font-family: monospace; color: red'>[\" + new Date (). toISOString () + \"][error]\" + e . message + \"</div>\" ); } logSocket . onmessage = function ( message ) { var color = \"#44EE44\" ; if ( message . data . startsWith ( \"[error]\" )) { color = \"red\" ; } var date = new Date (); var id = date . getTime (); document . writeln ( \"<div id='\" + id + \"' style='background-color: black; font-family: monospace; color: \" + color + \"'>[\" + date . toISOString () + \"]\" + message . data + \"</div>\" ); window . location . hash = \"#\" + id ; }; setInterval ( clear , 60000 ); } var clear = function () { document . body . innerHTML = '' ; document . writeln ( \"<div style='background-color: black; font-family: monospace; color: gray'>[\" + new Date (). toISOString () + \"][clear]...</div>\" ); } </ script > ... source code here The assumption here is that the protocol of the WebSocket connection has the same security level as the page itself http->ws https->wss. On receiving a log message the \"logSocket.onmessage\" function is called. The other function \"clear\" is added just for usability and performance reasons. The above user interface can be used stand-alone or can be embedded in the Registry portal or in the WebIDE. Can it be easier? Yes - with Eclipse Dirigible! The support of WebSockets in Dirigible's Scripting Services is coming with release 2.4 - today! 2.0 compliant API is on place and sample will be provided shortly. Enjoy!","title":"WebSocket Client"},{"location":"2016/08/01/blogs-why-enterprise-js/","text":"Enterprise JavaScript - it sounds like an oxymoron, doesn't it? JavaScript evolved in the last years not only as \"the language\" for the browsers, but also as a server-side scripting language. There are already some implementations based on different underlying engines, which lead to different features sets. Let's name a few engines e.g. Mozilla Rhino, Nashorn, V8, SpiderMonkey and a couple of frameworks enhancing each of these base JavaScript engines - Node.js, RingoJS, Narwhal and many more. Here we don't even count the JavaScript derivative intermediate languages like Objective-J, TypeScript, CoffeeScript, etc. How can the business application developer choose where to start from? Which engine best meets the developer\u2019s needs? Is it true that the fastest engine is always the best one? Are there others non-functional requirements like maintainability, portability, supportability, compatibility, testability, usability, etc. which can have even bigger priority than the particular engine's and framework's performance? If we talk about the biggest enterprises, the questions like \"shall we invest in a given language, engine, framework or platform?\" are never so simple to answer. There are many different viewpoints, various aspects that have to be considered, and last but not least, the quality attributes of the given component have to be precisely evaluated. What did we do so far in Eclipse Dirigible when it comes to consciousness and pragmatism? We already set JavaScript as the major scripting language supported by the platform. The availability - engines and tooling, maturity and popularity were the main driving forces in our case. The dynamic typing nature of the language itself is a perfect match with the concept of dynamic applications we focus on, but has never been the strongest reason for choosing it. So far so good. Now, what can the developers do with this JavaScript language? Shall they start building all the required commodity frameworks for HTTP communication, database access, encryption and many more, which are de-facto standard basis in the other languages? Obviously, this is not exactly the perspective that the business developers see for themselves by starting to use one of the most powerful cloud development platforms, which Eclipse Dirigible claims to be. Coming from the Java world as many of you, the natural question is - can I reuse somehow the frameworks and APIs that I am already familiar with? But in the same time, should I go to JavaScript only modules to keep the source code clean, or can I mix Java classes and JavaScript? How easy would it be to port my source code from Java based engines like Rhino and Nashorn to non-Java ones e.g V8 later on? Having in mind all these questions, we can define our mission statement for the Enterprise JavaScript: The ultimate goal of the \"Enterprise JavaScript\" is to provide a set of a standard yet powerful APIs, which can be used by the business applications developer right away. The benefits are: Completeness Rich, but still standardized APIs; Expose legacy components and frameworks to the new environment; Portability No tight vendor lock-in to the currently chosen underlying JavaScript platform; OS, platform and database agnostic; Developers can stick to native JavaScript objects and primitives only in their source code; Usability The API itself is a standard Eclipse Dirigible project, hence can have the same life-cycle as the rest of the projects; Let's see a few examples what we are targeting on: Database Access Very natural for a Java-saurus is to use the JDBC API for access and management of relational databases. It provides classes and methods for the manipulation of the data and the metadata. It is powerful enough and in the same time, well known, so can we reuse it? The module db/database gives the port of the main JDBC objects for data management - Datasource, Connection, Statement, ResultSet. An example of how to query the records from a table and print the result into the response stream looks like this: var database = require ( 'db/database' ); var response = require ( 'net/http/response' ); var datasource = database . getDatasource (); // default var connection = datasource . getConnection (); try { var statement = connection . prepareStatement ( \"select * from DGB_FILES where FILE_PATH like ?\" ); var i = 0 ; statement . setString ( ++ i , \"%\" ); var resultSet = statement . executeQuery (); while ( resultSet . next ()) { response . println ( \"[path]: \" + resultSet . getString ( \"FILE_PATH\" )); } resultSet . close (); statement . close (); } catch ( e ) { console . trace ( e ); response . println ( e . message ); } finally { connection . close (); } response . flush (); response . close (); HTTP Communication Another very popular API used in the above example is the Servlet API giving the access from the service body to the current Request, Response and Session objects. You can find more info in the modules net/http/request , net/http/response and net/http/session . What we decided to include also in the Enterprise JavaScript is the de facto standard component for handling of the uploaded binary files. Here is the moment to send big thanks to the Apache guys. Of course, we simplified a lot the API itself and added some utilities functions to make it convenient for the JavaScript developers: var upload = require ( 'net/http/upload' ); var request = require ( 'net/http/request' ); var response = require ( 'net/http/response' ); if ( request . getMethod () === \"POST\" ) { if ( upload . isMultipartContent ()) { var files = upload . parseRequest (); files . forEach ( function ( file ) { response . println ( \"[File Name] \" + file . name ); response . println ( \"[File Data]\" ); // response.println(file.data); // as a raw byte array or as a string below response . println ( String . fromCharCode . apply ( null , file . data )); }); } else { response . println ( \"The request's content must be 'multipart'\" ); } } else if ( request . getMethod () === \"GET\" ) { response . println ( \"Use POST request.\" ); } response . flush (); response . close (); More info can be found in module net/http/upload . We also needed an HTTP client API to call external services. This one we defined similar to jQuery, no matter it is backed by the Apache's HTTPClient. You can use the module net/http/client to retrieve the raw data from an endpoint and print it to the response stream: var http = require ( 'net/http/client' ); var response = require ( 'net/http/response' ); var options = { method : 'GET' , // default host : 'http://services.odata.org' , port : 80 , path : '/V4/Northwind/Northwind.svc/' , binary : false }; var httpResponse = http . request ( options ); response . println ( httpResponse . statusMessage ); response . println ( httpResponse . data ); response . flush (); response . close (); or even simpler: var httpResponse = httpClient . get ( 'http://services.odata.org/V4/Northwind/Northwind.svc/' ); WebSockets and SOAP If you need bi-directional communication channel in your use case, you can utilize the WebSockets module net/websocket . You can implement a handler for the received messages as well as to send messages back through the channel of the same session. Can we name this API \"Enterprise\" without including the SOAP web services? Of course, not! In the module net/soap you can find how to construct a SOAP massage in order to be able to call an external SOAP web service. You can even create your own SOAP web service! Although, we are not completely sure why you would do this, but you can. Files and Streams To standardize the IO access to the underlying file system we added the module io/files . How to create, copy and delete a file with this module is shown below: files . createFile ( \"../temp/file1.txt\" ); files . copy ( \"../temp/file1.txt\" , \"../temp/file2.txt\" ); files . delete ( \"../temp/file2.txt\" ); You can also read, write and inspect the file and folders attributes. Reading from and writing to streams, for example, memory byte arrays, is possible via the module net/streams . Indexing, Messaging, Mail... The modules under the main package service e.g. service/indexing , service/messaging , and so on represents the underlying platform services. These services, as well as their management and operation, are usually provided by the platform on which Eclipse Dirigible is running. The quality and the performance of the services themselves can differ depending on the platform provider, but the goal here is to provide a unified manner of using such standard services, or at least the common denominator of their capabilities. For instance if you want to create a free text search index, you can do it like this: var indexing = require ( 'service/indexing' ); var response = require ( 'net/http/response' ); var index = indexing . getIndex ( \"myIndex\" ); var document1 = { \"id\" : \"1\" , \"content\" : \"some cool content 1\" }; var document2 = { \"id\" : \"2\" , \"content\" : \"some cool content 2\" }; index . add ( document1 ); index . add ( document2 ); var results = index . search ( \"cool\" ); for ( var i = 0 ; i < results . length ; i ++ ) { var result = results [ i ]; response . println ( \"[Found for 'cool']: \" + result . id ); } results = index . search ( \"1\" ); for ( var i = 0 ; i < results . length ; i ++ ) { result = results [ i ]; response . println ( \"[Found for '1']: \" + result . id ); } results = index . search ( \"2\" ); for ( var i = 0 ; i < results . length ; i ++ ) { result = results [ i ]; response . println ( \"[Found for '2']: \" + result . id ); } index . clear (); response . flush (); response . close (); Eclipse Dirigible provides default sample implementations of all the service APIs. To be able to redirect the API to the real platform service, you need to implement an adapter plugin to this service if it is not already available and then to configure the usage. Utilities Another set of modules under the package utils , provides some commodity functionality backed mainly by the Apache Commons - utils/base64 , utils/digest , utils/hex , etc. var hex = require ( 'utils/hex' ); var response = require ( 'net/http/response' ); response . println ( hex . encode ( 'Hex Encoded' )); response . println ( hex . decode ( '48657820456e636f646564' )); response . flush (); response . close (); Threads What would you say to have thread management API in JavaScript? This is missing even in the most popular - Node.js framework. In the Enterprise JavaScript module core/threads you can use a simple function as a \"runnable\" object. You can start/stop new threads, wait and notify locks and even use of synchronized functions. var threads = require ( 'core/threads' ); var response = require ( 'net/http/response' ); response . setContentType ( \"text/plain; charset=UTF-8\" ); // Define a JavaScript function function runnable () { response . println ( \"Hello World from a Thread!\" ); }; // Pass a JavaScript function var worker = threads . create ( runnable , \"I am a thread\" ); response . println ( worker . getName ()); worker . start (); worker . join (); // to be able to print to the response response . flush (); response . close (); Be sure that you use this module with caution! References Did you like it? Do you plan to base your development on Eclipse Dirigible against the Enterprise JavaScript? Everything you need to know about it is at http://api.dirigible.io and http://samples.dirigible.io . Enjoy!","title":"Why Enterprise JavaScript?"},{"location":"2016/08/01/blogs-why-enterprise-js/#database-access","text":"Very natural for a Java-saurus is to use the JDBC API for access and management of relational databases. It provides classes and methods for the manipulation of the data and the metadata. It is powerful enough and in the same time, well known, so can we reuse it? The module db/database gives the port of the main JDBC objects for data management - Datasource, Connection, Statement, ResultSet. An example of how to query the records from a table and print the result into the response stream looks like this: var database = require ( 'db/database' ); var response = require ( 'net/http/response' ); var datasource = database . getDatasource (); // default var connection = datasource . getConnection (); try { var statement = connection . prepareStatement ( \"select * from DGB_FILES where FILE_PATH like ?\" ); var i = 0 ; statement . setString ( ++ i , \"%\" ); var resultSet = statement . executeQuery (); while ( resultSet . next ()) { response . println ( \"[path]: \" + resultSet . getString ( \"FILE_PATH\" )); } resultSet . close (); statement . close (); } catch ( e ) { console . trace ( e ); response . println ( e . message ); } finally { connection . close (); } response . flush (); response . close ();","title":"Database Access"},{"location":"2016/08/01/blogs-why-enterprise-js/#http-communication","text":"Another very popular API used in the above example is the Servlet API giving the access from the service body to the current Request, Response and Session objects. You can find more info in the modules net/http/request , net/http/response and net/http/session . What we decided to include also in the Enterprise JavaScript is the de facto standard component for handling of the uploaded binary files. Here is the moment to send big thanks to the Apache guys. Of course, we simplified a lot the API itself and added some utilities functions to make it convenient for the JavaScript developers: var upload = require ( 'net/http/upload' ); var request = require ( 'net/http/request' ); var response = require ( 'net/http/response' ); if ( request . getMethod () === \"POST\" ) { if ( upload . isMultipartContent ()) { var files = upload . parseRequest (); files . forEach ( function ( file ) { response . println ( \"[File Name] \" + file . name ); response . println ( \"[File Data]\" ); // response.println(file.data); // as a raw byte array or as a string below response . println ( String . fromCharCode . apply ( null , file . data )); }); } else { response . println ( \"The request's content must be 'multipart'\" ); } } else if ( request . getMethod () === \"GET\" ) { response . println ( \"Use POST request.\" ); } response . flush (); response . close (); More info can be found in module net/http/upload . We also needed an HTTP client API to call external services. This one we defined similar to jQuery, no matter it is backed by the Apache's HTTPClient. You can use the module net/http/client to retrieve the raw data from an endpoint and print it to the response stream: var http = require ( 'net/http/client' ); var response = require ( 'net/http/response' ); var options = { method : 'GET' , // default host : 'http://services.odata.org' , port : 80 , path : '/V4/Northwind/Northwind.svc/' , binary : false }; var httpResponse = http . request ( options ); response . println ( httpResponse . statusMessage ); response . println ( httpResponse . data ); response . flush (); response . close (); or even simpler: var httpResponse = httpClient . get ( 'http://services.odata.org/V4/Northwind/Northwind.svc/' );","title":"HTTP Communication"},{"location":"2016/08/01/blogs-why-enterprise-js/#websockets-and-soap","text":"If you need bi-directional communication channel in your use case, you can utilize the WebSockets module net/websocket . You can implement a handler for the received messages as well as to send messages back through the channel of the same session. Can we name this API \"Enterprise\" without including the SOAP web services? Of course, not! In the module net/soap you can find how to construct a SOAP massage in order to be able to call an external SOAP web service. You can even create your own SOAP web service! Although, we are not completely sure why you would do this, but you can.","title":"WebSockets and SOAP"},{"location":"2016/08/01/blogs-why-enterprise-js/#files-and-streams","text":"To standardize the IO access to the underlying file system we added the module io/files . How to create, copy and delete a file with this module is shown below: files . createFile ( \"../temp/file1.txt\" ); files . copy ( \"../temp/file1.txt\" , \"../temp/file2.txt\" ); files . delete ( \"../temp/file2.txt\" ); You can also read, write and inspect the file and folders attributes. Reading from and writing to streams, for example, memory byte arrays, is possible via the module net/streams .","title":"Files and Streams"},{"location":"2016/08/01/blogs-why-enterprise-js/#indexing-messaging-mail","text":"The modules under the main package service e.g. service/indexing , service/messaging , and so on represents the underlying platform services. These services, as well as their management and operation, are usually provided by the platform on which Eclipse Dirigible is running. The quality and the performance of the services themselves can differ depending on the platform provider, but the goal here is to provide a unified manner of using such standard services, or at least the common denominator of their capabilities. For instance if you want to create a free text search index, you can do it like this: var indexing = require ( 'service/indexing' ); var response = require ( 'net/http/response' ); var index = indexing . getIndex ( \"myIndex\" ); var document1 = { \"id\" : \"1\" , \"content\" : \"some cool content 1\" }; var document2 = { \"id\" : \"2\" , \"content\" : \"some cool content 2\" }; index . add ( document1 ); index . add ( document2 ); var results = index . search ( \"cool\" ); for ( var i = 0 ; i < results . length ; i ++ ) { var result = results [ i ]; response . println ( \"[Found for 'cool']: \" + result . id ); } results = index . search ( \"1\" ); for ( var i = 0 ; i < results . length ; i ++ ) { result = results [ i ]; response . println ( \"[Found for '1']: \" + result . id ); } results = index . search ( \"2\" ); for ( var i = 0 ; i < results . length ; i ++ ) { result = results [ i ]; response . println ( \"[Found for '2']: \" + result . id ); } index . clear (); response . flush (); response . close (); Eclipse Dirigible provides default sample implementations of all the service APIs. To be able to redirect the API to the real platform service, you need to implement an adapter plugin to this service if it is not already available and then to configure the usage.","title":"Indexing, Messaging, Mail..."},{"location":"2016/08/01/blogs-why-enterprise-js/#utilities","text":"Another set of modules under the package utils , provides some commodity functionality backed mainly by the Apache Commons - utils/base64 , utils/digest , utils/hex , etc. var hex = require ( 'utils/hex' ); var response = require ( 'net/http/response' ); response . println ( hex . encode ( 'Hex Encoded' )); response . println ( hex . decode ( '48657820456e636f646564' )); response . flush (); response . close ();","title":"Utilities"},{"location":"2016/08/01/blogs-why-enterprise-js/#threads","text":"What would you say to have thread management API in JavaScript? This is missing even in the most popular - Node.js framework. In the Enterprise JavaScript module core/threads you can use a simple function as a \"runnable\" object. You can start/stop new threads, wait and notify locks and even use of synchronized functions. var threads = require ( 'core/threads' ); var response = require ( 'net/http/response' ); response . setContentType ( \"text/plain; charset=UTF-8\" ); // Define a JavaScript function function runnable () { response . println ( \"Hello World from a Thread!\" ); }; // Pass a JavaScript function var worker = threads . create ( runnable , \"I am a thread\" ); response . println ( worker . getName ()); worker . start (); worker . join (); // to be able to print to the response response . flush (); response . close (); Be sure that you use this module with caution!","title":"Threads"},{"location":"2016/08/01/blogs-why-enterprise-js/#references","text":"Did you like it? Do you plan to base your development on Eclipse Dirigible against the Enterprise JavaScript? Everything you need to know about it is at http://api.dirigible.io and http://samples.dirigible.io .","title":"References"},{"location":"2016/08/01/blogs-why-enterprise-js/#enjoy","text":"","title":"Enjoy!"},{"location":"2016/08/09/how-to-run-dirigible-anywhere-microsoft-azure/","text":"This blog is the first of series of blogs on the hot topic - \"How to Run Dirigible Anywhere?\" . Our first target to run Eclipse Dirigible on is Microsoft Azure . Micrsoft Azure Why Microsoft Azure? Microsoft Azure public cloud has proven to be mature enough to run your enterprise bussines applications, scale your business, benefit from the variety of services it provides, simplify the complex environments, pay only for what you've used and a lot more. It seems to be the right place to run your one Eclipse Dirigible instance. Here are some resources on why to choose Microsoft Azure: What is Azure Business Apps on Azure Case Studies Steps Bellow you can find the steps of how to deploy Eclipse Dirigible on Microsoft Azure. Sing Up for Microsoft Azure. To benefit from the 1 month free trial, you should have a Microsft Account, or you should create a new account. To sign up for Microsoft Azure, go to https://azure.microsoft.com and start the free trial. With the Microsoft Azure subscribtion, you will have access to 20+ availability zones around the globe, 14 virtual machines , CDN , IoT services, Hadoop and many more. NOTE: You can register only once for the free trial! Log in to Microsoft Azure Portal. The Microsoft Azure Portal is a central place for management, operations, delivery and even development of your cloud services and solutions. Create new VM with Tomcat server. To deploy Eclipse Dirigible on Microsoft Azure, you need a virtual machine equiped with JDK 6+ and Tomcat . Luckily, the provisioning of VMs in Azure is really easy and you can select between a large set of templates. Click on the New button and search for 'tomcat' . Create a new VM with the \"Apache Tomcat 8\" template, published by \"Microsoft\" . NOTE: Select the \"Pin to dashboard\" checkbox to add the VM to the portal's dashboard. Creating the VM may take up to 5 minutes, so relax, lay back and wait patiently for it. On your dashboard, you will see a tile with the VM info. Wait until the VM is in \"Running\" state. 4. Deploy Eclipse Dirigible. * Before deploying Eclipse Dirigible, ypu can get rid of some of the default content deployed with the Tomcat server. To do that, you benefit from some of the finest web-based development tools, that Azure offers. 1. Click on your WEB APP tile. 2. From the menu, click on the Tools bar and launch the App Service Editor (Preview) . 3. The web-based workbench should be displayed, with the files and folders under the wwwroot directory on the VM. 4. Navigate to \"bin/apache-tomcat-8.0.33/webapps\" and delete all files and folders under this directory. 5. Close the App Service Editor (Preview) tool and go back to your WEB APP settings page. 6. Now, you are going to use another web-based tool - the good old Console . 7. Navigate to the \"webapps\" folder of the server. cd bin/apache-tomcat-8.0.33/webapps NOTE: Unfortunately, the Console lacks the paste functionality, so you should type manualy each command in it. Now it's time to download and run the latest release of Eclipse Dirigible on this VM. Go to http://download.eclipse.org/dirigible/ and open the latest release. From the different types of downloads, you need the \"Tomcat\" and more specificaly the \"allinone\" . Right click on \"tomcat/allinone/ROOT.war\" and copy the link address. NOTE: At the time of this blog, the latest relase was 2.5. Go back to the Azure web-based Console and execute under the \"webapps\" directory the following command: curl http://download.eclipse.org/dirigible/drops/R-2.5-201608041010/tomcat/allinone/ROOT.war -o ROOT.war NOTE: The address from the curl request is the one that you've copied earlier, from the \"Eclipse Dirigible Downloads\" page. The whole download may take up to 5 min. After the download is completed, the Tomcat server will extract and run the ROOT.war. After a while, Eclipse Dirigible will be accessible through the HTTP. However, there is one more step that needs to be done, before you can use it. Assign the Eclipse Dirigible specific roles. The last step is to crete a user and assign it the following roles, in order to use Eclipse Dirigible: Everyone Developer Operator Use the web-based App Service Editor (Preview) to edit \"bin/apache-tomcat-8.0.33/conf/tomcat-users.xml\" . Add the following content: ` <role rolename=\"Everyone\"/> <role rolename=\"Developer\"/> <role rolename=\"Operator\"/> <user username=\"dirigible\" password=\"dirigible\" roles=\"Developer,Operator,Everyone\"/> ` 3. Restart the VM to apply the users configuration. Launch Eclipse Dirigible. Finally, you can launch the Eclipse Dirigible platform. To do that, click on the Browse button from the WEB APP settings. References Bookstore Sample Enjoy!","title":"How to Run Dirigible Anywhere - Microsoft Azure?"},{"location":"2016/08/09/how-to-run-dirigible-anywhere-microsoft-azure/#micrsoft-azure","text":"Why Microsoft Azure? Microsoft Azure public cloud has proven to be mature enough to run your enterprise bussines applications, scale your business, benefit from the variety of services it provides, simplify the complex environments, pay only for what you've used and a lot more. It seems to be the right place to run your one Eclipse Dirigible instance. Here are some resources on why to choose Microsoft Azure: What is Azure Business Apps on Azure Case Studies","title":"Micrsoft Azure"},{"location":"2016/08/09/how-to-run-dirigible-anywhere-microsoft-azure/#steps","text":"Bellow you can find the steps of how to deploy Eclipse Dirigible on Microsoft Azure. Sing Up for Microsoft Azure. To benefit from the 1 month free trial, you should have a Microsft Account, or you should create a new account. To sign up for Microsoft Azure, go to https://azure.microsoft.com and start the free trial. With the Microsoft Azure subscribtion, you will have access to 20+ availability zones around the globe, 14 virtual machines , CDN , IoT services, Hadoop and many more. NOTE: You can register only once for the free trial! Log in to Microsoft Azure Portal. The Microsoft Azure Portal is a central place for management, operations, delivery and even development of your cloud services and solutions. Create new VM with Tomcat server. To deploy Eclipse Dirigible on Microsoft Azure, you need a virtual machine equiped with JDK 6+ and Tomcat . Luckily, the provisioning of VMs in Azure is really easy and you can select between a large set of templates. Click on the New button and search for 'tomcat' . Create a new VM with the \"Apache Tomcat 8\" template, published by \"Microsoft\" . NOTE: Select the \"Pin to dashboard\" checkbox to add the VM to the portal's dashboard. Creating the VM may take up to 5 minutes, so relax, lay back and wait patiently for it. On your dashboard, you will see a tile with the VM info. Wait until the VM is in \"Running\" state. 4. Deploy Eclipse Dirigible. * Before deploying Eclipse Dirigible, ypu can get rid of some of the default content deployed with the Tomcat server. To do that, you benefit from some of the finest web-based development tools, that Azure offers. 1. Click on your WEB APP tile. 2. From the menu, click on the Tools bar and launch the App Service Editor (Preview) . 3. The web-based workbench should be displayed, with the files and folders under the wwwroot directory on the VM. 4. Navigate to \"bin/apache-tomcat-8.0.33/webapps\" and delete all files and folders under this directory. 5. Close the App Service Editor (Preview) tool and go back to your WEB APP settings page. 6. Now, you are going to use another web-based tool - the good old Console . 7. Navigate to the \"webapps\" folder of the server. cd bin/apache-tomcat-8.0.33/webapps NOTE: Unfortunately, the Console lacks the paste functionality, so you should type manualy each command in it. Now it's time to download and run the latest release of Eclipse Dirigible on this VM. Go to http://download.eclipse.org/dirigible/ and open the latest release. From the different types of downloads, you need the \"Tomcat\" and more specificaly the \"allinone\" . Right click on \"tomcat/allinone/ROOT.war\" and copy the link address. NOTE: At the time of this blog, the latest relase was 2.5. Go back to the Azure web-based Console and execute under the \"webapps\" directory the following command: curl http://download.eclipse.org/dirigible/drops/R-2.5-201608041010/tomcat/allinone/ROOT.war -o ROOT.war NOTE: The address from the curl request is the one that you've copied earlier, from the \"Eclipse Dirigible Downloads\" page. The whole download may take up to 5 min. After the download is completed, the Tomcat server will extract and run the ROOT.war. After a while, Eclipse Dirigible will be accessible through the HTTP. However, there is one more step that needs to be done, before you can use it. Assign the Eclipse Dirigible specific roles. The last step is to crete a user and assign it the following roles, in order to use Eclipse Dirigible: Everyone Developer Operator Use the web-based App Service Editor (Preview) to edit \"bin/apache-tomcat-8.0.33/conf/tomcat-users.xml\" . Add the following content: ` <role rolename=\"Everyone\"/> <role rolename=\"Developer\"/> <role rolename=\"Operator\"/> <user username=\"dirigible\" password=\"dirigible\" roles=\"Developer,Operator,Everyone\"/> ` 3. Restart the VM to apply the users configuration. Launch Eclipse Dirigible. Finally, you can launch the Eclipse Dirigible platform. To do that, click on the Browse button from the WEB APP settings.","title":"Steps"},{"location":"2016/08/09/how-to-run-dirigible-anywhere-microsoft-azure/#references","text":"Bookstore Sample","title":"References"},{"location":"2016/08/09/how-to-run-dirigible-anywhere-microsoft-azure/#enjoy","text":"","title":"Enjoy!"},{"location":"2016/08/17/sap-summer-practice/","text":"My name is Viktor and in the past two weeks, I took part in the Summer Student Practice in SAP Labs Bulgaria, which brought together IT students from different Bulgarian universities: Sofia University \u201cSt. Kliment Ohridski\u201d, Technical University \u2013 Sofia, University of Plovdiv \"Paisii Hilendarski\" and others. Within two weeks we, the participants, had the opportunity to expand our knowledge about modern IT topics and learn from professionals how to develop software \u2013 both theoretically and practically. Volunteers from SAP shared with us their professional experience in software architecture and design. During the first few days the schedule included lectures and theoretical introduction to the most \u201chot\u201d technology trends, methodologies and principles of project management, user experience, prototyping, documentation, maintenance and lifecycle of software products. Then, based on everyone\u2019s personal preferences, we were divided into teams. Each team, with the help of a mentor, had to prepare a practical project, based on the acquired knowledge, and present it on the last day. The Eclipse Dirigible platform grabbed my attention and I wanted to try it out. I was a part of a team of five: Vili, Boris, Desi, Svilen, and I. Here are we in the beginning - designing the application. And here are some words about the team project from each of us: Viktor: We had to develop a project using Eclipse Dirigible and we got the idea to make a \u201ctravel guide\u201d application. The plan was to gather everything you need to plan your trip in one place. We wanted every part of the application to be independent so we decided to use microservices architecture. When we saw what Eclipse Dirigible offers, we knew that it would be perfect for our needs. It turned out that it\u2019s really easy to make RESTful services in Eclipse Dirigible in just a few minutes, so we had enough time to focus on the implementation details. Each of us worked on a different part of the project, with its own user interface, and in the end we merged them together. My contribution to the project were two of those microservices: one for user profiles and one for finding hotels by given destinations and booking hotel rooms. Eclipse Dirigible helped me a lot with generating the user interface for the hotels service based on the AngularJS Framework. With just a little modification and coding, I managed to make a good looking and responsive web page. Now users can filter the hotels by different criteria, search for them on the map, check the available rooms and reserve them. Vili: Working in a team of young, educated and ambitious people is such a precious experience to me. We put our hearts and minds in this project. We had a week to work on this application that helps all the people out there to organize the desired trip. The design pattern we applied on this application is MVC. When it comes to data, we used JQuery Ajax requests and REST services. Furthermore, a great advantage of the Eclipse Dirigible, the platform we used, is its ability to generate automatically REST services depending on the built model. As all other developing platforms, Eclipse Dirigible has its own advantages and disadvantages. However, the 'crew' behind the idea puts every effort to fix the bugs. Hence, it's up to you to try and feel the adventure of Eclipse Dirigible! Desi: When you want to develop your application as fast as you can, Eclipse Dirigible helps you to concentrate on your business logic. Also, the auto-generated CRUD operations can help you out if you\u2019re not really sure how things are done in the back-end. The generated user interface is the best thing for people who don't like to deal with front-end. Being part of this project also introduced me to AngularJS and the MVC style. My part was to implement an Events service where you can find different types of events near a specified location or create your own public event. Svilen: Our two weeks of practice in SAP were very intriguing. We had a couple of lectures and a team project. Our team of five had to develop a WEB Appliaction by our own idea using the Eclipse Dirigible platform. We\u2019ve decided to create the app based on independent micro-services. I took part by creating a small service about transports - (Storky Transports Web Page). Our project is based on the MVC design pattern thanks to AngularJS. My application has two own entities in the database, one for Tickets, and one for Transports and one master entity about Cities and their airports. The data is being transfered by HTTP and REST. From my application you can choose your type of travel, city of departure, city of arrival and date. To create the RESTful API I used Eclipse Dirigible's automated scripting services for each of the entities from my database. Thanks to that, whatever the operation, I receive/pass the data in JSON format and easily can import it in my view with angular. After you choose the type of travel, an angular controller sends GET request to the server and only cities that have transport to or from are being displayed. The \"Search\" button has attached function from other angular controller that serves the tickets entity. The function sends another GET request, this time to a service of database view, filters the cities in the back-end and returns them again in JSON. The controller imports them as a view in the HTML and you can sort them by price or class thanks to angular. That's all. Boris: My part of the application is divided into two similar parts: Entertainments : When from another page (\u201cWelcome\u201d, \u201cTransport\u201d, \u201cEvents\u201d, \u201cHotels\u201d or \u201cSightseeings\u201d) is being navigated to Entertainments, the name of the destination is automatically put into the textbox bellow the \u201cDestination\u201d label. The Entertainments, that are near this city are filtered and are shown together with their entertainment type and category, rating, additional information and destination. The user may, however, delete the name of destination and see all of the entertainments in the database: With the help of the buttons \u201cFirst\u201d, \u201cPrevious\u201d, \u201dNext\u201d, \u201dLast\u201d the user may navigate through all of the entertainments if they are too many. Sightseeings : Summary I found it really easy to develop using Eclipse Dirigible. It provides everything I needed \u2013 from database modeling and management, through RESTful services, to user interface generation. It was the perfect platform. I would say that the Summer Student Practice was a great success. We had a nearly finished the project in just a few days, with a platform we never used before, and in the end everyone had learned something new. Our project got second place in the rank list. Here are we after the presentation.","title":"Summer Practice in SAP"},{"location":"2016/08/17/sap-summer-practice/#viktor","text":"We had to develop a project using Eclipse Dirigible and we got the idea to make a \u201ctravel guide\u201d application. The plan was to gather everything you need to plan your trip in one place. We wanted every part of the application to be independent so we decided to use microservices architecture. When we saw what Eclipse Dirigible offers, we knew that it would be perfect for our needs. It turned out that it\u2019s really easy to make RESTful services in Eclipse Dirigible in just a few minutes, so we had enough time to focus on the implementation details. Each of us worked on a different part of the project, with its own user interface, and in the end we merged them together. My contribution to the project were two of those microservices: one for user profiles and one for finding hotels by given destinations and booking hotel rooms. Eclipse Dirigible helped me a lot with generating the user interface for the hotels service based on the AngularJS Framework. With just a little modification and coding, I managed to make a good looking and responsive web page. Now users can filter the hotels by different criteria, search for them on the map, check the available rooms and reserve them.","title":"Viktor:"},{"location":"2016/08/17/sap-summer-practice/#vili","text":"Working in a team of young, educated and ambitious people is such a precious experience to me. We put our hearts and minds in this project. We had a week to work on this application that helps all the people out there to organize the desired trip. The design pattern we applied on this application is MVC. When it comes to data, we used JQuery Ajax requests and REST services. Furthermore, a great advantage of the Eclipse Dirigible, the platform we used, is its ability to generate automatically REST services depending on the built model. As all other developing platforms, Eclipse Dirigible has its own advantages and disadvantages. However, the 'crew' behind the idea puts every effort to fix the bugs. Hence, it's up to you to try and feel the adventure of Eclipse Dirigible!","title":"Vili:"},{"location":"2016/08/17/sap-summer-practice/#desi","text":"When you want to develop your application as fast as you can, Eclipse Dirigible helps you to concentrate on your business logic. Also, the auto-generated CRUD operations can help you out if you\u2019re not really sure how things are done in the back-end. The generated user interface is the best thing for people who don't like to deal with front-end. Being part of this project also introduced me to AngularJS and the MVC style. My part was to implement an Events service where you can find different types of events near a specified location or create your own public event.","title":"Desi:"},{"location":"2016/08/17/sap-summer-practice/#svilen","text":"Our two weeks of practice in SAP were very intriguing. We had a couple of lectures and a team project. Our team of five had to develop a WEB Appliaction by our own idea using the Eclipse Dirigible platform. We\u2019ve decided to create the app based on independent micro-services. I took part by creating a small service about transports - (Storky Transports Web Page). Our project is based on the MVC design pattern thanks to AngularJS. My application has two own entities in the database, one for Tickets, and one for Transports and one master entity about Cities and their airports. The data is being transfered by HTTP and REST. From my application you can choose your type of travel, city of departure, city of arrival and date. To create the RESTful API I used Eclipse Dirigible's automated scripting services for each of the entities from my database. Thanks to that, whatever the operation, I receive/pass the data in JSON format and easily can import it in my view with angular. After you choose the type of travel, an angular controller sends GET request to the server and only cities that have transport to or from are being displayed. The \"Search\" button has attached function from other angular controller that serves the tickets entity. The function sends another GET request, this time to a service of database view, filters the cities in the back-end and returns them again in JSON. The controller imports them as a view in the HTML and you can sort them by price or class thanks to angular. That's all.","title":"Svilen:"},{"location":"2016/08/17/sap-summer-practice/#boris","text":"My part of the application is divided into two similar parts: Entertainments : When from another page (\u201cWelcome\u201d, \u201cTransport\u201d, \u201cEvents\u201d, \u201cHotels\u201d or \u201cSightseeings\u201d) is being navigated to Entertainments, the name of the destination is automatically put into the textbox bellow the \u201cDestination\u201d label. The Entertainments, that are near this city are filtered and are shown together with their entertainment type and category, rating, additional information and destination. The user may, however, delete the name of destination and see all of the entertainments in the database: With the help of the buttons \u201cFirst\u201d, \u201cPrevious\u201d, \u201dNext\u201d, \u201dLast\u201d the user may navigate through all of the entertainments if they are too many. Sightseeings :","title":"Boris:"},{"location":"2016/08/17/sap-summer-practice/#summary","text":"I found it really easy to develop using Eclipse Dirigible. It provides everything I needed \u2013 from database modeling and management, through RESTful services, to user interface generation. It was the perfect platform. I would say that the Summer Student Practice was a great success. We had a nearly finished the project in just a few days, with a platform we never used before, and in the end everyone had learned something new. Our project got second place in the rank list. Here are we after the presentation.","title":"Summary"},{"location":"2016/08/18/how-to-run-dirigible-anywhere-microsoft-azure-part-2/","text":"This blog is part of the \"How to Run Dirigible Anywhere?\" series. In this edition, we will see how to simplify the deployment process on Microsoft Azure . Overview Do you remember the first blog, about the Microsoft Azure and how to run Eclipse Dirigible on it ( How to Run Dirigible Anywhere - Microsoft Azure )? In that blog, there were a lot of steps, before we can get Eclipse Dirigible up and running on Azure. Now, it's time to simplify the steps and narrow them down to only few mouse clicks. How is that possible? Thanks to Azure Quickstart Templates and more specifically to Azure App Service Samples - Tomcat Template . Steps Bellow, you can find the steps of how to deploy Eclipse Dirigible on Microsoft Azure through the templates available on DirigibleLabs . The templates that you can choose from are: Dirigible Trial - Azure Template This template offers the Trial Distribution of Eclipse Dirigible, which is available on http://trial.dirigible.io . Dirigible Tomcat - Azure Template This template offers the Tomcat Distribution of Eclipse Dirigible, protected with BASIC authentication, pre-configured with Default User and using the Apache Derby database. To deploy your own instance of Eclipse Dirigible on Microsoft Azure , first choose one of the available templates and then follow the steps: Click the Deploy to Azure button. Fill in the required properties. 3. Wait until the deployment is finished. 4. Use the links from the Deploy page, or log into your Microsoft Azure Portal . 5. Now, you are ready to start developing with Eclipse Dirigible on Microsoft Azure . References Help API Bookstore Sample How to Run Dirigible Anywhere - Microsoft Azure? Enjoy!","title":"How to Run Dirigible Anywhere - Microsoft Azure - Part II?"},{"location":"2016/08/18/how-to-run-dirigible-anywhere-microsoft-azure-part-2/#overview","text":"Do you remember the first blog, about the Microsoft Azure and how to run Eclipse Dirigible on it ( How to Run Dirigible Anywhere - Microsoft Azure )? In that blog, there were a lot of steps, before we can get Eclipse Dirigible up and running on Azure. Now, it's time to simplify the steps and narrow them down to only few mouse clicks. How is that possible? Thanks to Azure Quickstart Templates and more specifically to Azure App Service Samples - Tomcat Template .","title":"Overview"},{"location":"2016/08/18/how-to-run-dirigible-anywhere-microsoft-azure-part-2/#steps","text":"Bellow, you can find the steps of how to deploy Eclipse Dirigible on Microsoft Azure through the templates available on DirigibleLabs . The templates that you can choose from are: Dirigible Trial - Azure Template This template offers the Trial Distribution of Eclipse Dirigible, which is available on http://trial.dirigible.io . Dirigible Tomcat - Azure Template This template offers the Tomcat Distribution of Eclipse Dirigible, protected with BASIC authentication, pre-configured with Default User and using the Apache Derby database. To deploy your own instance of Eclipse Dirigible on Microsoft Azure , first choose one of the available templates and then follow the steps: Click the Deploy to Azure button. Fill in the required properties. 3. Wait until the deployment is finished. 4. Use the links from the Deploy page, or log into your Microsoft Azure Portal . 5. Now, you are ready to start developing with Eclipse Dirigible on Microsoft Azure .","title":"Steps"},{"location":"2016/08/18/how-to-run-dirigible-anywhere-microsoft-azure-part-2/#references","text":"Help API Bookstore Sample How to Run Dirigible Anywhere - Microsoft Azure?","title":"References"},{"location":"2016/08/18/how-to-run-dirigible-anywhere-microsoft-azure-part-2/#enjoy","text":"","title":"Enjoy!"},{"location":"2016/09/12/the-java-saurus/","text":"Have you met one? Are you one? Do you even know what this is? A long, long time ago\u2026 in March at the EclipseCon 2016, Vladimir Pavlov from SAP talked about Eclipse Dirigible, and, here I will quote, \u201chow the development of cloud applications and services with Eclipse Dirigible looks like in the eyes of a hard-core veteran Java guy.\u201d If you are curious, you can see the slides here . This is the first time people met the Java-saurus and started telling stories about it. Some say it is old, some say it is just old-fashioned, but if you ask us, the Java-saurus simply is too attached to its little Java-dinosaurs. Eclipse Dirigible is a very open-minded platform, welcoming all developers, no matter how experienced they are and no matter what programming languages they use. If you are a Java-saurus or know other Java dinosaurs that are willing to change the way they view the world, come on board of Eclipse Dirigible and enjoy the high-quality journey through the fields of JavaScript, a variety of RESTful services and widely used frameworks, and much much more. 5\u2026 4\u2026 3\u2026 2\u2026 1\u2026 and\u2026 Disclaimer: Any similarity to real-life dinosaurs is purely coincidental.","title":"The Java-saurus"},{"location":"2016/11/03/cmis-explorer-at-sap-hana-cloud-platform/","text":"Wondering how you can easily manage the SAP HANA Cloud Platform Document Service through the browser? Now this is possible with the help of Eclipse Dirigible and the CMIS Explorer application. Overview Wondering how you can easily manage the SAP HANA Cloud Platform Document Service through the browser? Now this is possible with the help of Eclipse Dirigible and the CMIS Explorer application. In this blog, you will see how to download, configure, deploy, and run Eclipse Dirigible on the SAP HANA Cloud Platform . After that, you can go through the steps of installing and running the CMIS Explorer application. Download Eclipse Dirigible Download the latest milestone/release, which you can find at http://download.eclipse.org/dirigible/ . Note : At the time of the blog, the latest milestone was M20161021-1818 . From the selected release/milestone, navigate to the HANA Cloud Platform category and download the sap/allinone/ROOT.war file. Create a Document Repository Log in to the SAP HANA Cloud Platform Cockpit. In the Repositories section, open the Document Repositories tab. Create a new document repository. Please remember the values for the Name and the Repository Key , as we will need them later on. Deploy Eclipse Dirigible In the Applications section, open the Java Applications tab. Click on the Deploy Application button. Browse to the ROOT.war file, which you have already downloaded. For the application name, you can specify whatever you want (for example, dirigible, doc, \u2026). Change the runtime to Java Web Tomcat 8 . For the JVM Arguments input, enter this: -DjndiCmisServiceName=<name_of_repository> -DjndiCmisServiceKey=<repository_key> These are the magic settings that will allow the Eclipse Dirigible to connect and use the document repository. List of all available environment variables can be found here . Finally, the Deploy Application wizard should look something like this: Wait till the deployment is finished, but don't start the application yet! Configure Data Source Note : This step can be skipped if the https://hanatrial.ondemand.com landscape is used. Navigate to your application (in this example dirigible ) in the Java Applications tab. In the Configuration section, open the Data Source Bindings tab. Create a New Binding : For the name of the data source, leave the default name. Select the desired DB/Schema ID. Provide the required credentials. Assign Security Roles After deploying the application in the cockpit, click on its name. In the Security section, open the Roles tab. Assign the Developer and Operator roles to your user. Launch Eclipse Dirigible Go back to the Overview section and click on the Start button. Wait till the application is started. Launch Eclipse Dirigible from the Application URLs link. Finally, Eclipse Dirigible has been configured and deployed and is running into your SAP HANA Cloud Platform account. Install the CMIS Explorer The CMIS Explorer is a project in the DirigibleLabs GitHub organization. Open the CMIS Explorer in GitHub, choose Clone or download and then copy the URL. Go back to the Eclipse Dirigible Registry UI . Note : If you are wondering, this is how it looks like: Click on the Develop tile and after that on the Web IDE . Note : If this is the first time you are launching the \"Web IDE\", cancel the \u201cGet Started Project Wizard\u201d. Right-click on the Workspace Explorer and select Team > Clone . Paste the Git URL that you\u2019ve copied earlier. Username and Password are not required, so just click OK . After the project is imported into the workspace, right-click on it and press the Publish button. Expand the project and find the index.html . The application should be visible in the Preview tab, you can copy the link and open it in a new tab. The application URL can be found also through the Eclipse Dirigible Registry UI from Discover > Web . Recap In this tutorial, you have downloaded, configured, deployed, and run Eclipse Dirigible on the SAP HANA Cloud Platform , and you have leveraged the Document Service and the CMIS Explorer application. Enjoy!","title":"Document Service Explorer at SAP HANA Cloud Platform"},{"location":"2016/11/03/cmis-explorer-at-sap-hana-cloud-platform/#overview","text":"Wondering how you can easily manage the SAP HANA Cloud Platform Document Service through the browser? Now this is possible with the help of Eclipse Dirigible and the CMIS Explorer application. In this blog, you will see how to download, configure, deploy, and run Eclipse Dirigible on the SAP HANA Cloud Platform . After that, you can go through the steps of installing and running the CMIS Explorer application.","title":"Overview"},{"location":"2016/11/03/cmis-explorer-at-sap-hana-cloud-platform/#download-eclipse-dirigible","text":"Download the latest milestone/release, which you can find at http://download.eclipse.org/dirigible/ . Note : At the time of the blog, the latest milestone was M20161021-1818 . From the selected release/milestone, navigate to the HANA Cloud Platform category and download the sap/allinone/ROOT.war file.","title":"Download Eclipse Dirigible"},{"location":"2016/11/03/cmis-explorer-at-sap-hana-cloud-platform/#create-a-document-repository","text":"Log in to the SAP HANA Cloud Platform Cockpit. In the Repositories section, open the Document Repositories tab. Create a new document repository. Please remember the values for the Name and the Repository Key , as we will need them later on.","title":"Create a Document Repository"},{"location":"2016/11/03/cmis-explorer-at-sap-hana-cloud-platform/#deploy-eclipse-dirigible","text":"In the Applications section, open the Java Applications tab. Click on the Deploy Application button. Browse to the ROOT.war file, which you have already downloaded. For the application name, you can specify whatever you want (for example, dirigible, doc, \u2026). Change the runtime to Java Web Tomcat 8 . For the JVM Arguments input, enter this: -DjndiCmisServiceName=<name_of_repository> -DjndiCmisServiceKey=<repository_key> These are the magic settings that will allow the Eclipse Dirigible to connect and use the document repository. List of all available environment variables can be found here . Finally, the Deploy Application wizard should look something like this: Wait till the deployment is finished, but don't start the application yet!","title":"Deploy Eclipse Dirigible"},{"location":"2016/11/03/cmis-explorer-at-sap-hana-cloud-platform/#configure-data-source","text":"Note : This step can be skipped if the https://hanatrial.ondemand.com landscape is used. Navigate to your application (in this example dirigible ) in the Java Applications tab. In the Configuration section, open the Data Source Bindings tab. Create a New Binding : For the name of the data source, leave the default name. Select the desired DB/Schema ID. Provide the required credentials.","title":"Configure Data Source"},{"location":"2016/11/03/cmis-explorer-at-sap-hana-cloud-platform/#assign-security-roles","text":"After deploying the application in the cockpit, click on its name. In the Security section, open the Roles tab. Assign the Developer and Operator roles to your user.","title":"Assign Security Roles"},{"location":"2016/11/03/cmis-explorer-at-sap-hana-cloud-platform/#launch-eclipse-dirigible","text":"Go back to the Overview section and click on the Start button. Wait till the application is started. Launch Eclipse Dirigible from the Application URLs link. Finally, Eclipse Dirigible has been configured and deployed and is running into your SAP HANA Cloud Platform account.","title":"Launch Eclipse Dirigible"},{"location":"2016/11/03/cmis-explorer-at-sap-hana-cloud-platform/#install-the-cmis-explorer","text":"The CMIS Explorer is a project in the DirigibleLabs GitHub organization. Open the CMIS Explorer in GitHub, choose Clone or download and then copy the URL. Go back to the Eclipse Dirigible Registry UI . Note : If you are wondering, this is how it looks like: Click on the Develop tile and after that on the Web IDE . Note : If this is the first time you are launching the \"Web IDE\", cancel the \u201cGet Started Project Wizard\u201d. Right-click on the Workspace Explorer and select Team > Clone . Paste the Git URL that you\u2019ve copied earlier. Username and Password are not required, so just click OK . After the project is imported into the workspace, right-click on it and press the Publish button. Expand the project and find the index.html . The application should be visible in the Preview tab, you can copy the link and open it in a new tab. The application URL can be found also through the Eclipse Dirigible Registry UI from Discover > Web .","title":"Install the CMIS Explorer"},{"location":"2016/11/03/cmis-explorer-at-sap-hana-cloud-platform/#recap","text":"In this tutorial, you have downloaded, configured, deployed, and run Eclipse Dirigible on the SAP HANA Cloud Platform , and you have leveraged the Document Service and the CMIS Explorer application.","title":"Recap"},{"location":"2016/11/03/cmis-explorer-at-sap-hana-cloud-platform/#enjoy","text":"","title":"Enjoy!"},{"location":"2017/03/10/blogs-apps-testfwks/","text":"Integrating new test frameworks in Dirigible presented some challenges that I will explore in this blog. They can serve as a \"watch-out list\" in the process of integrating any third-party libraries in Dirigible in future. Integration of Third-Party JavaScript Libraries in Dirigible In the last months I integrated (or tried to integrate) a number of third-party libraries to make them available to the server-side of the platform. License Before you start your integration efforts, which may turn out significant, make sure you are license compatible or can get along with the library author on the subject. That will spare you a lot of wasted time eventually. As you will see below sometimes the only way to integrate a third party library is to make some changes in its original code and you should ensure that you are on the safe side license-wise to do that. CommonJS CommonJS is designed as a common, standard module loading system, which is a fair attempt when there are some many yet so similar out there already. The downside of standards that are driven not by urgent and inevitable but nice, yet not critical needs is that people implement it with low priority, when they can and to the extent they feel they absolutely need to. Dirigible's scripting runtime ( Rhino ) supports CommonJS as module management systems. However, you should keep in mind that it is not entirely comparable to the same in NodeJS . So not every single NodeJS module out there is directly transferable in Dirigible as it is as far as module dependencies are concerned. I am referring to NodeJS here as a platform with pretty rich set of modules that would be beneficial for other CommonJS platforms too, but the principle is the same for any CommonJS-enabled library too. Lesson learned: Explore the dependencies of the library you try to integrate, how they are loaded and assess if Rhino can support that. Chances are that you may need to modify the library's loading mechanism and introduce its dependencies as Dirigible modules too. ECMAScript 2015 (ES6) ES6 introduces a great deal of improvements to JavaScript for good. In fact, they are sometimes so significant that applications and platforms can hardly catch up. For example, Dirigible's JavaScript runtime environment Rhino is ECMAScript 5 compliant and that's quite unfortunate when you need to integrate a great third party library that is pushing ES6 JavaScript to the edge (as they all should). There are translation engines ( Babel ) that are actually trying to fill this gap, but I had problems integrating Babel itself so for now it's not coming to the rescue. The point is that if you have e.g. Symbol in your library code you are likely lost. Sometimes there are useful polifills though so don;t rush to give up. Lesson learned: Check the JavaScript standard compliance of the third-party library. Look for Symbol or something else that's specific to ES6 if unsure. Type checks Type checks can be tricky in Rhino and not behave entirely as you might expect. For example, neither of the expressions below will evaluate to true in Rhino. Object . prototype . toString . apply ( function (){}) === '[object Function]' ; Object . prototype . toString . apply (( function (){})()) === '[object Function]' ; Object . prototype . toString . apply ( new Function ()) === '[object Function]' ; However, in Chrome for example it most certainly will. Unfortunately, the trick to use toString for type checks is quite common as it seems and eventually you may hit this problem. The fix is luckily trivial, and is to use something more conventional such as: typeof target === 'function', but it will certainly require that you modify the library code. And this son its own requires that the third-party code licenses you for such modifications. So take care to check beforehand. Lesson learned: Look for use of Object.prototype.toString.apply for type checks and modify accordingly (if possible) No globals for you Well written libraries will not tightly rely that they are executed in a browser and make it up for in browser-less environments. That includes global objects and functions. But you will find none of these in Rhino. And you may need to make it up for that if possible at all. For example (using Jasmine as example here), setTimeout is not a global function as Jasmine expects it to be and I need to create and inject it like this (note the use of Java): jasmineGlobal . console = console ; var timer = new java . util . Timer (); var counter = 1 ; var ids = {}; jasmineGlobal . setTimeout = function ( fn , delay ) { if ( fn ){ var id = counter ++ ; ids [ id ] = new JavaAdapter ( java . util . TimerTask , { run : fn }); timer . schedule ( ids [ id ], delay ); return id ; } }; In this example jamsineGlobal is initialized as 'this', which in the context of a Dirigible module is empty (and not window (as expected by Jasmine) Lesson learned: Look for browser-specific objects such as window, set/clearTimeout/setInterval/clearInterval, document or console. See what exactly is required from them and then mock them delegating to the Rhino/Dirigible environment.","title":"Integration of Third-Party JavaScript Libraries in Dirigible"},{"location":"2017/03/10/blogs-apps-testfwks/#integration-of-third-party-javascript-libraries-in-dirigible","text":"In the last months I integrated (or tried to integrate) a number of third-party libraries to make them available to the server-side of the platform.","title":"Integration of Third-Party JavaScript Libraries in Dirigible"},{"location":"2017/03/10/blogs-apps-testfwks/#license","text":"Before you start your integration efforts, which may turn out significant, make sure you are license compatible or can get along with the library author on the subject. That will spare you a lot of wasted time eventually. As you will see below sometimes the only way to integrate a third party library is to make some changes in its original code and you should ensure that you are on the safe side license-wise to do that.","title":"License"},{"location":"2017/03/10/blogs-apps-testfwks/#commonjs","text":"CommonJS is designed as a common, standard module loading system, which is a fair attempt when there are some many yet so similar out there already. The downside of standards that are driven not by urgent and inevitable but nice, yet not critical needs is that people implement it with low priority, when they can and to the extent they feel they absolutely need to. Dirigible's scripting runtime ( Rhino ) supports CommonJS as module management systems. However, you should keep in mind that it is not entirely comparable to the same in NodeJS . So not every single NodeJS module out there is directly transferable in Dirigible as it is as far as module dependencies are concerned. I am referring to NodeJS here as a platform with pretty rich set of modules that would be beneficial for other CommonJS platforms too, but the principle is the same for any CommonJS-enabled library too. Lesson learned: Explore the dependencies of the library you try to integrate, how they are loaded and assess if Rhino can support that. Chances are that you may need to modify the library's loading mechanism and introduce its dependencies as Dirigible modules too.","title":"CommonJS"},{"location":"2017/03/10/blogs-apps-testfwks/#ecmascript-2015-es6","text":"ES6 introduces a great deal of improvements to JavaScript for good. In fact, they are sometimes so significant that applications and platforms can hardly catch up. For example, Dirigible's JavaScript runtime environment Rhino is ECMAScript 5 compliant and that's quite unfortunate when you need to integrate a great third party library that is pushing ES6 JavaScript to the edge (as they all should). There are translation engines ( Babel ) that are actually trying to fill this gap, but I had problems integrating Babel itself so for now it's not coming to the rescue. The point is that if you have e.g. Symbol in your library code you are likely lost. Sometimes there are useful polifills though so don;t rush to give up. Lesson learned: Check the JavaScript standard compliance of the third-party library. Look for Symbol or something else that's specific to ES6 if unsure.","title":"ECMAScript 2015 (ES6)"},{"location":"2017/03/10/blogs-apps-testfwks/#type-checks","text":"Type checks can be tricky in Rhino and not behave entirely as you might expect. For example, neither of the expressions below will evaluate to true in Rhino. Object . prototype . toString . apply ( function (){}) === '[object Function]' ; Object . prototype . toString . apply (( function (){})()) === '[object Function]' ; Object . prototype . toString . apply ( new Function ()) === '[object Function]' ; However, in Chrome for example it most certainly will. Unfortunately, the trick to use toString for type checks is quite common as it seems and eventually you may hit this problem. The fix is luckily trivial, and is to use something more conventional such as: typeof target === 'function', but it will certainly require that you modify the library code. And this son its own requires that the third-party code licenses you for such modifications. So take care to check beforehand. Lesson learned: Look for use of Object.prototype.toString.apply for type checks and modify accordingly (if possible)","title":"Type checks"},{"location":"2017/03/10/blogs-apps-testfwks/#no-globals-for-you","text":"Well written libraries will not tightly rely that they are executed in a browser and make it up for in browser-less environments. That includes global objects and functions. But you will find none of these in Rhino. And you may need to make it up for that if possible at all. For example (using Jasmine as example here), setTimeout is not a global function as Jasmine expects it to be and I need to create and inject it like this (note the use of Java): jasmineGlobal . console = console ; var timer = new java . util . Timer (); var counter = 1 ; var ids = {}; jasmineGlobal . setTimeout = function ( fn , delay ) { if ( fn ){ var id = counter ++ ; ids [ id ] = new JavaAdapter ( java . util . TimerTask , { run : fn }); timer . schedule ( ids [ id ], delay ); return id ; } }; In this example jamsineGlobal is initialized as 'this', which in the context of a Dirigible module is empty (and not window (as expected by Jasmine) Lesson learned: Look for browser-specific objects such as window, set/clearTimeout/setInterval/clearInterval, document or console. See what exactly is required from them and then mock them delegating to the Rhino/Dirigible environment.","title":"No globals for you"},{"location":"2017/03/10/blogs-apps-tests-jasmine/","text":"Jasmine is a popular test framework that supports BDD (Behavior-Driven Development) with testing JavaScript code. It does not require DOM. And all that makes it a very good candidate for a test framework of choice for JavaScript Scripting Services in Dirigible. It is made available for you to use as a Dirigible GitHub project . Testing Server-Side JavaScript with Jasmine Writing tests with Jasmine is fun. It has interesting style of naming its functions so they sound quite natural. You literally describe what your code ( it ) is expect ed to do with nice fluent assertion API. Here's (a dumb simple) example of how it looks: $$j . describe ( \"A suite is just a function\" , function () { $$j . it ( \"and has a positive case\" , function () { $$j . expect ( false ). toBe ( true ); }); $$j . it ( \"and can have a negative case\" , function () { $$j . expect ( false ). not . toBe ( true ); }); }); It's pretty much a functional specification of your code. Hence, how it supports BDD. There's plenty of examples out there how to make use of it for client-side apps so I won't spend time on that right now. The point of this blog is to show how to make use of it for testing server side code in Dirigible. Jasmine API in Dirigible Scripting Environment Once you get the Jasmine project from GitHub into your workspace, you need to require the library first before you can make anything with it. Open up a file in the Test Cases section of your project (or create it manually if you don't have one yet) and start writing: var j = require(\"jasmine/jasmine\"); . The j variable is a reference to the Jasmine API and now you are completely into the Jasmine world. Set it up and use require to reference the code you want to test and start asserting its behavior using the Jasmine API as you normally would when testing client-side JavaScript libraries. var j = require ( \"jasmine/jasmine\" ); var jasmine = j . core ( j ); var env = jasmine . getEnv (); var $$j = j . interface ( jasmine , env ); Test runner So, you have your tests developed and now you are eager to run the suite. There are a couple of options here. You can go quite native to Jasmine and take care of integrating a test results reporter (we provide two reporters adapted for Dirigible for you out-of-the-box, one for console and one as a JSON service). Or even better, and I shall focus on this now, you can use the Jasmine Test Runner Service (require with path jasmine/jasmine_test_runner_svc ). It is designed to work with the core Test Runner module in Dirigible to deliver test results as a service in a form suitable for you user agent. If you requested the test suite file via a browser (or the Preview view in Dirigible IDE) it will redirect the results to the Test Dashboard HTML UI. And if you requested it e.g. with cUrl using Accept: application/json header it will deliver test results formatted as JSON as a response. This makes it suitable both for direct consumption or integration in third-party quality control systems and it will cost just a single line of code to enable. require ( \"jasmine/jasmine_test_runner_svc\" ). service ( env ); The test results output for HTML capable user agents looks like this: And when requested for JSON instead of HTML it will deliver the following payload: { \"tests\" : [ { \"id\" : \"spec0\" , \"name\" : \"and has a positive case\" , \"module\" : \"A suite is just a function \" , \"runtime\" : 36 , \"assertions\" : [ { \"message\" : \"Expected false to be true.\" , \"result\" : false } ], \"failed\" : true , \"total\" : 1 }, { \"id\" : \"spec1\" , \"name\" : \"and can have a negative case\" , \"module\" : \"A suite is just a function \" , \"runtime\" : 26 , \"assertions\" : [ { \"message\" : \"and can have a negative case assertion[toBe] passed.\" , \"result\" : true } ], \"failed\" : false , \"total\" : 1 } ], \"testSuite\" : { \"runtime\" : 122 , \"total\" : 2 , \"passed\" : 1 , \"failed\" : 1 } } Putting it all together Finally, putting it all together, here is the layout of Jasmine-Dirigible test suite with two test cases delivering results as a service, that we discussed so far. var j = require ( \"jasmine/jasmine\" ); var jasmine = j . core ( j ); var env = jasmine . getEnv (); var $$j = j . interface ( jasmine , env ); $$j . describe ( \"A suite is just a function\" , function () { $$j . it ( \"and has a positive case\" , function () { $$j . expect ( false ). toBe ( true ); }); $$j . it ( \"and can have a negative case\" , function () { $$j . expect ( false ). not . toBe ( true ); }); }); require ( \"jasmine/jasmine_test_runner_svc\" ). service ( env ); Now, you are all set to benefit from what Jasmine has to offer to you as a developer for JavaScript server-side code. References Jasmin project Dirigible Jasmin module GitHub project","title":"Testing Server-Side JavaScript with Jasmine"},{"location":"2017/03/10/blogs-apps-tests-jasmine/#testing-server-side-javascript-with-jasmine","text":"Writing tests with Jasmine is fun. It has interesting style of naming its functions so they sound quite natural. You literally describe what your code ( it ) is expect ed to do with nice fluent assertion API. Here's (a dumb simple) example of how it looks: $$j . describe ( \"A suite is just a function\" , function () { $$j . it ( \"and has a positive case\" , function () { $$j . expect ( false ). toBe ( true ); }); $$j . it ( \"and can have a negative case\" , function () { $$j . expect ( false ). not . toBe ( true ); }); }); It's pretty much a functional specification of your code. Hence, how it supports BDD. There's plenty of examples out there how to make use of it for client-side apps so I won't spend time on that right now. The point of this blog is to show how to make use of it for testing server side code in Dirigible.","title":"Testing Server-Side JavaScript with Jasmine"},{"location":"2017/03/10/blogs-apps-tests-jasmine/#jasmine-api-in-dirigible-scripting-environment","text":"Once you get the Jasmine project from GitHub into your workspace, you need to require the library first before you can make anything with it. Open up a file in the Test Cases section of your project (or create it manually if you don't have one yet) and start writing: var j = require(\"jasmine/jasmine\"); . The j variable is a reference to the Jasmine API and now you are completely into the Jasmine world. Set it up and use require to reference the code you want to test and start asserting its behavior using the Jasmine API as you normally would when testing client-side JavaScript libraries. var j = require ( \"jasmine/jasmine\" ); var jasmine = j . core ( j ); var env = jasmine . getEnv (); var $$j = j . interface ( jasmine , env );","title":"Jasmine API in Dirigible Scripting Environment"},{"location":"2017/03/10/blogs-apps-tests-jasmine/#test-runner","text":"So, you have your tests developed and now you are eager to run the suite. There are a couple of options here. You can go quite native to Jasmine and take care of integrating a test results reporter (we provide two reporters adapted for Dirigible for you out-of-the-box, one for console and one as a JSON service). Or even better, and I shall focus on this now, you can use the Jasmine Test Runner Service (require with path jasmine/jasmine_test_runner_svc ). It is designed to work with the core Test Runner module in Dirigible to deliver test results as a service in a form suitable for you user agent. If you requested the test suite file via a browser (or the Preview view in Dirigible IDE) it will redirect the results to the Test Dashboard HTML UI. And if you requested it e.g. with cUrl using Accept: application/json header it will deliver test results formatted as JSON as a response. This makes it suitable both for direct consumption or integration in third-party quality control systems and it will cost just a single line of code to enable. require ( \"jasmine/jasmine_test_runner_svc\" ). service ( env ); The test results output for HTML capable user agents looks like this: And when requested for JSON instead of HTML it will deliver the following payload: { \"tests\" : [ { \"id\" : \"spec0\" , \"name\" : \"and has a positive case\" , \"module\" : \"A suite is just a function \" , \"runtime\" : 36 , \"assertions\" : [ { \"message\" : \"Expected false to be true.\" , \"result\" : false } ], \"failed\" : true , \"total\" : 1 }, { \"id\" : \"spec1\" , \"name\" : \"and can have a negative case\" , \"module\" : \"A suite is just a function \" , \"runtime\" : 26 , \"assertions\" : [ { \"message\" : \"and can have a negative case assertion[toBe] passed.\" , \"result\" : true } ], \"failed\" : false , \"total\" : 1 } ], \"testSuite\" : { \"runtime\" : 122 , \"total\" : 2 , \"passed\" : 1 , \"failed\" : 1 } }","title":"Test runner"},{"location":"2017/03/10/blogs-apps-tests-jasmine/#putting-it-all-together","text":"Finally, putting it all together, here is the layout of Jasmine-Dirigible test suite with two test cases delivering results as a service, that we discussed so far. var j = require ( \"jasmine/jasmine\" ); var jasmine = j . core ( j ); var env = jasmine . getEnv (); var $$j = j . interface ( jasmine , env ); $$j . describe ( \"A suite is just a function\" , function () { $$j . it ( \"and has a positive case\" , function () { $$j . expect ( false ). toBe ( true ); }); $$j . it ( \"and can have a negative case\" , function () { $$j . expect ( false ). not . toBe ( true ); }); }); require ( \"jasmine/jasmine_test_runner_svc\" ). service ( env ); Now, you are all set to benefit from what Jasmine has to offer to you as a developer for JavaScript server-side code.","title":"Putting it all together"},{"location":"2017/03/10/blogs-apps-tests-jasmine/#references","text":"Jasmin project Dirigible Jasmin module GitHub project","title":"References"},{"location":"2017/03/10/blogs-apps-tests-jasmine1/","text":"In my previous blog I introduced Jasmine as a testing framework for server-side JavaScript. Here I will explore how to add the server console as another test results output channel. Server-Side Tests: Enabling Jasmine Test Results in Dirigible Console In my previous blog I introduced Jasmine as a testing framework for server-side JavaScript. Here I will explore how to add the server console as another test results output channel. It is rudimentary, yet handy channel. Lucky for you, it comes right out-of-the-box with the GitHub dirigiblelabs Jasmine project . To use it, you need to require the console reporter library and integrate it in Jasmine's environment. //get the console reporter library var console_reporter = require ( \"jasmine/reporters/console_reporter\" ); var jasmine = j . core ( j ); var env = jasmine . getEnv (); //add the reporter to Jasmine env env . addReporter ( console_reporter . jasmine_console_reporter ); $$j . describe ( \"A suite is just a function\" , function () { $$j . it ( \"and has a positive case\" , function () { $$j . expect ( false ). toBe ( true ); }); $$j . it ( \"and can have a negative case\" , function () { $$j . expect ( false ). not . toBe ( true ); }); }); With this setup, selecting a Jasmine test suite .js script file in the user workspace will yield results similar to the following when the test suite is done: [2017-03-09T23:11:05.131Z][info] [Jasmine started]: {totalSpecsDefined:2} [2017-03-09T23:11:05.139Z][info] [Suite started]: {id:\"suite1\", description:\"A suite is just a function\", fullName:\"A suite is just a function\", failedExpectations:[]} [2017-03-09T23:11:05.301Z][info] [Spec started]: {id:\"spec0\", description:\"and has a positive case\", fullName:\"A suite is just a function and has a positive case\", failedExpectations:[], passedExpectations:[], pendingReason:\"\"} [2017-03-09T23:11:05.321Z][info] [Spec done]: {id:\"spec0\", description:\"and has a positive case\", fullName:\"A suite is just a function and has a positive case\", failedExpectations:[{matcherName:\"toBe\", message:\"Expected false to be true.\", stack:undefined, passed:false, expected:true, actual:false}], passedExpectations:[], pendingReason:\"\", status:\"failed\"} [2017-03-09T23:11:05.327Z][info] [Spec started]: {id:\"spec1\", description:\"and can have a negative case\", fullName:\"A suite is just a function and can have a negative case\", failedExpectations:[], passedExpectations:[], pendingReason:\"\"} [2017-03-09T23:11:05.347Z][info] [Spec done]: {id:\"spec1\", description:\"and can have a negative case\", fullName:\"A suite is just a function and can have a negative case\", failedExpectations:[], passedExpectations:[{matcherName:\"toBe\", message:\"Passed.\", stack:\"\", passed:true}], pendingReason:\"\", status:\"passed\"} [2017-03-09T23:11:05.354Z][info] [Suite done]: {id:\"suite1\", description:\"A suite is just a function\", fullName:\"A suite is just a function\", failedExpectations:[], status:\"finished\"} [2017-03-09T23:11:05.360Z][info] [Jasmine done] And that's all folks. References Jasmin project Dirigible Jasmine module project","title":"Server-Side Tests: Enabling Jasmine Test Results in Dirigible Console"},{"location":"2017/03/10/blogs-apps-tests-jasmine1/#server-side-tests-enabling-jasmine-test-results-in-dirigible-console","text":"In my previous blog I introduced Jasmine as a testing framework for server-side JavaScript. Here I will explore how to add the server console as another test results output channel. It is rudimentary, yet handy channel. Lucky for you, it comes right out-of-the-box with the GitHub dirigiblelabs Jasmine project . To use it, you need to require the console reporter library and integrate it in Jasmine's environment. //get the console reporter library var console_reporter = require ( \"jasmine/reporters/console_reporter\" ); var jasmine = j . core ( j ); var env = jasmine . getEnv (); //add the reporter to Jasmine env env . addReporter ( console_reporter . jasmine_console_reporter ); $$j . describe ( \"A suite is just a function\" , function () { $$j . it ( \"and has a positive case\" , function () { $$j . expect ( false ). toBe ( true ); }); $$j . it ( \"and can have a negative case\" , function () { $$j . expect ( false ). not . toBe ( true ); }); }); With this setup, selecting a Jasmine test suite .js script file in the user workspace will yield results similar to the following when the test suite is done: [2017-03-09T23:11:05.131Z][info] [Jasmine started]: {totalSpecsDefined:2} [2017-03-09T23:11:05.139Z][info] [Suite started]: {id:\"suite1\", description:\"A suite is just a function\", fullName:\"A suite is just a function\", failedExpectations:[]} [2017-03-09T23:11:05.301Z][info] [Spec started]: {id:\"spec0\", description:\"and has a positive case\", fullName:\"A suite is just a function and has a positive case\", failedExpectations:[], passedExpectations:[], pendingReason:\"\"} [2017-03-09T23:11:05.321Z][info] [Spec done]: {id:\"spec0\", description:\"and has a positive case\", fullName:\"A suite is just a function and has a positive case\", failedExpectations:[{matcherName:\"toBe\", message:\"Expected false to be true.\", stack:undefined, passed:false, expected:true, actual:false}], passedExpectations:[], pendingReason:\"\", status:\"failed\"} [2017-03-09T23:11:05.327Z][info] [Spec started]: {id:\"spec1\", description:\"and can have a negative case\", fullName:\"A suite is just a function and can have a negative case\", failedExpectations:[], passedExpectations:[], pendingReason:\"\"} [2017-03-09T23:11:05.347Z][info] [Spec done]: {id:\"spec1\", description:\"and can have a negative case\", fullName:\"A suite is just a function and can have a negative case\", failedExpectations:[], passedExpectations:[{matcherName:\"toBe\", message:\"Passed.\", stack:\"\", passed:true}], pendingReason:\"\", status:\"passed\"} [2017-03-09T23:11:05.354Z][info] [Suite done]: {id:\"suite1\", description:\"A suite is just a function\", fullName:\"A suite is just a function\", failedExpectations:[], status:\"finished\"} [2017-03-09T23:11:05.360Z][info] [Jasmine done] And that's all folks.","title":"Server-Side Tests: Enabling Jasmine Test Results in Dirigible Console"},{"location":"2017/03/10/blogs-apps-tests-jasmine1/#references","text":"Jasmin project Dirigible Jasmine module project","title":"References"},{"location":"2018/02/02/blogs-3x-a-new-era-began/","text":"Hello Dirigibles! Let the new era begin! As a community-driven project, we stick to a very important principle that helps us navigate throughout the new emerging technologies and focus on the right priorities: We Listen! Based on the feedback we have received so far, we had to take a few major decisions: Moved to the standard Java/Maven development model for the core components The OSGi-based development model coming with Maven and Tycho integration, Orbit repositories and target platform definitions was listed many times as something too complex by the younger generation of developers who tried to understand the project source code and become contributors. The comparison was often with Spring Boot and other similar frameworks and projects. The decision was not easy, but we jumped to pure Maven project structure. The major benefit from this decision is that now you can build your own stack including also external dependencies more easily and in a more natural way. Moved from Eclipse RAP to pure Angular 1.x/Boostrap/GoldenLayout for WebIDE Of all the technologies we have been using, Eclipse RAP has been the most stable one. As a Web port of the Eclipse Rich Client Platform (RCP), we expected that the thousands of Eclipse plugin developers would recognize Dirigible as the easiest way to port their existing plugins to the Web or just to leverage their experience to create some brand new Cloud-related plugins. This did not happen. We decided to go to pure Angular 1.x/Boostrap/GoldenLayout user interfaces even for the WebIDE parts. The front-end now uses a set of RESTful services for workspace management, lifecycle management, repository, database, documents, etc., following the standard Web 2.0 approach. We have chosen that approach for building applications with Dirigible, hence now we are developing Dirigible with Dirigible itself. Finally we can say that! Adapt V8 engine in the API layer The JavaScript language and the community around it is quite an interesting phenomenon. The language itself, acknowledged as de facto THE front-end programming language, in recent years has also started gaining attention as a back-end language. There are several engines that were used for that and still exist, but now it seems we have a clear winner - V8. It supports the latest language specification and in most cases it is the fastest one. We adopted it via the great J2V8 bridge. As a side effect, we had to reimplement the Enterprise Javascript API layer to support such a non-JVM engine. Webjars for applications content To comply with the legacy of CI/CD processes, where requirements such as reproducibility, immutability, testability are must-have features of the underlying framework, we had to add a new approach of packaging of the applications for production. We decided to follow the so called \"webjars\" structures, where the application content (e.g. HTML files, database definitions, extensions, etc.) are packed as a standard Java archive and are accessible at runtime in the same way as they reside within the repository. In this way, nobody can change them once they are built into a deployable archive. The side benefit is that they can be distributed in the same way as the rest of the core modules - as standard Maven dependencies. The focus, the development model and the main goal - reconfirmed What remained and was reconfirmed was the focus on the business applications development in the cloud. Not just a general purpose IDE or a platform, but tailored for specific scenarios required by the businesses to optimize their processes. In-system programming was the clear differentiator giving an unique development experience and the fastest turn-around time in the Cloud. It was a hard period for all contributors to reimplement almost the whole stack from scratch - but we did it! In the next few days, we plan to publish a series of blog articles explaining how exactly we did it and how you can use it - to extend, configure, and run Dirigible in the most optimal way.","title":"3.x Series - A new era has begun!"},{"location":"2018/02/02/blogs-3x-a-new-era-began/#let-the-new-era-begin","text":"As a community-driven project, we stick to a very important principle that helps us navigate throughout the new emerging technologies and focus on the right priorities: We Listen! Based on the feedback we have received so far, we had to take a few major decisions:","title":"Let the new era begin!"},{"location":"2018/02/02/blogs-3x-a-new-era-began/#moved-to-the-standard-javamaven-development-model-for-the-core-components","text":"The OSGi-based development model coming with Maven and Tycho integration, Orbit repositories and target platform definitions was listed many times as something too complex by the younger generation of developers who tried to understand the project source code and become contributors. The comparison was often with Spring Boot and other similar frameworks and projects. The decision was not easy, but we jumped to pure Maven project structure. The major benefit from this decision is that now you can build your own stack including also external dependencies more easily and in a more natural way.","title":"Moved to the standard Java/Maven development model for the core components"},{"location":"2018/02/02/blogs-3x-a-new-era-began/#moved-from-eclipse-rap-to-pure-angular-1xboostrapgoldenlayout-for-webide","text":"Of all the technologies we have been using, Eclipse RAP has been the most stable one. As a Web port of the Eclipse Rich Client Platform (RCP), we expected that the thousands of Eclipse plugin developers would recognize Dirigible as the easiest way to port their existing plugins to the Web or just to leverage their experience to create some brand new Cloud-related plugins. This did not happen. We decided to go to pure Angular 1.x/Boostrap/GoldenLayout user interfaces even for the WebIDE parts. The front-end now uses a set of RESTful services for workspace management, lifecycle management, repository, database, documents, etc., following the standard Web 2.0 approach. We have chosen that approach for building applications with Dirigible, hence now we are developing Dirigible with Dirigible itself. Finally we can say that!","title":"Moved from Eclipse RAP to pure Angular 1.x/Boostrap/GoldenLayout for WebIDE"},{"location":"2018/02/02/blogs-3x-a-new-era-began/#adapt-v8-engine-in-the-api-layer","text":"The JavaScript language and the community around it is quite an interesting phenomenon. The language itself, acknowledged as de facto THE front-end programming language, in recent years has also started gaining attention as a back-end language. There are several engines that were used for that and still exist, but now it seems we have a clear winner - V8. It supports the latest language specification and in most cases it is the fastest one. We adopted it via the great J2V8 bridge. As a side effect, we had to reimplement the Enterprise Javascript API layer to support such a non-JVM engine.","title":"Adapt V8 engine in the API layer"},{"location":"2018/02/02/blogs-3x-a-new-era-began/#webjars-for-applications-content","text":"To comply with the legacy of CI/CD processes, where requirements such as reproducibility, immutability, testability are must-have features of the underlying framework, we had to add a new approach of packaging of the applications for production. We decided to follow the so called \"webjars\" structures, where the application content (e.g. HTML files, database definitions, extensions, etc.) are packed as a standard Java archive and are accessible at runtime in the same way as they reside within the repository. In this way, nobody can change them once they are built into a deployable archive. The side benefit is that they can be distributed in the same way as the rest of the core modules - as standard Maven dependencies.","title":"Webjars for applications content"},{"location":"2018/02/02/blogs-3x-a-new-era-began/#the-focus-the-development-model-and-the-main-goal-reconfirmed","text":"What remained and was reconfirmed was the focus on the business applications development in the cloud. Not just a general purpose IDE or a platform, but tailored for specific scenarios required by the businesses to optimize their processes. In-system programming was the clear differentiator giving an unique development experience and the fastest turn-around time in the Cloud. It was a hard period for all contributors to reimplement almost the whole stack from scratch - but we did it! In the next few days, we plan to publish a series of blog articles explaining how exactly we did it and how you can use it - to extend, configure, and run Dirigible in the most optimal way.","title":"The focus, the development model and the main goal - reconfirmed"},{"location":"2018/02/22/blogs-eclipse-dirigible/","text":"This article was republished from Eclipse Newsletter , February 2018 Overview Dirigible is an open-source cloud development platform, part of the Eclipse foundation and the top-level Eclipse Cloud Development project. The ultimate goal of the platform is to provide software developers with the right toolset for building, running, and operating business applications in the cloud. To achieve this goal, Dirigible provides both independent Design Time and Runtime components. Mission Nowadays, providing a full-stack application development platform is not enough. Building and running on top of it has to be fast and smooth! Having that in mind, slow and cumbersome \"Build\" , \"CI\" , and \"Deployment\" processes have a direct impact on development productivity. In this line of thought, it isn't hard to imagine that the Java development model for web applications doesn't fit in the cloud world. Luckily, one of the strongest advantages of Dirigible comes at hand - the In-System Development model. Right from the early days of Dirigible, it was clear that it is going to be the platform for Business Applications Development in the cloud and not just another general purpose IDE in the browser. The reason for that decision is pretty simple - \"One size doesn't fit all\" ! Making a choice between providing \"In-System Development\" in the cloud and adding support for a new language (Java, C#, PHP, ...) , is really easy. The new language doesn't really add much to the uniqueness and usability of the platform, as the In-System development model does! Architecture The goal of the In-System development model is to ultimately change the state of the system while it's up and running, without affecting the overall performance and without service degradation. You can easily think of several such systems like Programmable Microcontrollers, Relational Database Management Systems, ABAP. As mentioned earlier, Dirigible provides a suitable design time and runtime for that, so let's talk a little bit about the architecture. The Dirigible stack is pretty simple: The building blocks are: Application Server (provided) Runtime (built-in) Engine(s) - (Rhino/Nashorn/V8) Repository - (fs/database) Design Time (built-in) Web IDE (workspace/database/git/... perspective) Applications (developed) Application (database/rest/ui) Application (indexing/messaging/job) Application (extensionpoint/extension) ... Enterprise JavaScript The language of choice in the Dirigible business application platform is JavaScript ! But why JavaScript? Why not Java? Is it mature enough, is it scalable, can it satisfy the business application needs? The answer is: It sure does! The code that is being written is similar to Java. The developers can write their business logic in a synchronous fashion and can leverage a large set of Enterprise JavaScript APIs. For heavy loads, the Dirigible stack performs better than the NodeJS due to multithreading of the underlying JVM and the application server, and using the same V8 engine underneath. Examples Request/Response API var response = require ( 'http/v3/response' ); response . println ( \"Hello World!\" ); response . flush (); response . close (); Database API : var database = require ( 'db/v3/database' ); var response = require ( 'http/v3/response' ); var connection = database . getConnection (); try { var statement = connection . prepareStatement ( \"select * from MY_TABLE where MY_PATH like ?\" ); var i = 0 ; statement . setString ( ++ i , \"%\" ); var resultSet = statement . executeQuery (); while ( resultSet . next ()) { response . println ( \"[path]: \" + resultSet . getString ( \"MY_PATH\" )); } resultSet . close (); statement . close (); } catch ( e ) { console . trace ( e ); response . println ( e . message ); } finally { connection . close (); } response . flush (); response . close (); The provided Enterprise JavaScript APIs leverage some of the mature Java specifications and de facto standards (e.g. JDBC, Servlet, CMIS, ActiveMQ, File System, Streams, etc.). Eliminating the build process (due to the lack of compilation) and at the same time exposing proven frameworks (that does the heavy lifting) , results in having the perfect environment for in-system development of business applications, with close to \"Zero Turn-Around-Time\" . In conclusion, the Dirigible platform is really tailored to the needs of Business Application Developers . Getting Started Download Get the latest release from: http://download.eclipse.org/dirigible The latest master branch can be found at: https://github.com/eclipse/dirigible Download the latest Tomcat 8.x from: https://tomcat.apache.org/download-80.cgi NOTE: You can use the try out instance, that is available at http://dirigible.eclipse.org and skip through the Develop section Start Put the ROOT.war into the ${tomcat-dir}/webapps directory Execute ./catalina.sh start from the ${tomcat-dir}/bin directory Login Open: http://localhost:8080 Log in with the default dirigible/dirigible credentials Develop Project Create a project Click + -> Project Database table Generate a Database Table Right-click New > Generate > Database table Edit the students.table definition { \"name\" : \"Students\" , \"type\" : \"TABLE\" , \"columns\" : [{ \"name\" : \"ID\" , \"type\" : \"INTEGER\" , \"primaryKey\" : \"true\" }, { \"name\" : \"FIRST_NAME\" , \"type\" : \"VARCHAR\" , \"length\" : \"50\" }, { \"name\" : \"LAST_NAME\" , \"type\" : \"VARCHAR\" , \"length\" : \"50\" }, { \"name\" : \"AGE\" , \"type\" : \"INTEGER\" }] } Publish Right-click the project and select Publish NOTE: The auto publish function is enabled by default Explore The database scheme can be explored from the Database perspective Click Window > Open Perspective > Database Insert some sample data insert into students values ( 1 , 'John' , 'Doe' , 25 ) insert into students values ( 2 , 'Jane' , 'Doe' , 23 ) Note: The perspectives are available also from the side menu REST service Generate a Hello World service Right-click New > Generate > Hello World Edit the students.js service var database = require ( 'db/v3/database' ); var response = require ( 'http/v3/response' ); var students = listStudents (); response . println ( students ); response . flush (); response . close (); function listStudents () { let students = []; var connection = database . getConnection (); try { var statement = connection . prepareStatement ( \"select * from STUDENTS\" ); var resultSet = statement . executeQuery (); while ( resultSet . next ()) { students . push ({ 'id' : resultSet . getInt ( 'ID' ), 'firstName' : resultSet . getString ( 'FIRST_NAME' ), 'lastName' : resultSet . getString ( 'LAST_NAME' ), 'age' : resultSet . getInt ( 'AGE' ) }); } resultSet . close (); statement . close (); } catch ( e ) { console . error ( e ); response . println ( e . message ); } finally { connection . close (); } return students ; } Explore The student.js service is accessible through the Preview view NOTE: All backend services are up and running after save/publish, due to the In-System Development Create a UI Generate a HTML5 (AngularJS) page Right-click New > Generate > HTML5 (AngularJS) Edit the page <!DOCTYPE html> < html lang = \"en\" ng-app = \"page\" > < head > < meta charset = \"utf-8\" > < meta http-equiv = \"X-UA-Compatible\" content = \"IE=edge\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1.0\" > < meta name = \"description\" content = \"\" > < meta name = \"author\" content = \"\" > < link type = \"text/css\" rel = \"stylesheet\" href = \"/services/v3/core/theme/bootstrap.min.css\" > < link type = \"text/css\" rel = \"stylesheet\" href = \"/services/v3/web/resources/font-awesome-4.7.0/css/font-awesome.min.css\" > < script type = \"text/javascript\" src = \"/services/v3/web/resources/angular/1.4.7/angular.min.js\" ></ script > < script type = \"text/javascript\" src = \"/services/v3/web/resources/angular/1.4.7/angular-resource.min.js\" ></ script > </ head > < body ng-controller = \"PageController\" > < div > < div class = \"page-header\" > < h1 > Students </ h1 > </ div > < div class = \"container\" > < table class = \"table table-hover\" > < thead > < th > Id </ th > < th > First Name </ th > < th > Last Name </ th > < th > Age </ th > </ thead > < tbody > < tr ng-repeat = \"student in students\" > {% raw %} < td > {{student.id}} </ td > < td > {{student.firstName}} </ td > < td > {{student.lastName}} </ td > < td > {{student.age}} </ td > {% endraw %} </ tr > </ tbody > </ table > </ div > </ div > < script type = \"text/javascript\" > angular . module ( 'page' , []); angular . module ( 'page' ). controller ( 'PageController' , function ( $scope , $http ) { $http . get ( '../../js/university/students.js' ) . success ( function ( data ) { $scope . students = data ; }); }); </ script > </ body > </ html > \"What's next?\" The In-System Development model provides the business application developers with the right toolset for rapid application development. By leveraging a few built-in templates and the Enterprise JavaScript API , whole vertical scenarios can be set up in several minutes. With the close to Zero Turn-Around-Time , changes in the backend can be made and applied on the fly, through an elegant Web IDE. The perfect fit for your digital transformation ! The goal of the Dirigible platform is clear - ease the developers as much as possible and let them concentrate on the development of critical business logic. So, what's next? Can I provide my own set of templates? Can I expose a new Enterprise JavaScript API? Can I provide a new perspective/view? Can I build my own Dirigible stack? Can it be integrated with the services of my cloud provider? To all these questions, the answer is simple: Yes , you can do it! Resources Site: http://www.dirigible.io Help: http://www.dirigible.io/help/ API: http://www.dirigible.io/api/ YouTube: https://www.youtube.com/c/dirigibleio Facebook: https://www.facebook.com/dirigible.io Twitter: https://twitter.com/dirigible_io Enjoy!","title":"Eclipse Dirigible - Getting Started"},{"location":"2018/02/22/blogs-eclipse-dirigible/#overview","text":"Dirigible is an open-source cloud development platform, part of the Eclipse foundation and the top-level Eclipse Cloud Development project. The ultimate goal of the platform is to provide software developers with the right toolset for building, running, and operating business applications in the cloud. To achieve this goal, Dirigible provides both independent Design Time and Runtime components.","title":"Overview"},{"location":"2018/02/22/blogs-eclipse-dirigible/#mission","text":"Nowadays, providing a full-stack application development platform is not enough. Building and running on top of it has to be fast and smooth! Having that in mind, slow and cumbersome \"Build\" , \"CI\" , and \"Deployment\" processes have a direct impact on development productivity. In this line of thought, it isn't hard to imagine that the Java development model for web applications doesn't fit in the cloud world. Luckily, one of the strongest advantages of Dirigible comes at hand - the In-System Development model. Right from the early days of Dirigible, it was clear that it is going to be the platform for Business Applications Development in the cloud and not just another general purpose IDE in the browser. The reason for that decision is pretty simple - \"One size doesn't fit all\" ! Making a choice between providing \"In-System Development\" in the cloud and adding support for a new language (Java, C#, PHP, ...) , is really easy. The new language doesn't really add much to the uniqueness and usability of the platform, as the In-System development model does!","title":"Mission"},{"location":"2018/02/22/blogs-eclipse-dirigible/#architecture","text":"The goal of the In-System development model is to ultimately change the state of the system while it's up and running, without affecting the overall performance and without service degradation. You can easily think of several such systems like Programmable Microcontrollers, Relational Database Management Systems, ABAP. As mentioned earlier, Dirigible provides a suitable design time and runtime for that, so let's talk a little bit about the architecture. The Dirigible stack is pretty simple: The building blocks are: Application Server (provided) Runtime (built-in) Engine(s) - (Rhino/Nashorn/V8) Repository - (fs/database) Design Time (built-in) Web IDE (workspace/database/git/... perspective) Applications (developed) Application (database/rest/ui) Application (indexing/messaging/job) Application (extensionpoint/extension) ...","title":"Architecture"},{"location":"2018/02/22/blogs-eclipse-dirigible/#enterprise-javascript","text":"The language of choice in the Dirigible business application platform is JavaScript ! But why JavaScript? Why not Java? Is it mature enough, is it scalable, can it satisfy the business application needs? The answer is: It sure does! The code that is being written is similar to Java. The developers can write their business logic in a synchronous fashion and can leverage a large set of Enterprise JavaScript APIs. For heavy loads, the Dirigible stack performs better than the NodeJS due to multithreading of the underlying JVM and the application server, and using the same V8 engine underneath.","title":"Enterprise JavaScript"},{"location":"2018/02/22/blogs-eclipse-dirigible/#examples","text":"Request/Response API var response = require ( 'http/v3/response' ); response . println ( \"Hello World!\" ); response . flush (); response . close (); Database API : var database = require ( 'db/v3/database' ); var response = require ( 'http/v3/response' ); var connection = database . getConnection (); try { var statement = connection . prepareStatement ( \"select * from MY_TABLE where MY_PATH like ?\" ); var i = 0 ; statement . setString ( ++ i , \"%\" ); var resultSet = statement . executeQuery (); while ( resultSet . next ()) { response . println ( \"[path]: \" + resultSet . getString ( \"MY_PATH\" )); } resultSet . close (); statement . close (); } catch ( e ) { console . trace ( e ); response . println ( e . message ); } finally { connection . close (); } response . flush (); response . close (); The provided Enterprise JavaScript APIs leverage some of the mature Java specifications and de facto standards (e.g. JDBC, Servlet, CMIS, ActiveMQ, File System, Streams, etc.). Eliminating the build process (due to the lack of compilation) and at the same time exposing proven frameworks (that does the heavy lifting) , results in having the perfect environment for in-system development of business applications, with close to \"Zero Turn-Around-Time\" . In conclusion, the Dirigible platform is really tailored to the needs of Business Application Developers .","title":"Examples"},{"location":"2018/02/22/blogs-eclipse-dirigible/#getting-started","text":"Download Get the latest release from: http://download.eclipse.org/dirigible The latest master branch can be found at: https://github.com/eclipse/dirigible Download the latest Tomcat 8.x from: https://tomcat.apache.org/download-80.cgi NOTE: You can use the try out instance, that is available at http://dirigible.eclipse.org and skip through the Develop section Start Put the ROOT.war into the ${tomcat-dir}/webapps directory Execute ./catalina.sh start from the ${tomcat-dir}/bin directory Login Open: http://localhost:8080 Log in with the default dirigible/dirigible credentials Develop Project Create a project Click + -> Project Database table Generate a Database Table Right-click New > Generate > Database table Edit the students.table definition { \"name\" : \"Students\" , \"type\" : \"TABLE\" , \"columns\" : [{ \"name\" : \"ID\" , \"type\" : \"INTEGER\" , \"primaryKey\" : \"true\" }, { \"name\" : \"FIRST_NAME\" , \"type\" : \"VARCHAR\" , \"length\" : \"50\" }, { \"name\" : \"LAST_NAME\" , \"type\" : \"VARCHAR\" , \"length\" : \"50\" }, { \"name\" : \"AGE\" , \"type\" : \"INTEGER\" }] } Publish Right-click the project and select Publish NOTE: The auto publish function is enabled by default Explore The database scheme can be explored from the Database perspective Click Window > Open Perspective > Database Insert some sample data insert into students values ( 1 , 'John' , 'Doe' , 25 ) insert into students values ( 2 , 'Jane' , 'Doe' , 23 ) Note: The perspectives are available also from the side menu REST service Generate a Hello World service Right-click New > Generate > Hello World Edit the students.js service var database = require ( 'db/v3/database' ); var response = require ( 'http/v3/response' ); var students = listStudents (); response . println ( students ); response . flush (); response . close (); function listStudents () { let students = []; var connection = database . getConnection (); try { var statement = connection . prepareStatement ( \"select * from STUDENTS\" ); var resultSet = statement . executeQuery (); while ( resultSet . next ()) { students . push ({ 'id' : resultSet . getInt ( 'ID' ), 'firstName' : resultSet . getString ( 'FIRST_NAME' ), 'lastName' : resultSet . getString ( 'LAST_NAME' ), 'age' : resultSet . getInt ( 'AGE' ) }); } resultSet . close (); statement . close (); } catch ( e ) { console . error ( e ); response . println ( e . message ); } finally { connection . close (); } return students ; } Explore The student.js service is accessible through the Preview view NOTE: All backend services are up and running after save/publish, due to the In-System Development Create a UI Generate a HTML5 (AngularJS) page Right-click New > Generate > HTML5 (AngularJS) Edit the page <!DOCTYPE html> < html lang = \"en\" ng-app = \"page\" > < head > < meta charset = \"utf-8\" > < meta http-equiv = \"X-UA-Compatible\" content = \"IE=edge\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1.0\" > < meta name = \"description\" content = \"\" > < meta name = \"author\" content = \"\" > < link type = \"text/css\" rel = \"stylesheet\" href = \"/services/v3/core/theme/bootstrap.min.css\" > < link type = \"text/css\" rel = \"stylesheet\" href = \"/services/v3/web/resources/font-awesome-4.7.0/css/font-awesome.min.css\" > < script type = \"text/javascript\" src = \"/services/v3/web/resources/angular/1.4.7/angular.min.js\" ></ script > < script type = \"text/javascript\" src = \"/services/v3/web/resources/angular/1.4.7/angular-resource.min.js\" ></ script > </ head > < body ng-controller = \"PageController\" > < div > < div class = \"page-header\" > < h1 > Students </ h1 > </ div > < div class = \"container\" > < table class = \"table table-hover\" > < thead > < th > Id </ th > < th > First Name </ th > < th > Last Name </ th > < th > Age </ th > </ thead > < tbody > < tr ng-repeat = \"student in students\" > {% raw %} < td > {{student.id}} </ td > < td > {{student.firstName}} </ td > < td > {{student.lastName}} </ td > < td > {{student.age}} </ td > {% endraw %} </ tr > </ tbody > </ table > </ div > </ div > < script type = \"text/javascript\" > angular . module ( 'page' , []); angular . module ( 'page' ). controller ( 'PageController' , function ( $scope , $http ) { $http . get ( '../../js/university/students.js' ) . success ( function ( data ) { $scope . students = data ; }); }); </ script > </ body > </ html >","title":"Getting Started"},{"location":"2018/02/22/blogs-eclipse-dirigible/#whats-next","text":"The In-System Development model provides the business application developers with the right toolset for rapid application development. By leveraging a few built-in templates and the Enterprise JavaScript API , whole vertical scenarios can be set up in several minutes. With the close to Zero Turn-Around-Time , changes in the backend can be made and applied on the fly, through an elegant Web IDE. The perfect fit for your digital transformation ! The goal of the Dirigible platform is clear - ease the developers as much as possible and let them concentrate on the development of critical business logic. So, what's next? Can I provide my own set of templates? Can I expose a new Enterprise JavaScript API? Can I provide a new perspective/view? Can I build my own Dirigible stack? Can it be integrated with the services of my cloud provider? To all these questions, the answer is simple: Yes , you can do it!","title":"\"What's next?\""},{"location":"2018/02/22/blogs-eclipse-dirigible/#resources","text":"Site: http://www.dirigible.io Help: http://www.dirigible.io/help/ API: http://www.dirigible.io/api/ YouTube: https://www.youtube.com/c/dirigibleio Facebook: https://www.facebook.com/dirigible.io Twitter: https://twitter.com/dirigible_io Enjoy!","title":"Resources"},{"location":"2018/03/02/blogs-3x-ide/","text":"Dirigible is a cloud runtime platform that comes with a neat, all-in-one, frustration-free package of devops productivity tools, including a brand new cloud IDE for in-system application development. This blog is a getting-to-know the new cloud IDE. Meet and greet A picture's worth a thousand words they say... We took our years-long experience in building high-productivity web-based IDE and used it to create a completely new cloud IDE from ground up , using exclusively lean, web-native UI technologies (vanilla JS, JQuery , AngularJS , Twitter Bootstrap ) backed by lightweight services. The new design follows the latest trends in the segment, enriched with some unique flavors, such as interactive work plot personalization (a.k.a. as perspective in Eclipse) to provide a real-world, high-productivity finishing. Our primary design goals (in short) were to deliver - functionally rich, desktop-like experience (performant, reliable) - highly configurable and interactively composable working area - highly extensible and customizable core - leveraging popular in the community, web-native technologies and frameworks Building blocks From end user perspective, the IDE is composition of perspectives , each consisting of the necessary tools to accomplish certain goal. There are three areas with fixed positions: - top-area toolbar for the menus , theme selection and user control; - sidebar to the left, with shortcuts e.g. to the perspectives; - statusbar at the bottom, for notifications and other use by the tools. The tools that constitute perspectives are laid out in different (predefined) regions of the work plot, but their position is not fixed and can be changed by dragging it to a new one. The perspectives are simply predefined configurations and end users are free to open , move or close tools on the work plot of a perspective as they see fit. The tools can also be maximized or minimized , even popped out in own window. The tools are the minimal atomic parts in the IDE. They are referred to as views or editors and each type is handled differently. What's in the box The IDE package contains the Workbench , Git , Database , Repository and Terminal perspectives. The list is neither limited, nor fixed. Workbench The Workbench perspective comprises Workspace , Import , Properties , Console and Preview views, plus the editors registered for each file type. In a word, the minimal toolset for file management, preview and editing operations. A very handy feature is also the template-based project and artifacts scaffolding generators. New in 3.x - Multiple workspaces per user . User now can create multiple workspaces and use the Workspace view to manage and switch between them. - Non-normative project file structure . The projects file organization is now non-normative and entirely up-to your preferences. - Multiple editors . The IDE now supports multiple editors registered for different file (MIME) types. More than one editor can be registered for one file type and in this case a \" Open with... \" context menu entry is rendered for the user to select, which one to use. Git Git perspective is built from tools supporting Git client operations. It doesn't aim to be what EGit is to Eclipse, but rather to present a simplified interface for the most common operations, such as cloning a repository to a workspace, pulling changes, and pushing commits. For more sophisticated command-line interaction see Terminal Database The Database perspective is essential for full stack developers. It features a database explorer, a console to execute SQL statements and preview results in table format. Repository The Repository perspective offers explorer for the repository of the Dirigible instance where the IDE is running, and import/export snapshots. Terminal The key view in the perspective is a terminal that emulates console client connected to the environment of the Dirigible IDE that can execute commands. Editors The editors that have been integrated are Orion and Ace . More sophisticated and also visual editors are in the pipeline. Core UI framework The Core UI framework is an Angular module ( ideUiCore ) that exposes a number of key components, building the backbone of perspectives and tools in the cloud IDE. Many of these can be further customized and some are part of larger compositions (e.g. the Angular directives for menu, sidebar or statusbar). The design encourages customizations if you are up to some tricky scenario or just rolling out a whole new fully functional perspective with a just a couple lines of code. To give you the idea how literally true is that, here is a clue: < body ng-app = \"workbench\" ng-controller = \"WorkbenchController as wb\" > < div menu menu-data-url = \"../../js/ide/services/menu.js\" ></ div > < div class = \"shell\" > < div class = \"sidebar list-group\" sidebar active = \"Workbench\" ></ div > < div id = \"workbench\" class = \"plane\" views-layout views-layout-model = \"wb.layoutModel\" ></ div > </ div > < div class = \"statusbar\" status-bar > {{message}} </ div > </ body > In this HTML template, the directives menu , sidebar , views-layout and statusbar will build a standard perspective layout for you. All you need to add to that (in the corresponding controller) is, which are the views that will be part of the perspective (note the views array property in the layoutModel member below): ... . controller ( 'WorkbenchController' , [ 'Layouts' , function ( Layouts ){ this . layoutModel = { views : [ 'workspace' , 'import' , 'editor' , 'properties' , 'console' , 'preview' ] ... } The components in the core UI framework provide a coherent look and feel for the perspectives and their views and editors in the IDE. A brief description for some of the key components follows. Message Hub The IDE building blocks work together in choreography . An action in the Workspace view for example opens-up an editor, or triggers the Preview view to send a request to a service. The communication between the isolated choreography participants is decentralized, and realized by a highly performant, entirely client-side framework - the Message Hub . The Message Hub is a self-contained (no dependencies) JS library that leverages the HTML5 messaging API to convey messages across the UI component isolation (iframe) boundaries via a simplified, yet highly customizable interface. It works across domains if necessary, yet implementing all best-practices for secure cross-origin client-side communication. Here is a hint for the simplicity of the API: A. Load the library < script type = \"text/javascript\" src = \"ui/message-hub.js\" ></ script > <!-- and the following for Angular Apps --> < script type = \"text/javascript\" src = \"ui/ui-layout.js\" ></ script > < script type = \"text/javascript\" src = \"ui/ui-core-ng-modules.js\" ></ script > B. Get an instance - With vanilla JS var messageHub = new FramesMessageHub (); Or in Anguar app angular . module ( 'App' , [ 'ideUiCore' ]) ... . controller ( 'SomeController' , [ 'messageHub' , function ( messageHub ){ ... C. Subscirbe for messages of type 'namespace.messageName' javascript , messageHub.on('namespace.messageName', function(message, postEvent){ //do something... }); D. Post a message to subscribers for the 'namespace.messageName' messages messageHub . send ( 'namespace.messageName' , message ); Using the message hub framework, we build highly performant loosely coupled integrations between isolated components, reducing the boilerplate in setting up communication channels to the bare minimum. Layouts The cloud IDE layout API delegates the layout management to the GoldenLayout framework \u2013 a \u201c multi-window JavaScript layout manager for webapps \u201c, as the author defines it. It handles all the magic of laying things out, stacking, moving interactively, popping out, maximize/minimize, close and open of views and editors. The angular directive views-layout encapsulates all the complexity of initializing the layout of views in a perspective and reduces it all to a very simple configuration. < div id = \"workbench\" class = \"plane\" views-layout views-layout-model = \"wb.layoutModel\" ></ div > In that code snippet above, wb.layoutModel is a JSON like: { views : [ 'workspace' , 'import' , 'editor' , 'properties' , 'console' , 'preview' ] } The strings in the views array are in fact identifiers for the views registered in the Views Registry maintained by the framework. The registry entries provide essential configuration information for the views, such as their intended region on the work plot, factory function, label and specific configuration. The registry is populated by the views.js service. Layouts is a convenience bag of functions that significantly simplifies the work with layouts. It takes care of views registry setup, the work plot regions configuration, layout initialization, serialization, control on the layout manager, open view and open editor functions, global notifications and others. For example, the API to open programmatically a view (after layout initialization): Layouts.manager.open(viewId, regionId); Toolbar The Toolbar is a composite that aggregates the drop-down menus, the theme selection, the user name and sign-out control. It uses the corresponding UI microservices available in the ideUiCore module as Menu , User and Theme . Having a fully functional standard template menu in a perspective costs as much as this line: < div menu menu-data-url = \"../../js/ide/services/menu.js\" ></ div > The Angular directive menu will take care to render it as appropriate and all it needs for that is the (perspective-specific) URL of a service that will provide the menu items configuration. By convention, all UI components are built with Bootstrap 3.x CSS and the themes in the cloud IDE are actually custom Bootstrap CSS. A UI microservice enables dynamic change of the CSS upon change of the theme automatically. It is available as Angular Factory Theme . The Angular service User provides the details for the user that are rendered by the Menu directive, such as the user name. Sidebar The sidebar is Angular directive that takes care to render a standard sidebar in the framework template. It works with the perspectives.js service to populate the registered perspectives as shortcuts. Statusbar The statusbar is an Angular directive that renders a standard, fixed-position footer. The component is subscribed to listen to message types configured as value of the status-bar-topic attribute, or by default to status-message messages. Extensibility and Dynamic Compositions The content of the Menu, Sidebar, Open With (editor) action list, the views/editors in a perspective and their positions on the work plot are all examples of configurations that are delivered at runtime by Dirigible services. They leverage the extensions concept in Dirigible, so integrating a new menu (for example) is as easy as a Dirigible module \u201cadvertising\u201d itself (see file.extension ) to the corresponding Dirigible extension point (see menu.extensionpoint ). At runtime, the extensions will be invoked dynamically, their results aggregated and ultimately delivered to the front-end by a service responsible to mashup everything into a lean consumable JSON model (see menu.js ). The whole mechanism shares concepts with the Eclipse plugin framework. It detaches the client-side, which would render anything that can satisfy its data format needs, and server-side data composition mechanisms from the data contributors which are resolved and invoked dynamically. Therefore, to extend the system, all you need to do is to focus on providing a compliant data contributor and the system will take care to resolve and request it for its piece of data when required. Enjoy!","title":"Dirigible Cloud IDE"},{"location":"2018/03/02/blogs-3x-ide/#meet-and-greet","text":"A picture's worth a thousand words they say... We took our years-long experience in building high-productivity web-based IDE and used it to create a completely new cloud IDE from ground up , using exclusively lean, web-native UI technologies (vanilla JS, JQuery , AngularJS , Twitter Bootstrap ) backed by lightweight services. The new design follows the latest trends in the segment, enriched with some unique flavors, such as interactive work plot personalization (a.k.a. as perspective in Eclipse) to provide a real-world, high-productivity finishing. Our primary design goals (in short) were to deliver - functionally rich, desktop-like experience (performant, reliable) - highly configurable and interactively composable working area - highly extensible and customizable core - leveraging popular in the community, web-native technologies and frameworks","title":"Meet and greet"},{"location":"2018/03/02/blogs-3x-ide/#building-blocks","text":"From end user perspective, the IDE is composition of perspectives , each consisting of the necessary tools to accomplish certain goal. There are three areas with fixed positions: - top-area toolbar for the menus , theme selection and user control; - sidebar to the left, with shortcuts e.g. to the perspectives; - statusbar at the bottom, for notifications and other use by the tools. The tools that constitute perspectives are laid out in different (predefined) regions of the work plot, but their position is not fixed and can be changed by dragging it to a new one. The perspectives are simply predefined configurations and end users are free to open , move or close tools on the work plot of a perspective as they see fit. The tools can also be maximized or minimized , even popped out in own window. The tools are the minimal atomic parts in the IDE. They are referred to as views or editors and each type is handled differently.","title":"Building blocks"},{"location":"2018/03/02/blogs-3x-ide/#whats-in-the-box","text":"The IDE package contains the Workbench , Git , Database , Repository and Terminal perspectives. The list is neither limited, nor fixed.","title":"What's in the box"},{"location":"2018/03/02/blogs-3x-ide/#workbench","text":"The Workbench perspective comprises Workspace , Import , Properties , Console and Preview views, plus the editors registered for each file type. In a word, the minimal toolset for file management, preview and editing operations. A very handy feature is also the template-based project and artifacts scaffolding generators. New in 3.x - Multiple workspaces per user . User now can create multiple workspaces and use the Workspace view to manage and switch between them. - Non-normative project file structure . The projects file organization is now non-normative and entirely up-to your preferences. - Multiple editors . The IDE now supports multiple editors registered for different file (MIME) types. More than one editor can be registered for one file type and in this case a \" Open with... \" context menu entry is rendered for the user to select, which one to use.","title":"Workbench"},{"location":"2018/03/02/blogs-3x-ide/#git","text":"Git perspective is built from tools supporting Git client operations. It doesn't aim to be what EGit is to Eclipse, but rather to present a simplified interface for the most common operations, such as cloning a repository to a workspace, pulling changes, and pushing commits. For more sophisticated command-line interaction see Terminal","title":"Git"},{"location":"2018/03/02/blogs-3x-ide/#database","text":"The Database perspective is essential for full stack developers. It features a database explorer, a console to execute SQL statements and preview results in table format.","title":"Database"},{"location":"2018/03/02/blogs-3x-ide/#repository","text":"The Repository perspective offers explorer for the repository of the Dirigible instance where the IDE is running, and import/export snapshots.","title":"Repository"},{"location":"2018/03/02/blogs-3x-ide/#terminal","text":"The key view in the perspective is a terminal that emulates console client connected to the environment of the Dirigible IDE that can execute commands.","title":"Terminal"},{"location":"2018/03/02/blogs-3x-ide/#editors","text":"The editors that have been integrated are Orion and Ace . More sophisticated and also visual editors are in the pipeline.","title":"Editors"},{"location":"2018/03/02/blogs-3x-ide/#core-ui-framework","text":"The Core UI framework is an Angular module ( ideUiCore ) that exposes a number of key components, building the backbone of perspectives and tools in the cloud IDE. Many of these can be further customized and some are part of larger compositions (e.g. the Angular directives for menu, sidebar or statusbar). The design encourages customizations if you are up to some tricky scenario or just rolling out a whole new fully functional perspective with a just a couple lines of code. To give you the idea how literally true is that, here is a clue: < body ng-app = \"workbench\" ng-controller = \"WorkbenchController as wb\" > < div menu menu-data-url = \"../../js/ide/services/menu.js\" ></ div > < div class = \"shell\" > < div class = \"sidebar list-group\" sidebar active = \"Workbench\" ></ div > < div id = \"workbench\" class = \"plane\" views-layout views-layout-model = \"wb.layoutModel\" ></ div > </ div > < div class = \"statusbar\" status-bar > {{message}} </ div > </ body > In this HTML template, the directives menu , sidebar , views-layout and statusbar will build a standard perspective layout for you. All you need to add to that (in the corresponding controller) is, which are the views that will be part of the perspective (note the views array property in the layoutModel member below): ... . controller ( 'WorkbenchController' , [ 'Layouts' , function ( Layouts ){ this . layoutModel = { views : [ 'workspace' , 'import' , 'editor' , 'properties' , 'console' , 'preview' ] ... } The components in the core UI framework provide a coherent look and feel for the perspectives and their views and editors in the IDE. A brief description for some of the key components follows.","title":"Core UI framework"},{"location":"2018/03/02/blogs-3x-ide/#message-hub","text":"The IDE building blocks work together in choreography . An action in the Workspace view for example opens-up an editor, or triggers the Preview view to send a request to a service. The communication between the isolated choreography participants is decentralized, and realized by a highly performant, entirely client-side framework - the Message Hub . The Message Hub is a self-contained (no dependencies) JS library that leverages the HTML5 messaging API to convey messages across the UI component isolation (iframe) boundaries via a simplified, yet highly customizable interface. It works across domains if necessary, yet implementing all best-practices for secure cross-origin client-side communication. Here is a hint for the simplicity of the API: A. Load the library < script type = \"text/javascript\" src = \"ui/message-hub.js\" ></ script > <!-- and the following for Angular Apps --> < script type = \"text/javascript\" src = \"ui/ui-layout.js\" ></ script > < script type = \"text/javascript\" src = \"ui/ui-core-ng-modules.js\" ></ script > B. Get an instance - With vanilla JS var messageHub = new FramesMessageHub (); Or in Anguar app angular . module ( 'App' , [ 'ideUiCore' ]) ... . controller ( 'SomeController' , [ 'messageHub' , function ( messageHub ){ ... C. Subscirbe for messages of type 'namespace.messageName' javascript , messageHub.on('namespace.messageName', function(message, postEvent){ //do something... }); D. Post a message to subscribers for the 'namespace.messageName' messages messageHub . send ( 'namespace.messageName' , message ); Using the message hub framework, we build highly performant loosely coupled integrations between isolated components, reducing the boilerplate in setting up communication channels to the bare minimum.","title":"Message Hub"},{"location":"2018/03/02/blogs-3x-ide/#layouts","text":"The cloud IDE layout API delegates the layout management to the GoldenLayout framework \u2013 a \u201c multi-window JavaScript layout manager for webapps \u201c, as the author defines it. It handles all the magic of laying things out, stacking, moving interactively, popping out, maximize/minimize, close and open of views and editors. The angular directive views-layout encapsulates all the complexity of initializing the layout of views in a perspective and reduces it all to a very simple configuration. < div id = \"workbench\" class = \"plane\" views-layout views-layout-model = \"wb.layoutModel\" ></ div > In that code snippet above, wb.layoutModel is a JSON like: { views : [ 'workspace' , 'import' , 'editor' , 'properties' , 'console' , 'preview' ] } The strings in the views array are in fact identifiers for the views registered in the Views Registry maintained by the framework. The registry entries provide essential configuration information for the views, such as their intended region on the work plot, factory function, label and specific configuration. The registry is populated by the views.js service. Layouts is a convenience bag of functions that significantly simplifies the work with layouts. It takes care of views registry setup, the work plot regions configuration, layout initialization, serialization, control on the layout manager, open view and open editor functions, global notifications and others. For example, the API to open programmatically a view (after layout initialization): Layouts.manager.open(viewId, regionId);","title":"Layouts"},{"location":"2018/03/02/blogs-3x-ide/#toolbar","text":"The Toolbar is a composite that aggregates the drop-down menus, the theme selection, the user name and sign-out control. It uses the corresponding UI microservices available in the ideUiCore module as Menu , User and Theme . Having a fully functional standard template menu in a perspective costs as much as this line: < div menu menu-data-url = \"../../js/ide/services/menu.js\" ></ div > The Angular directive menu will take care to render it as appropriate and all it needs for that is the (perspective-specific) URL of a service that will provide the menu items configuration. By convention, all UI components are built with Bootstrap 3.x CSS and the themes in the cloud IDE are actually custom Bootstrap CSS. A UI microservice enables dynamic change of the CSS upon change of the theme automatically. It is available as Angular Factory Theme . The Angular service User provides the details for the user that are rendered by the Menu directive, such as the user name.","title":"Toolbar"},{"location":"2018/03/02/blogs-3x-ide/#sidebar","text":"The sidebar is Angular directive that takes care to render a standard sidebar in the framework template. It works with the perspectives.js service to populate the registered perspectives as shortcuts.","title":"Sidebar"},{"location":"2018/03/02/blogs-3x-ide/#statusbar","text":"The statusbar is an Angular directive that renders a standard, fixed-position footer. The component is subscribed to listen to message types configured as value of the status-bar-topic attribute, or by default to status-message messages.","title":"Statusbar"},{"location":"2018/03/02/blogs-3x-ide/#extensibility-and-dynamic-compositions","text":"The content of the Menu, Sidebar, Open With (editor) action list, the views/editors in a perspective and their positions on the work plot are all examples of configurations that are delivered at runtime by Dirigible services. They leverage the extensions concept in Dirigible, so integrating a new menu (for example) is as easy as a Dirigible module \u201cadvertising\u201d itself (see file.extension ) to the corresponding Dirigible extension point (see menu.extensionpoint ). At runtime, the extensions will be invoked dynamically, their results aggregated and ultimately delivered to the front-end by a service responsible to mashup everything into a lean consumable JSON model (see menu.js ). The whole mechanism shares concepts with the Eclipse plugin framework. It detaches the client-side, which would render anything that can satisfy its data format needs, and server-side data composition mechanisms from the data contributors which are resolved and invoked dynamically. Therefore, to extend the system, all you need to do is to focus on providing a compliant data contributor and the system will take care to resolve and request it for its piece of data when required.","title":"Extensibility and Dynamic Compositions"},{"location":"2018/03/02/blogs-3x-ide/#enjoy","text":"","title":"Enjoy!"},{"location":"2018/06/25/kubernetes-keycloak-postgresql-dirigible/","text":"This article is dedicated to the \"production-ready\" setup of Eclipse Dirigible in a Kubernetes cluster. Overview In this article we are going to use Kubernes cluster, Keycloak IAM and PostgreSQL database for setting up a productive Eclipse Dirigible development platform. The target Kubernetes deployment is shown bellow: Kubernetes is an open source system for automating deployment, scaling, and management of containerized applications in a cluster environment. You can read more about Kubernetes here . Keycloak is an open source Identity and Access Management system for applications and services. You can read more about Keycloak here . PostgreSQL is a powerful open source object-relational database system with over 30 years of active development that has earned it a strong reputation for reliability, feature robustness, and performance. You can read more about PostgreSQL here . Eclipse Dirigible is a Cloud Development Platform providing development tools and runtime environment. It supports full development life-cycle of on-demand applications by leveraging in-system programming models and rapid application development techniques. You can read more about Dirigible here . Prerequisites In this article we assume that you have already running productive Kubernetes Cluster and configured kubectl for it. If you don't have such, you can create one by using the GKE or the open-source Gardener project. Also we need a configured Helm (The Kubernetes Package Manager) , if you don't have it, you can follow this installation guide . Kubernetes Certificate Management Controller cert-manager is a native Kubernetes certificate management controller. It can help with issuing certificates from a variety of sources, such as Let\u2019s Encrypt , HashiCorp Vault, or a simple signing keypair. You can read more about the cert-manager here . helm install --name cert-manager --namespace kube-system stable/cert-manager We are going to use cert-manager for issuing certificates for our access points to the Keycloak Admin Console and Eclipse Dirigible IDE. Namespaces and ClusterIssuer auth.yaml kind : Namespace metadata : name : auth ... kind : ClusterIssuer metadata : name : letsencrypt-production spec : acme : server : https://acme-v02.api.letsencrypt.org/directory email : <your-email-address> privateKeySecretRef : name : letsencrypt-production http01 : {} Note: For testing purposes it's recommended to use the staging Let's Encrypt server. The whole YAML is available here . Before creating the Kubernetes resources, you should replace the placeholders with the correct values. kubectl create -f auth.yaml Keycloak and PostgreSQL keycloak.yaml kind : StatefulSet metadata : name : postgres ... spec : containers : - name : postgres image : postgres env : - name : PGDATA value : \"/var/lib/postgresql/data/pgdata\" - name : POSTGRES_USER value : \"keycloak\" - name : POSTGRES_PASSWORD value : \"keycloak\" ... kind : Deployment metadata : name : keycloak ... spec : containers : - name : keycloak image : jboss/keycloak env : - name : PROXY_ADDRESS_FORWARDING value : \"true\" - name : DB_VENDOR value : \"postgres\" - name : DB_USER value : \"keycloak\" - name : DB_PASSWORD value : \"keycloak\" - name : DB_ADDR value : \"postgres-jdbc.auth\" - name : KEYCLOAK_USER value : \"admin\" - name : KEYCLOAK_PASSWORD value : \"admin\" ... kind : Ingress metadata : name : keycloak annotations : ingress.kubernetes.io/ssl-redirect : \"true\" kubernetes.io/tls-acme : \"true\" certmanager.k8s.io/cluster-issuer : \"letsencrypt-production\" kubernetes.io/ingress.class : \"nginx\" spec : tls : - hosts : - keycloak.<your-domain-name> secretName : keycloak-production-letsencrypt rules : - host : keycloak.<your-domain-name> ... Note: The maximum length of the host name (e.g. keycloak.YOUR-DOMAIN-NAME ) used for issuing Let's Encrypt certificate is 63 symbols. The whole YAML is available here . Before creating the Kubernetes resources, you should replace the placeholders with the correct values. kubectl create -f keycloak.yaml Dirigible and PostgreSQL dirigible.yaml kind : StatefulSet metadata : name : postgres ... spec : containers : - name : postgres image : postgres env : - name : PGDATA value : \"/var/lib/postgresql/data/pgdata\" - name : POSTGRES_USER value : \"dirigible\" - name : POSTGRES_PASSWORD value : \"dirigible\" ... kind : StatefulSet metadata : name : dirigible ... spec : containers : - name : dirigible image : dirigiblelabs/dirigible-keycloak env : - name : DIRIGIBLE_DATABASE_PROVIDER value : \"custom\" - name : DIRIGIBLE_DATABASE_CUSTOM_DATASOURCES value : \"POSTGRES\" - name : POSTGRES_URL value : \"jdbc:postgresql://postgres-jdbc.dirigible:5432/dirigible\" - name : POSTGRES_USERNAME value : \"dirigible\" - name : POSTGRES_PASSWORD value : \"dirigible\" ... - name : KEYCLOAK_CONFIDENTIAL_PORT value : \"443\" - name : KEYCLOAK_SSL_REQUIRED value : \"none\" - name : KEYCLOAK_CLIENT_ID value : \"dirigible\" - name : KEYCLOAK_REALM value : \"master\" - name : KEYCLOAK_AUTH_SERVER_URL value : \"https://keycloak.<your-domain-name>/auth\" ... kind : Ingress metadata : annotations : ingress.kubernetes.io/ssl-redirect : \"true\" kubernetes.io/tls-acme : \"true\" certmanager.k8s.io/cluster-issuer : \"letsencrypt-production\" kubernetes.io/ingress.class : \"nginx\" name : dirigible namespace : dirigible spec : tls : - hosts : - ide.<your-domain-name> secretName : dirigible-certificate rules : - host : ide.<your-domain-name> Note: The maximum length of the host name (e.g. ide.YOUR-DOMAIN-NAME ) used for issuing Let's Encrypt certificate is 63 symbols. The whole list of Eclipse Dirigible environment variables can be found here The whole YAML is available here . Before creating the Kubernetes resources, you should replace the placeholders with the correct values. kubectl create -f dirigible.yaml Add Keycloak Client and Users Open the Keycloak welcome page ( https://keycloak.YOUR-DOMAIN-NAME ) and click on the Admin Console , login with admin/admin credentials (see keycloak.yaml ). Create new client named dirigible For the Root URL add Root URL: http://ide.YOUR-DOMAIN-NAME Add Client Roles: Everyone Developer Operator Create new user Assign User's roles from the dirigible client Set password from the Credentials tab Open the Eclipse Dirigible IDE: http://ide.YOUR-DOMAIN-NAME/ and login with the credentials that were created previously in the Keycloak Admin Console. The Keycloak documentation can be found here . Also you can find out how to enable Keycloak Social Login with GitHub here . Credits How to launch nginx-ingress and cert-manager in Kubernetes","title":"Kubernetes, Keycloak, PostgreSQL & Dirigible"},{"location":"2018/06/25/kubernetes-keycloak-postgresql-dirigible/#overview","text":"In this article we are going to use Kubernes cluster, Keycloak IAM and PostgreSQL database for setting up a productive Eclipse Dirigible development platform. The target Kubernetes deployment is shown bellow:","title":"Overview"},{"location":"2018/06/25/kubernetes-keycloak-postgresql-dirigible/#kubernetes","text":"is an open source system for automating deployment, scaling, and management of containerized applications in a cluster environment. You can read more about Kubernetes here .","title":"Kubernetes"},{"location":"2018/06/25/kubernetes-keycloak-postgresql-dirigible/#keycloak","text":"is an open source Identity and Access Management system for applications and services. You can read more about Keycloak here .","title":"Keycloak"},{"location":"2018/06/25/kubernetes-keycloak-postgresql-dirigible/#postgresql","text":"is a powerful open source object-relational database system with over 30 years of active development that has earned it a strong reputation for reliability, feature robustness, and performance. You can read more about PostgreSQL here .","title":"PostgreSQL"},{"location":"2018/06/25/kubernetes-keycloak-postgresql-dirigible/#eclipse-dirigible","text":"is a Cloud Development Platform providing development tools and runtime environment. It supports full development life-cycle of on-demand applications by leveraging in-system programming models and rapid application development techniques. You can read more about Dirigible here .","title":"Eclipse Dirigible"},{"location":"2018/06/25/kubernetes-keycloak-postgresql-dirigible/#prerequisites","text":"In this article we assume that you have already running productive Kubernetes Cluster and configured kubectl for it. If you don't have such, you can create one by using the GKE or the open-source Gardener project. Also we need a configured Helm (The Kubernetes Package Manager) , if you don't have it, you can follow this installation guide .","title":"Prerequisites"},{"location":"2018/06/25/kubernetes-keycloak-postgresql-dirigible/#kubernetes-certificate-management-controller","text":"cert-manager is a native Kubernetes certificate management controller. It can help with issuing certificates from a variety of sources, such as Let\u2019s Encrypt , HashiCorp Vault, or a simple signing keypair. You can read more about the cert-manager here . helm install --name cert-manager --namespace kube-system stable/cert-manager We are going to use cert-manager for issuing certificates for our access points to the Keycloak Admin Console and Eclipse Dirigible IDE.","title":"Kubernetes Certificate Management Controller"},{"location":"2018/06/25/kubernetes-keycloak-postgresql-dirigible/#namespaces-and-clusterissuer","text":"","title":"Namespaces and ClusterIssuer"},{"location":"2018/06/25/kubernetes-keycloak-postgresql-dirigible/#authyaml","text":"kind : Namespace metadata : name : auth ... kind : ClusterIssuer metadata : name : letsencrypt-production spec : acme : server : https://acme-v02.api.letsencrypt.org/directory email : <your-email-address> privateKeySecretRef : name : letsencrypt-production http01 : {} Note: For testing purposes it's recommended to use the staging Let's Encrypt server. The whole YAML is available here . Before creating the Kubernetes resources, you should replace the placeholders with the correct values. kubectl create -f auth.yaml","title":"auth.yaml"},{"location":"2018/06/25/kubernetes-keycloak-postgresql-dirigible/#keycloak-and-postgresql","text":"","title":"Keycloak and PostgreSQL"},{"location":"2018/06/25/kubernetes-keycloak-postgresql-dirigible/#keycloakyaml","text":"kind : StatefulSet metadata : name : postgres ... spec : containers : - name : postgres image : postgres env : - name : PGDATA value : \"/var/lib/postgresql/data/pgdata\" - name : POSTGRES_USER value : \"keycloak\" - name : POSTGRES_PASSWORD value : \"keycloak\" ... kind : Deployment metadata : name : keycloak ... spec : containers : - name : keycloak image : jboss/keycloak env : - name : PROXY_ADDRESS_FORWARDING value : \"true\" - name : DB_VENDOR value : \"postgres\" - name : DB_USER value : \"keycloak\" - name : DB_PASSWORD value : \"keycloak\" - name : DB_ADDR value : \"postgres-jdbc.auth\" - name : KEYCLOAK_USER value : \"admin\" - name : KEYCLOAK_PASSWORD value : \"admin\" ... kind : Ingress metadata : name : keycloak annotations : ingress.kubernetes.io/ssl-redirect : \"true\" kubernetes.io/tls-acme : \"true\" certmanager.k8s.io/cluster-issuer : \"letsencrypt-production\" kubernetes.io/ingress.class : \"nginx\" spec : tls : - hosts : - keycloak.<your-domain-name> secretName : keycloak-production-letsencrypt rules : - host : keycloak.<your-domain-name> ... Note: The maximum length of the host name (e.g. keycloak.YOUR-DOMAIN-NAME ) used for issuing Let's Encrypt certificate is 63 symbols. The whole YAML is available here . Before creating the Kubernetes resources, you should replace the placeholders with the correct values. kubectl create -f keycloak.yaml","title":"keycloak.yaml"},{"location":"2018/06/25/kubernetes-keycloak-postgresql-dirigible/#dirigible-and-postgresql","text":"","title":"Dirigible and PostgreSQL"},{"location":"2018/06/25/kubernetes-keycloak-postgresql-dirigible/#dirigibleyaml","text":"kind : StatefulSet metadata : name : postgres ... spec : containers : - name : postgres image : postgres env : - name : PGDATA value : \"/var/lib/postgresql/data/pgdata\" - name : POSTGRES_USER value : \"dirigible\" - name : POSTGRES_PASSWORD value : \"dirigible\" ... kind : StatefulSet metadata : name : dirigible ... spec : containers : - name : dirigible image : dirigiblelabs/dirigible-keycloak env : - name : DIRIGIBLE_DATABASE_PROVIDER value : \"custom\" - name : DIRIGIBLE_DATABASE_CUSTOM_DATASOURCES value : \"POSTGRES\" - name : POSTGRES_URL value : \"jdbc:postgresql://postgres-jdbc.dirigible:5432/dirigible\" - name : POSTGRES_USERNAME value : \"dirigible\" - name : POSTGRES_PASSWORD value : \"dirigible\" ... - name : KEYCLOAK_CONFIDENTIAL_PORT value : \"443\" - name : KEYCLOAK_SSL_REQUIRED value : \"none\" - name : KEYCLOAK_CLIENT_ID value : \"dirigible\" - name : KEYCLOAK_REALM value : \"master\" - name : KEYCLOAK_AUTH_SERVER_URL value : \"https://keycloak.<your-domain-name>/auth\" ... kind : Ingress metadata : annotations : ingress.kubernetes.io/ssl-redirect : \"true\" kubernetes.io/tls-acme : \"true\" certmanager.k8s.io/cluster-issuer : \"letsencrypt-production\" kubernetes.io/ingress.class : \"nginx\" name : dirigible namespace : dirigible spec : tls : - hosts : - ide.<your-domain-name> secretName : dirigible-certificate rules : - host : ide.<your-domain-name> Note: The maximum length of the host name (e.g. ide.YOUR-DOMAIN-NAME ) used for issuing Let's Encrypt certificate is 63 symbols. The whole list of Eclipse Dirigible environment variables can be found here The whole YAML is available here . Before creating the Kubernetes resources, you should replace the placeholders with the correct values. kubectl create -f dirigible.yaml","title":"dirigible.yaml"},{"location":"2018/06/25/kubernetes-keycloak-postgresql-dirigible/#add-keycloak-client-and-users","text":"Open the Keycloak welcome page ( https://keycloak.YOUR-DOMAIN-NAME ) and click on the Admin Console , login with admin/admin credentials (see keycloak.yaml ). Create new client named dirigible For the Root URL add Root URL: http://ide.YOUR-DOMAIN-NAME Add Client Roles: Everyone Developer Operator Create new user Assign User's roles from the dirigible client Set password from the Credentials tab Open the Eclipse Dirigible IDE: http://ide.YOUR-DOMAIN-NAME/ and login with the credentials that were created previously in the Keycloak Admin Console. The Keycloak documentation can be found here . Also you can find out how to enable Keycloak Social Login with GitHub here .","title":"Add Keycloak Client and Users"},{"location":"2018/06/25/kubernetes-keycloak-postgresql-dirigible/#credits","text":"How to launch nginx-ingress and cert-manager in Kubernetes","title":"Credits"},{"location":"2018/11/08/blogs-make-low-code-great-again/","text":"Buy Application or Build Application It's been a fundamental question for Enterprises and SMEs over the last few decades. Can you save money by buying a packaged software, which follows the best practices and does exactly what you need or is this just an imaginary dream? Obviously, there is no single answer to this simple question. Usually, companies still stick to the packaged ready-to-use solutions coming from well-established software vendors for their core business processes, but at the same time open the door for in-house LoB applications built by their own IT staff or a partner. While the first one gives them stability for the mainstream business, the latter gives them an innovation power to cover the brand new yet differentiating business models. Sounds reasonable and pragmatic, no chief executive will be fired by following this strategy. Let's leave aside the packaged software case with its configurability, customization and extensibility strengths for another blog post and focus now on the build-your-own-application scenario. What does \"build\" mean for a company, which doesn't have an IT background? There are several options here - you can hire and manage your own developers, contact partners, use freelancers ... All these options have their pros and cons. What happens in reality is that most companies just go for all of them at the same time to minimize the risks. On one hand, this leads very easily to a situation, where the company has its own staff that has to gain technological know-how for the projects and try to control the development processes, time frames and feature sets. On the other hand, there are partners that offer developer manpower, but in fact they strive for maximum execution time i.e. budget, claiming the highest possible quality at the lowest possible cost. Depending on the complexity of the project and the technology stack, the negotiations can be very hard for everyone in that round and the outcome always results in a - wrong estimation . The good thing here is that nobody actually expects that the estimation will be precise, so why are we wasting time in useless activities? Can we build the application directly instead of making estimations? Here comes the Low-Code/No-Code hype. Also, the reborn hype related to BPM. Also, the reborn hype for MDA and RAD. Are they just the next wave of cool stuff that soon will be just a record in software history without a meaningful impact? Only time will tell. What is different nowadays that can lead to a massive usage of high productivity tools for building tailored applications? It is unlikely that there will be a new vendor of packaged software, who can build everything for everybody - it's an unrealistic huge investment The companies no more stop at the stage where they just follow the standard best practices for their industry - they need to be innovative to survive Cloud infrastructure is already matured enough at a level where one can use it to run a high-scale SaaS solution entirely based on the Cloud Old-fashioned software technologies do not fit well with the new infrastructure and quality requirements Transparency (and the ability to prove it) of the whole lifecycle of the business software development is already a must - not a nice-to-have feature and Time-to-market is the most important goal when applying a new business model - nobody can wait months and years for its implementation All abovementioned reasons, make the Low-Code/No-Code tools and even whole development platforms to rise again. This time in the Cloud - instantly accessible via a Web interface. They will fight for a multibillion business segment in the next few years, hence their business case looks promising. There is just a small problem - all of them are quite expensive for the SMBs, even for enterprises, although they have better ROI, TCD and TCO metrics than the standard development models. And there is a reason for this. The business scenarios are usually complex; hence somebody has to invest a fairly large amount of effort to build a solution. If it is not paid by the customer for the development, then it is paid by the platform vendor for building RAD tools, providing robust middleware, giving great monitoring and operations tools. Then the customer pays for the platform more and less for the development on this platform. It is quite simple and fair, isn't it? What exactly does such a platform provide? Is there an open-source alternative? BPM or MDA? There are two major approaches at the moment - Business-Process-Model driven tools and platforms and those who rely on the Model-Driven-Architecture concepts. While the first one gives extremely fast and exhaustive implementation of the workflows including automated and user interactive tasks, the second one is focused on the definition of the domain model (or similar) and generation of the backbone of an application - mainly with CRUD support. So, following both approaches you can cover both - the structure and the behavior aspects for a given business scenario. What did we decide to provide in Dirigible? As you can guess, following our principle to cover the full development lifecycle end-to-end, the decision was simple - to provide both... and more. We managed to integrate the world's leading BPM engine - Flowable along with the browser-based BPMN 2.0 modeler. Now, you can create a business model on-the-fly on the live system and even to implement the steps in the same way using our famous Enterprise JavaScript API . More information about how to create a simple process you can find under the samples section . For the second stream, we decided to follow the Entity-Data-Model approach, where all the information needed for the generation process is included in a single artifact. Now, you can visually drag-and-drop the entities representing your domain model as well as to set the parameters in separated spaces e.g. General, Database, User Interface. Then, you can choose one of the provided generation templates and build a whole business application including the whole database layout, RESTful backend services, CRUD forms, reports and even a launchpad home page. This one is very useful for administrative or simple internal LoB applications. For more sophisticated user interfaces, for instance, you can use the already available (during the generation) extension points. To try it by yourself, you can just follow the steps in this tutorial . For the complementary features with regards to collaborative development, issue tracking, requirements and project management, validation and verification of the releases and even design-thinking tools, we mainly integrate GitHub, TravisCI, Docker, and some other Cloud-based third-party development services as well. Does it mean that Dirigible is a full-fledged Low-Code/No-Code platform? No. It is an open-source project providing lots of features covering variety of scenarios in this space, but it is not a product. - Can I consider building a real productive Low-Code/No-Code platform based on Dirigible? Definitely - YES!","title":"Make Low-Code/No-Code Platforms Great Again!"},{"location":"2018/11/08/blogs-make-low-code-great-again/#buy-application-or-build-application","text":"It's been a fundamental question for Enterprises and SMEs over the last few decades. Can you save money by buying a packaged software, which follows the best practices and does exactly what you need or is this just an imaginary dream? Obviously, there is no single answer to this simple question. Usually, companies still stick to the packaged ready-to-use solutions coming from well-established software vendors for their core business processes, but at the same time open the door for in-house LoB applications built by their own IT staff or a partner. While the first one gives them stability for the mainstream business, the latter gives them an innovation power to cover the brand new yet differentiating business models. Sounds reasonable and pragmatic, no chief executive will be fired by following this strategy. Let's leave aside the packaged software case with its configurability, customization and extensibility strengths for another blog post and focus now on the build-your-own-application scenario.","title":"Buy Application or Build Application"},{"location":"2018/11/08/blogs-make-low-code-great-again/#what-does-build-mean-for-a-company-which-doesnt-have-an-it-background","text":"There are several options here - you can hire and manage your own developers, contact partners, use freelancers ... All these options have their pros and cons. What happens in reality is that most companies just go for all of them at the same time to minimize the risks. On one hand, this leads very easily to a situation, where the company has its own staff that has to gain technological know-how for the projects and try to control the development processes, time frames and feature sets. On the other hand, there are partners that offer developer manpower, but in fact they strive for maximum execution time i.e. budget, claiming the highest possible quality at the lowest possible cost. Depending on the complexity of the project and the technology stack, the negotiations can be very hard for everyone in that round and the outcome always results in a - wrong estimation . The good thing here is that nobody actually expects that the estimation will be precise, so why are we wasting time in useless activities?","title":"What does \"build\" mean for a company, which doesn't have an IT background?"},{"location":"2018/11/08/blogs-make-low-code-great-again/#can-we-build-the-application-directly-instead-of-making-estimations","text":"Here comes the Low-Code/No-Code hype. Also, the reborn hype related to BPM. Also, the reborn hype for MDA and RAD. Are they just the next wave of cool stuff that soon will be just a record in software history without a meaningful impact? Only time will tell. What is different nowadays that can lead to a massive usage of high productivity tools for building tailored applications? It is unlikely that there will be a new vendor of packaged software, who can build everything for everybody - it's an unrealistic huge investment The companies no more stop at the stage where they just follow the standard best practices for their industry - they need to be innovative to survive Cloud infrastructure is already matured enough at a level where one can use it to run a high-scale SaaS solution entirely based on the Cloud Old-fashioned software technologies do not fit well with the new infrastructure and quality requirements Transparency (and the ability to prove it) of the whole lifecycle of the business software development is already a must - not a nice-to-have feature and Time-to-market is the most important goal when applying a new business model - nobody can wait months and years for its implementation All abovementioned reasons, make the Low-Code/No-Code tools and even whole development platforms to rise again. This time in the Cloud - instantly accessible via a Web interface. They will fight for a multibillion business segment in the next few years, hence their business case looks promising. There is just a small problem - all of them are quite expensive for the SMBs, even for enterprises, although they have better ROI, TCD and TCO metrics than the standard development models. And there is a reason for this. The business scenarios are usually complex; hence somebody has to invest a fairly large amount of effort to build a solution. If it is not paid by the customer for the development, then it is paid by the platform vendor for building RAD tools, providing robust middleware, giving great monitoring and operations tools. Then the customer pays for the platform more and less for the development on this platform. It is quite simple and fair, isn't it? What exactly does such a platform provide? Is there an open-source alternative?","title":"Can we build the application directly instead of making estimations?"},{"location":"2018/11/08/blogs-make-low-code-great-again/#bpm-or-mda","text":"There are two major approaches at the moment - Business-Process-Model driven tools and platforms and those who rely on the Model-Driven-Architecture concepts. While the first one gives extremely fast and exhaustive implementation of the workflows including automated and user interactive tasks, the second one is focused on the definition of the domain model (or similar) and generation of the backbone of an application - mainly with CRUD support. So, following both approaches you can cover both - the structure and the behavior aspects for a given business scenario. What did we decide to provide in Dirigible? As you can guess, following our principle to cover the full development lifecycle end-to-end, the decision was simple - to provide both... and more. We managed to integrate the world's leading BPM engine - Flowable along with the browser-based BPMN 2.0 modeler. Now, you can create a business model on-the-fly on the live system and even to implement the steps in the same way using our famous Enterprise JavaScript API . More information about how to create a simple process you can find under the samples section . For the second stream, we decided to follow the Entity-Data-Model approach, where all the information needed for the generation process is included in a single artifact. Now, you can visually drag-and-drop the entities representing your domain model as well as to set the parameters in separated spaces e.g. General, Database, User Interface. Then, you can choose one of the provided generation templates and build a whole business application including the whole database layout, RESTful backend services, CRUD forms, reports and even a launchpad home page. This one is very useful for administrative or simple internal LoB applications. For more sophisticated user interfaces, for instance, you can use the already available (during the generation) extension points. To try it by yourself, you can just follow the steps in this tutorial . For the complementary features with regards to collaborative development, issue tracking, requirements and project management, validation and verification of the releases and even design-thinking tools, we mainly integrate GitHub, TravisCI, Docker, and some other Cloud-based third-party development services as well.","title":"BPM or MDA?"},{"location":"2018/11/08/blogs-make-low-code-great-again/#does-it-mean-that-dirigible-is-a-full-fledged-low-codeno-code-platform","text":"No. It is an open-source project providing lots of features covering variety of scenarios in this space, but it is not a product.","title":"Does it mean that Dirigible is a full-fledged Low-Code/No-Code platform?"},{"location":"2018/11/08/blogs-make-low-code-great-again/#-can-i-consider-building-a-real-productive-low-codeno-code-platform-based-on-dirigible","text":"Definitely - YES!","title":"- Can I consider building a real productive Low-Code/No-Code platform based on Dirigible?"},{"location":"2018/11/09/dirigible-extend-embed-reuse/","text":"The latest major upgrade of Dirigible to 3.x opens the door for scenarios like building custom stacks, standardized application CI, embedded Dirigible and many more ... Overview With the latest major upgrade to Eclipse Dirigible 3.x, there are a lot of improvements starting from the project structure and stretching all the way up to the entirely new Web IDE. Some of them are: - New Web IDE - Entity Data Modeler - Support for Business Processes - Standardized Applications CI (WebJars) - Keycloak Integration - 200+ Maven Artifacts - OSGi Free Modules - and many more ... In this post, we will emphasize on the improved CI process and the scenarios it unlocks. For example, with the 200+ Maven Artifacts, the customization of Dirigible is more flexible than ever. Custom Dirigible Stack can be built with ease, or only some modules can be added as dependencies in existing projects (e.g. SQL & Persistency modules). Finally, having a hybrid (embedded) deployment is another interesting capability that is worth looking at (e.g. in existing Spring Stack). Custom Stack Building a custom Dirigible stack is the second-best option to run in production (if the pre-built releases don't fit the project needs). The Helium Custom Stack tutorial explores in detail all aspects of building a custom Dirigible stack: 1. Setting up maven project(s) layout 2. Building WebJar(s) 3. Exposing an Enterprise JavaScript API (both Java facade and JavaScript API) 4. Consuming Dirigible dependencies Reuse of Modules With 200+ Maven artifacts, it's obvious that some modules can be reused even in non-Dirigible related projects. For example, the SQL builder and the ORM can be consumed on their own: Database - SQL Builder: Builder: ... import org.eclipse.dirigible.database.sql.SqlFactory ; ... public class StudentsDao { private static SqlFactory sqlFactory ; public List < StudentEntity > searchByFirstName ( String name ) { String sql = getSqlFactory () . select () . column ( \"*\" ) . from ( \"STUDENTS\" ) . where ( \"STUDENT_FIRST_NAME like ?\" ) . build (); return query ( sql , name ); } private SqlFactory getSqlFactory () { if ( sqlFactory == null ) { Connection connection = null ; try { connection = getConnection (); sqlFactory = SqlFactory . getNative ( connection ); } finally { closeConnection ( connection ); } } return sqlFactory ; } ... Dependency: <dependency> <groupId> org.eclipse.dirigible </groupId> <artifactId> dirigible-database-sql </artifactId> <version> 3.2.8 </version> </dependency> Database - ORM: Dirigible's ORM is compatible with the Java Persistence API : Entity ... import javax.persistence.Column ; import javax.persistence.GeneratedValue ; import javax.persistence.GenerationType ; import javax.persistence.Id ; import javax.persistence.Table ; ... @Table ( name = \"STUDENTS\" ) public class StudentEntity { @Id @GeneratedValue ( strategy = GenerationType . IDENTITY ) @Column ( name = \"STUDENT_ID\" , columnDefinition = \"BIGINT\" ) private Long id ; ... 1. DAO: ... import org.eclipse.dirigible.database.persistence.PersistenceManager ; ... public class StudentsDao { private PersistenceManager < StudentEntity > persistenceManager = new PersistenceManager <> (); public StudentEntity find ( Long id ) { Connection connection = null ; try { connection = getConnection (); return persistenceManager . find ( connection , StudentEntity . class , id ); } finally { closeConnection ( connection ); } } public List < StudentEntity > query ( String sql , Object ... values ) { Connection connection = null ; try { connection = getConnection (); return persistenceManager . query ( connection , StudentEntity . class , sql , values ); } finally { closeConnection ( connection ); } } ... 1. Dependency <dependency> <groupId> org.eclipse.dirigible </groupId> <artifactId> dirigible-database-persistence </artifactId> <version> 3.2.8 </version> </dependency> Embedded Dirigible The last option is the hybrid/embedded deployment, where a legacy application is running in coexistence with part of the Dirigible stack. The setup targets the scenarios where there is a lot of legacy (Java) code, but the low-code/no-code and In-System Development capabilities of Dirigible are desired. ... import org.eclipse.dirigible.runtime.core.embed.EmbeddedDirigible ; ... public void callDirigible () { EmbeddedDirigible dirigible = new EmbeddedDirigible (); dirigible . load ( \"./content\" ); dirigible . executeJavaScript ( \"project/api.js\" ); } ... For more details about this setup, check out the embedded Dirigible sample . Resources Experiment with the single-click deployment of the following demos from EclipseCon 2018 : ( GitHub ) ( GitHub ) ( GitHub ) ( GitHub ) ( GitHub )","title":"Dirigible - Extend, Embed, Reuse"},{"location":"2018/11/09/dirigible-extend-embed-reuse/#overview","text":"With the latest major upgrade to Eclipse Dirigible 3.x, there are a lot of improvements starting from the project structure and stretching all the way up to the entirely new Web IDE. Some of them are: - New Web IDE - Entity Data Modeler - Support for Business Processes - Standardized Applications CI (WebJars) - Keycloak Integration - 200+ Maven Artifacts - OSGi Free Modules - and many more ... In this post, we will emphasize on the improved CI process and the scenarios it unlocks. For example, with the 200+ Maven Artifacts, the customization of Dirigible is more flexible than ever. Custom Dirigible Stack can be built with ease, or only some modules can be added as dependencies in existing projects (e.g. SQL & Persistency modules). Finally, having a hybrid (embedded) deployment is another interesting capability that is worth looking at (e.g. in existing Spring Stack).","title":"Overview"},{"location":"2018/11/09/dirigible-extend-embed-reuse/#custom-stack","text":"Building a custom Dirigible stack is the second-best option to run in production (if the pre-built releases don't fit the project needs). The Helium Custom Stack tutorial explores in detail all aspects of building a custom Dirigible stack: 1. Setting up maven project(s) layout 2. Building WebJar(s) 3. Exposing an Enterprise JavaScript API (both Java facade and JavaScript API) 4. Consuming Dirigible dependencies","title":"Custom Stack"},{"location":"2018/11/09/dirigible-extend-embed-reuse/#reuse-of-modules","text":"With 200+ Maven artifacts, it's obvious that some modules can be reused even in non-Dirigible related projects. For example, the SQL builder and the ORM can be consumed on their own:","title":"Reuse of Modules"},{"location":"2018/11/09/dirigible-extend-embed-reuse/#database-sql-builder","text":"Builder: ... import org.eclipse.dirigible.database.sql.SqlFactory ; ... public class StudentsDao { private static SqlFactory sqlFactory ; public List < StudentEntity > searchByFirstName ( String name ) { String sql = getSqlFactory () . select () . column ( \"*\" ) . from ( \"STUDENTS\" ) . where ( \"STUDENT_FIRST_NAME like ?\" ) . build (); return query ( sql , name ); } private SqlFactory getSqlFactory () { if ( sqlFactory == null ) { Connection connection = null ; try { connection = getConnection (); sqlFactory = SqlFactory . getNative ( connection ); } finally { closeConnection ( connection ); } } return sqlFactory ; } ... Dependency: <dependency> <groupId> org.eclipse.dirigible </groupId> <artifactId> dirigible-database-sql </artifactId> <version> 3.2.8 </version> </dependency>","title":"Database - SQL Builder:"},{"location":"2018/11/09/dirigible-extend-embed-reuse/#database-orm","text":"Dirigible's ORM is compatible with the Java Persistence API : Entity ... import javax.persistence.Column ; import javax.persistence.GeneratedValue ; import javax.persistence.GenerationType ; import javax.persistence.Id ; import javax.persistence.Table ; ... @Table ( name = \"STUDENTS\" ) public class StudentEntity { @Id @GeneratedValue ( strategy = GenerationType . IDENTITY ) @Column ( name = \"STUDENT_ID\" , columnDefinition = \"BIGINT\" ) private Long id ; ... 1. DAO: ... import org.eclipse.dirigible.database.persistence.PersistenceManager ; ... public class StudentsDao { private PersistenceManager < StudentEntity > persistenceManager = new PersistenceManager <> (); public StudentEntity find ( Long id ) { Connection connection = null ; try { connection = getConnection (); return persistenceManager . find ( connection , StudentEntity . class , id ); } finally { closeConnection ( connection ); } } public List < StudentEntity > query ( String sql , Object ... values ) { Connection connection = null ; try { connection = getConnection (); return persistenceManager . query ( connection , StudentEntity . class , sql , values ); } finally { closeConnection ( connection ); } } ... 1. Dependency <dependency> <groupId> org.eclipse.dirigible </groupId> <artifactId> dirigible-database-persistence </artifactId> <version> 3.2.8 </version> </dependency>","title":"Database - ORM:"},{"location":"2018/11/09/dirigible-extend-embed-reuse/#embedded-dirigible","text":"The last option is the hybrid/embedded deployment, where a legacy application is running in coexistence with part of the Dirigible stack. The setup targets the scenarios where there is a lot of legacy (Java) code, but the low-code/no-code and In-System Development capabilities of Dirigible are desired. ... import org.eclipse.dirigible.runtime.core.embed.EmbeddedDirigible ; ... public void callDirigible () { EmbeddedDirigible dirigible = new EmbeddedDirigible (); dirigible . load ( \"./content\" ); dirigible . executeJavaScript ( \"project/api.js\" ); } ... For more details about this setup, check out the embedded Dirigible sample .","title":"Embedded Dirigible"},{"location":"2018/11/09/dirigible-extend-embed-reuse/#resources","text":"Experiment with the single-click deployment of the following demos from EclipseCon 2018 : ( GitHub ) ( GitHub ) ( GitHub ) ( GitHub ) ( GitHub )","title":"Resources"},{"location":"2018/11/12/blogs-dirigible-ide-on-che-workspaces/","text":"Eclipse Cloud Development - ONE team, ONE product What is Eclipse Cloud Development (ECD) and why should I consider it? ECD is a Top-Level Project (TLP) at Eclipse Foundation that provides open-source implementations of standards, services and frameworks that enable developing for and in the Cloud. If you read the definition from the home page, you will be misled that this is a single product or at least a bunch of projects that are complementary to each other and can run simultaneously. That was not exactly true for several years, since the beginning of the ECD initiative itself. Apart from Flux, which was a collaboration development middleware framework, there were actually three major full-stack development platforms - Orion, Che and Dirigible, each of them with its own Web IDE and own backend. They were just three fancy vehicles parked in a special place in front of the Eclipse Foundation house. Each of them was built for its own purpose and based on its own technology stack, without so much care about the others. Good start - there was quite a big room for improvement. I remember the invaluable initial discussions we had in San Francisco more than three years ago. We tried to understand each other, which are the scenarios we want to cover, how to integrate them and how exactly to respond to the community, which required from us ONE single offering for their Cloud development needs. The plan was just roughly defined, but all of us were eager to go for it and to collaborate much closely. Time has passed, many things have changed. The projects matured by passing several reimplementation phases, clarified the goals and strategies, some acquisitions happened, new players appeared and even one was retired. Still, nobody has seen a prominent path forward for a much deeper integration. Until now! Eclipse Che as a center of gravity for Cloud development environments In the last few months Che guys were working hard on the new approach for workspaces management for the 7.0 release. One of the major changes is that now they allow contributions not only for IDE plugins, but also for whole Web IDE stacks as separate Docker containers. This helped them to retire the old-fashioned GWT based Web IDE and replace it with the modern Monaco based one - Theia. This also gives an opportunity for other development platforms, leveraging different development models and covering different development scenarios to land on the Che planet as well. This was seen by all of us as a great opportunity to finally start a new age of integration, which can lead to a real consolidation of tools, models, and efforts. Dirigible as a Che editor plugin Following the new extensibility concept, first we had to create a plugin descriptor for Dirigible. It contains the metadata about the content you want to contribute - name, description, Docker image identifier, environment variables, etc. version: 1.0.0 type: Che Editor name: eclipse-dirigible id: org.eclipse.che.editor.dirigible title: Eclipse Dirigible as Editor for Eclipse Che description: Eclipse Dirigible as Editor for Eclipse Che icon: http://download.eclipse.org/dirigible/dirigible.png endpoints: - name: \"dirigible\" public: true targetPort: 8080 attributes: protocol: http type: ide containers: - name: eclipse-dirigible image: dirigiblelabs/dirigible-anonymous env: - name: DIRIGIBLE_REPOSITORY_LOCAL_ROOT_FOLDER value: /projects/dirigible/repository - name: DIRIGIBLE_REPOSITORY_LOCAL_ROOT_FOLDER_IS_ABSOLUTE value: true - name: DIRIGIBLE_REPOSITORY_SEARCH_ROOT_FOLDER value: /projects/dirigible/repository - name: DIRIGIBLE_REPOSITORY_SEARCH_ROOT_FOLDER_IS_ABSOLUTE value: true - name: DIRIGIBLE_CMS_INTERNAL_ROOT_FOLDER value: /projects/dirigible/cms - name: DIRIGIBLE_CMS_INTERNAL_ROOT_FOLDER_IS_ABSOLUTE value: true - name: DIRIGIBLE_DATABASE_H2_ROOT_FOLDER_DEFAULT value: /projects/dirigible/h2 - name: DIRIGIBLE_DATABASE_H2_URL value: jdbc:h2:/projects/dirigible/h2 - name: DIRIGIBLE_OPERATIONS_LOGS_ROOT_FOLDER_DEFAULT value: /usr/local/tomcat/logs volumes: - mountPath: \"/projects\" name: projects ports: - exposedPort: 8080 memory-limit: \"512M\" The GitHub repository containing the project is at: https://github.com/dirigiblelabs/dirigible-che-editor-plugin Because the development environment is secured by OpenShift underneath, we use here the dirigiblelabs/dirigible-anonymous Docker image of Dirigible. Environment variables mainly redirect the file system dependent components of Dirigible to use the default /projects persistent folder. We made several experiments with different sets of configurations, which we activated on the local Che registry by using the publish_plugin command. publish_plugin dirigible-che-editor-plugin 1.0.0 che Once we agreed how the first version of the plugin should look like, we had to add our plugin definition to the official Che registry metadata to be available by default on any standard Che environment. The process says we have to make a PR to the repository: https://github.com/eclipse/che-plugin-registry . The actual PR is: https://github.com/eclipse/che-plugin-registry/pull/54 What it does is adding the metadata of the given plugin under a predefined folder structure of the Che registry: id: org.eclipse.che.editor.dirigible version: 1.0.0 type: Che Editor name: dirigible-che-editor-plugin title: Eclipse Dirigible for Eclipse Che description: Eclipse Dirigible as App Development Platform for Eclipse Che icon: https://www.dirigible.io/img/logo/dirigible-logo.png url: https://github.com/dirigiblelabs/dirigible-che-editor-plugin/releases/download/1.0.0/dirigible-che-editor-plugin.tar.gz The document with all the needed steps presented by Gorkem and Florent at EclipseCon Europe 2018 can be found at: https://docs.google.com/document/d/1hFXTwzIU3MnqcciXH9E7xyUtEJRUeUuJ-e0JlEhTKjo/edit Start Dirigible based workspace in Che Since the beginning of this week you can go on the public Che environment on the OpenShift platform at: https://che.openshift.io/dashboard/#/workspaces and create a new workspace. In the configurations wizard, choose Che 7 dev stack and dirigible-che-editor-plugin . Click the \"Create\" button and wait until the workspace gets created. Note: You may need to wait a bit more the first time for Dirigible to get initialized, then refresh the browser. What is the benefit of running Dirigible on Che Now, there are two more options for running Dirigible - on the OpenShift Cloud platform and on the Eclipse Che workspace management platform. So, what are the benefits then to run Dirigible on Che? First of all, you get a holistic experience on your development process. Che is the basis, where all the different frameworks, languages and tools complement each other to cover the maximum set of requirements for building your next generation Cloud services. We have to be really honest with ourselves here - in Dirigible it is quite unlikely that we build C/C++ tooling, for instance. This doesn't fit our goals and vision about high-productivity development of business applications. But, also we have to admit that somebody might need such a low-level component written in C/C++ for e.g. integration purposes. Thanks to this variety of options in Che now (and even more in the future), the developer doesn't need to go out to some external environments for such cases. Second, you have the ability to run Dirigible on any platform where Che is deployed. It includes the front runner OpenShift platform as well as some other commercial and enterprise-grade Cloud platforms. This opens to the huge Che community all the scenarios for business applications development such as model-driven approach, business-process and in-system programming models and all the rest coming with Dirigible. Last but not least, it is crucial for us to organize the efforts spent on the Cloud development topics under Eclipse Foundation in a much better way. Nobody has unlimited resources to solve the infinite numbers of challenges that we are currently facing. It will not be easy, but I am sure: together we can do it!","title":"Running Dirigible on Che Workspaces"},{"location":"2018/11/12/blogs-dirigible-ide-on-che-workspaces/#eclipse-cloud-development-one-team-one-product","text":"What is Eclipse Cloud Development (ECD) and why should I consider it? ECD is a Top-Level Project (TLP) at Eclipse Foundation that provides open-source implementations of standards, services and frameworks that enable developing for and in the Cloud. If you read the definition from the home page, you will be misled that this is a single product or at least a bunch of projects that are complementary to each other and can run simultaneously. That was not exactly true for several years, since the beginning of the ECD initiative itself. Apart from Flux, which was a collaboration development middleware framework, there were actually three major full-stack development platforms - Orion, Che and Dirigible, each of them with its own Web IDE and own backend. They were just three fancy vehicles parked in a special place in front of the Eclipse Foundation house. Each of them was built for its own purpose and based on its own technology stack, without so much care about the others. Good start - there was quite a big room for improvement. I remember the invaluable initial discussions we had in San Francisco more than three years ago. We tried to understand each other, which are the scenarios we want to cover, how to integrate them and how exactly to respond to the community, which required from us ONE single offering for their Cloud development needs. The plan was just roughly defined, but all of us were eager to go for it and to collaborate much closely. Time has passed, many things have changed. The projects matured by passing several reimplementation phases, clarified the goals and strategies, some acquisitions happened, new players appeared and even one was retired. Still, nobody has seen a prominent path forward for a much deeper integration. Until now!","title":"Eclipse Cloud Development - ONE team, ONE product"},{"location":"2018/11/12/blogs-dirigible-ide-on-che-workspaces/#eclipse-che-as-a-center-of-gravity-for-cloud-development-environments","text":"In the last few months Che guys were working hard on the new approach for workspaces management for the 7.0 release. One of the major changes is that now they allow contributions not only for IDE plugins, but also for whole Web IDE stacks as separate Docker containers. This helped them to retire the old-fashioned GWT based Web IDE and replace it with the modern Monaco based one - Theia. This also gives an opportunity for other development platforms, leveraging different development models and covering different development scenarios to land on the Che planet as well. This was seen by all of us as a great opportunity to finally start a new age of integration, which can lead to a real consolidation of tools, models, and efforts.","title":"Eclipse Che as a center of gravity for Cloud development environments"},{"location":"2018/11/12/blogs-dirigible-ide-on-che-workspaces/#dirigible-as-a-che-editor-plugin","text":"Following the new extensibility concept, first we had to create a plugin descriptor for Dirigible. It contains the metadata about the content you want to contribute - name, description, Docker image identifier, environment variables, etc. version: 1.0.0 type: Che Editor name: eclipse-dirigible id: org.eclipse.che.editor.dirigible title: Eclipse Dirigible as Editor for Eclipse Che description: Eclipse Dirigible as Editor for Eclipse Che icon: http://download.eclipse.org/dirigible/dirigible.png endpoints: - name: \"dirigible\" public: true targetPort: 8080 attributes: protocol: http type: ide containers: - name: eclipse-dirigible image: dirigiblelabs/dirigible-anonymous env: - name: DIRIGIBLE_REPOSITORY_LOCAL_ROOT_FOLDER value: /projects/dirigible/repository - name: DIRIGIBLE_REPOSITORY_LOCAL_ROOT_FOLDER_IS_ABSOLUTE value: true - name: DIRIGIBLE_REPOSITORY_SEARCH_ROOT_FOLDER value: /projects/dirigible/repository - name: DIRIGIBLE_REPOSITORY_SEARCH_ROOT_FOLDER_IS_ABSOLUTE value: true - name: DIRIGIBLE_CMS_INTERNAL_ROOT_FOLDER value: /projects/dirigible/cms - name: DIRIGIBLE_CMS_INTERNAL_ROOT_FOLDER_IS_ABSOLUTE value: true - name: DIRIGIBLE_DATABASE_H2_ROOT_FOLDER_DEFAULT value: /projects/dirigible/h2 - name: DIRIGIBLE_DATABASE_H2_URL value: jdbc:h2:/projects/dirigible/h2 - name: DIRIGIBLE_OPERATIONS_LOGS_ROOT_FOLDER_DEFAULT value: /usr/local/tomcat/logs volumes: - mountPath: \"/projects\" name: projects ports: - exposedPort: 8080 memory-limit: \"512M\" The GitHub repository containing the project is at: https://github.com/dirigiblelabs/dirigible-che-editor-plugin Because the development environment is secured by OpenShift underneath, we use here the dirigiblelabs/dirigible-anonymous Docker image of Dirigible. Environment variables mainly redirect the file system dependent components of Dirigible to use the default /projects persistent folder. We made several experiments with different sets of configurations, which we activated on the local Che registry by using the publish_plugin command. publish_plugin dirigible-che-editor-plugin 1.0.0 che Once we agreed how the first version of the plugin should look like, we had to add our plugin definition to the official Che registry metadata to be available by default on any standard Che environment. The process says we have to make a PR to the repository: https://github.com/eclipse/che-plugin-registry . The actual PR is: https://github.com/eclipse/che-plugin-registry/pull/54 What it does is adding the metadata of the given plugin under a predefined folder structure of the Che registry: id: org.eclipse.che.editor.dirigible version: 1.0.0 type: Che Editor name: dirigible-che-editor-plugin title: Eclipse Dirigible for Eclipse Che description: Eclipse Dirigible as App Development Platform for Eclipse Che icon: https://www.dirigible.io/img/logo/dirigible-logo.png url: https://github.com/dirigiblelabs/dirigible-che-editor-plugin/releases/download/1.0.0/dirigible-che-editor-plugin.tar.gz The document with all the needed steps presented by Gorkem and Florent at EclipseCon Europe 2018 can be found at: https://docs.google.com/document/d/1hFXTwzIU3MnqcciXH9E7xyUtEJRUeUuJ-e0JlEhTKjo/edit","title":"Dirigible as a Che editor plugin"},{"location":"2018/11/12/blogs-dirigible-ide-on-che-workspaces/#start-dirigible-based-workspace-in-che","text":"Since the beginning of this week you can go on the public Che environment on the OpenShift platform at: https://che.openshift.io/dashboard/#/workspaces and create a new workspace. In the configurations wizard, choose Che 7 dev stack and dirigible-che-editor-plugin . Click the \"Create\" button and wait until the workspace gets created. Note: You may need to wait a bit more the first time for Dirigible to get initialized, then refresh the browser.","title":"Start Dirigible based workspace in Che"},{"location":"2018/11/12/blogs-dirigible-ide-on-che-workspaces/#what-is-the-benefit-of-running-dirigible-on-che","text":"Now, there are two more options for running Dirigible - on the OpenShift Cloud platform and on the Eclipse Che workspace management platform. So, what are the benefits then to run Dirigible on Che? First of all, you get a holistic experience on your development process. Che is the basis, where all the different frameworks, languages and tools complement each other to cover the maximum set of requirements for building your next generation Cloud services. We have to be really honest with ourselves here - in Dirigible it is quite unlikely that we build C/C++ tooling, for instance. This doesn't fit our goals and vision about high-productivity development of business applications. But, also we have to admit that somebody might need such a low-level component written in C/C++ for e.g. integration purposes. Thanks to this variety of options in Che now (and even more in the future), the developer doesn't need to go out to some external environments for such cases. Second, you have the ability to run Dirigible on any platform where Che is deployed. It includes the front runner OpenShift platform as well as some other commercial and enterprise-grade Cloud platforms. This opens to the huge Che community all the scenarios for business applications development such as model-driven approach, business-process and in-system programming models and all the rest coming with Dirigible. Last but not least, it is crucial for us to organize the efforts spent on the Cloud development topics under Eclipse Foundation in a much better way. Nobody has unlimited resources to solve the infinite numbers of challenges that we are currently facing. It will not be easy, but I am sure: together we can do it!","title":"What is the benefit of running Dirigible on Che"},{"location":"2018/11/13/blogs-dirigible-landed-in-faculty-of-mathematics-and-informatics/","text":"For third consecutive year, the User Assistance (UA) team of SAP Labs Bulgaria holds a Software Documentation course at the Faculty of Mathematics and Informatics at Sofia University \u2018St. Kliment Ohridski\u2019. The number of participating students is growing rapidly and this year it has reached one hundred. The main idea of the course is to present the Technical Communicator profession as well as to give students the opportunity to dive in this area and guide them through their first steps. For that purpose, we needed an open-source software to be documented by the students. It had to be easily accessible, innovative, interesting, and with a simple UI in order to grab their attention. That\u2019s how Eclipse Dirigible took central part in the Software Documentation course. Eclipse Dirigible is a Cloud Development Platform providing development tools and a runtime environment. Dirigible provides capabilities for end-to-end development processes \u2013 modeling and management of databases, development of RESTful services, generation of pattern-based user interfaces, role-based security, integration of external services, testing, debugging, operations, and monitoring. Dirigible also supports full development lifecycle of on-demand applications by leveraging in-system programming models and rapid application development techniques. The most important thing for us is that Dirigible enables the students to develop their own projects, test different technologies and scenarios, learn popular programming languages. As a member of the UA team and a graphic expert, I prepared the lecture and the assignments concerning Simple Graphics and Infographics. The lecture includes the general principles of creating graphics and infographics, typography basics, and the required software for the purpose. The assignments for the past two years consisted of creating a simple graphic for a certain task or feature, revamping existing graphics, or creating an infographic including determined information. Here is what two of our students came up with last year: I am looking forward to seeing more well-executed assignments at the end of the curriculum this year. I hope to meet some of the students as contributors to the open-source world and more specifically to the Eclipse Dirigible project.","title":"Dirigible Landed in the Faculty of Mathematics and Informatics"},{"location":"2018/11/24/blogs-node-in-dirigible/","text":"Node.js in Dirigible? Are you kidding? Why do I need Node.js, when I already have Enterprise JavaScript? Well, it was a few years ago when we defined what Enterprise JavaScript is. We described it as a set of stable API facades in the JavaScript language, which can be used by business application developers to reliably code against these API facades. For simplicity, we chose the synchronous model with blocking API, handled in a multi-threaded environment to lower the entry barrier for our target group of developers. More details can be found in the Why Enterprise JavaScript? and Understanding Dirigible blog posts. Having these reasons in mind, we set a border line between how we understand the usage of the JavaScript language and how the Node.js guys have implemented it. We are targeting different scenarios, that is why we have never striven to be compatible with it. This is still true for the main scenarios that we target. What about the \"Function as a Service\" scenarios? No doubt, there are cases and types of functionalities, which are simple (e.g. single action), short-living (quickly executable), rarely triggered, stateless, contextless, and atomic enough (without external dependencies), that can be run as \"functions\" only. This can reduce the required computation power for the execution in scale. Hence, this can significantly reduce the cost. The current Java-based runtime of Dirigible is not quite suitable for such cases. It has a longer bootstrap time, as well as a bigger memory footprint for the initialization in comparison with more lightweight frameworks like Node.js and Go. Hence, the natural path forward is to introduce Node.js and Go support to be able to offload the main stateful business application instances by moving the stateless modules out as \"functions\". This can be beneficial in a well-managed, highly distributed environment such as Kubernetes clusters. Otherwise, we recommend the standard built-in message-driven approach as well as the BPM capabilities of Dirigible. Command Engine Recently, we introduced a feature that allows the developer to write modules in any native language, which is supported by the underlying operation system. Then such modules can be executed as regular Dirigible services. We pipe the input and output streams and provide the result of the execution as a response to the HTTP call. The main purpose of this functionality is to allow developers to implement complementary extensions for their business applications via arbitrary integration channels not supported by the Dirigible core runtime out of the box. The simplest example is just to run a shell script. There are three noteworthy features of the Command Engine: 1. You can use different command line arguments depending on the target OS e.g. linux , mac , windows 2. You can set environment variables before the execution and also define the ones to be cleared 3. The target directory of the execution is set as the root directory of the Registry space of the Dirigible Repository (in case of File-System Repository) How to execute my Node.js code? Prerequisites Let's assume that you already have Node.js installed on your machine or container where the Dirigible instance is running. To test that, open the Terminal perspective and type: node -v If the command is unknown, install Node.js depending on your operating system - package-manager Hello World Example Navigate to the Workspace perspective Create a new project called hello_node . Create a new file named hello.js . Open the file in the editor and type the following line: console.log('Hello from Node!'); Save the file (auto-publish on save is set by default). Note: If you have noticed the \"Hello from Node!\" message in the Console view, it is still executed by the built-in JavaScript engine - stay calm. Create a new file named run.command Open the file in the editor and insert the following lines: { \"description\" : \"command description\" , \"contentType\" : \"text/plain\" , \"commands\" : [ { \"os\" : \"linux\" , \"command\" : \"node hello_node/hello.js\" }, { \"os\" : \"mac\" , \"command\" : \"node hello_node/hello.js\" }, { \"os\" : \"windows\" , \"command\" : \"node hello_node/hello.js\" } ] } Save the file, select it in the Workspace view, and check the result of the execution in the Preview window. What about dependencies? The simplest scenario with a single script file works well, what about multiple files with cross references? Create a new project called complex_node Create a file that will play the role of the library module node_lib.js Enter the following lines: exports . sum = function ( a , b ) { return a + b ; } Create a new file that will play the role of the service module node_service.js Enter the following lines: var node_lib = require ( \"../complex_node/node_lib\" ); var sum = node_lib . sum ( 2 , 3 ); console . log ( \"The Sum is: \" + sum ); Now, create the command file with name run.command and with the following content: { \"description\" : \"command description\" , \"contentType\" : \"text/plain\" , \"commands\" : [ { \"os\" : \"linux\" , \"command\" : \"node complex_node/node_service.js\" }, { \"os\" : \"mac\" , \"command\" : \"node complex_node/node_service.js\" }, { \"os\" : \"windows\" , \"command\" : \"node complex_node/node_service.js\" } ] } Select the file in the Workspace Explorer and see the result of the execution in the Preview window: The Sum is: 5 Node.js is cool, but it is still JavaScript. What about Go? Well, let's try a simple Go program following the same approach. If you do not have Go installed on your machine or container, just follow these instructions . Create a project called hello_go Create a file with name hello.go with the following content: package main import \"fmt\" func main () { fmt . Println ( \"hello world\" ) } Create the run.command with the following content: { \"description\" : \"command description\" , \"contentType\" : \"text/plain\" , \"commands\" : [ { \"os\" : \"linux\" , \"command\" : \"go run ./hello_go/hello.go\" }, { \"os\" : \"mac\" , \"command\" : \"go run ./hello_go/hello.go\" }, { \"os\" : \"windows\" , \"command\" : \"go run ./hello_go/hello.go\" } ] } After you publish and select the run.command file in the Workspace Explorer, you should see the following result in the Preview window: hello world That was nice! Can I run Java maybe? Let's assume that for some reason you would like to write a \"function\" module in the Java programming language (e.g. with the newest GraalVM). In this case we will need a preliminary step to compile the Java class before the execution. Create a project named hello_java Create a file named Hello.java with the following content: package hello_java ; public class Hello { public static void main ( String [] args ) { System . out . println ( \"Hello World!\" ); } } Create a shell command script run.sh to first compile and then execute the class: javac ./hello_java/Hello.java java hello_java.Hello Finally create the command file run.command with the following content: { \"description\" : \"command description\" , \"contentType\" : \"text/plain\" , \"commands\" : [ { \"os\" : \"linux\" , \"command\" : \"sh hello_java/run.sh\" }, { \"os\" : \"mac\" , \"command\" : \"sh hello_java/run.sh\" }, { \"os\" : \"windows\" , \"command\" : \"hello_java/run.bat\" } ] } Save all, publish and voila - you have a running Java program as a service in Dirigible! What's next? We have to admit that the introduction of the new Command engine is just the first step in the direction of the native multi-language support in Dirigible. There is still a lot to be done to adapt the Language Server Protocol extensions for the different languages. Fortunately, there are already very good examples available on how to do this at the backend like Eclipse standalone IDE, and in the editors - Monaco and Orion. But still, this is not a trivial task and it will take some time to implement in Dirigible. Another idea is to have a pool of running native servers, e.g Express nodes, to push the code to and to use them as external executors. In this case, we can pipe the network Socket streams instead of the standard in/out streams. This approach can lay the foundation for many more scenarios as well. Integration with the actual \"Function as a Service\" offerings by the Cloud platforms is also something that is yet to come. For instance, a tool for building Docker images based on these native modules and Cloud platform Registry integration would be a very useful feature of the Dirigible Web IDE. Summary? Well, for those of you who have read the blog post too fast, it explains how to write native modules in Node.js , Go , and Java and use them as integrated services in Dirigible environment. Enjoy!","title":"Node.js in Dirigible?"},{"location":"2018/11/24/blogs-node-in-dirigible/#why-do-i-need-nodejs-when-i-already-have-enterprise-javascript","text":"Well, it was a few years ago when we defined what Enterprise JavaScript is. We described it as a set of stable API facades in the JavaScript language, which can be used by business application developers to reliably code against these API facades. For simplicity, we chose the synchronous model with blocking API, handled in a multi-threaded environment to lower the entry barrier for our target group of developers. More details can be found in the Why Enterprise JavaScript? and Understanding Dirigible blog posts. Having these reasons in mind, we set a border line between how we understand the usage of the JavaScript language and how the Node.js guys have implemented it. We are targeting different scenarios, that is why we have never striven to be compatible with it. This is still true for the main scenarios that we target. What about the \"Function as a Service\" scenarios? No doubt, there are cases and types of functionalities, which are simple (e.g. single action), short-living (quickly executable), rarely triggered, stateless, contextless, and atomic enough (without external dependencies), that can be run as \"functions\" only. This can reduce the required computation power for the execution in scale. Hence, this can significantly reduce the cost. The current Java-based runtime of Dirigible is not quite suitable for such cases. It has a longer bootstrap time, as well as a bigger memory footprint for the initialization in comparison with more lightweight frameworks like Node.js and Go. Hence, the natural path forward is to introduce Node.js and Go support to be able to offload the main stateful business application instances by moving the stateless modules out as \"functions\". This can be beneficial in a well-managed, highly distributed environment such as Kubernetes clusters. Otherwise, we recommend the standard built-in message-driven approach as well as the BPM capabilities of Dirigible.","title":"Why do I need Node.js, when I already have Enterprise JavaScript?"},{"location":"2018/11/24/blogs-node-in-dirigible/#command-engine","text":"Recently, we introduced a feature that allows the developer to write modules in any native language, which is supported by the underlying operation system. Then such modules can be executed as regular Dirigible services. We pipe the input and output streams and provide the result of the execution as a response to the HTTP call. The main purpose of this functionality is to allow developers to implement complementary extensions for their business applications via arbitrary integration channels not supported by the Dirigible core runtime out of the box. The simplest example is just to run a shell script. There are three noteworthy features of the Command Engine: 1. You can use different command line arguments depending on the target OS e.g. linux , mac , windows 2. You can set environment variables before the execution and also define the ones to be cleared 3. The target directory of the execution is set as the root directory of the Registry space of the Dirigible Repository (in case of File-System Repository)","title":"Command Engine"},{"location":"2018/11/24/blogs-node-in-dirigible/#how-to-execute-my-nodejs-code","text":"","title":"How to execute my Node.js code?"},{"location":"2018/11/24/blogs-node-in-dirigible/#prerequisites","text":"Let's assume that you already have Node.js installed on your machine or container where the Dirigible instance is running. To test that, open the Terminal perspective and type: node -v If the command is unknown, install Node.js depending on your operating system - package-manager","title":"Prerequisites"},{"location":"2018/11/24/blogs-node-in-dirigible/#hello-world-example","text":"Navigate to the Workspace perspective Create a new project called hello_node . Create a new file named hello.js . Open the file in the editor and type the following line: console.log('Hello from Node!'); Save the file (auto-publish on save is set by default). Note: If you have noticed the \"Hello from Node!\" message in the Console view, it is still executed by the built-in JavaScript engine - stay calm. Create a new file named run.command Open the file in the editor and insert the following lines: { \"description\" : \"command description\" , \"contentType\" : \"text/plain\" , \"commands\" : [ { \"os\" : \"linux\" , \"command\" : \"node hello_node/hello.js\" }, { \"os\" : \"mac\" , \"command\" : \"node hello_node/hello.js\" }, { \"os\" : \"windows\" , \"command\" : \"node hello_node/hello.js\" } ] } Save the file, select it in the Workspace view, and check the result of the execution in the Preview window.","title":"Hello World Example"},{"location":"2018/11/24/blogs-node-in-dirigible/#what-about-dependencies","text":"The simplest scenario with a single script file works well, what about multiple files with cross references? Create a new project called complex_node Create a file that will play the role of the library module node_lib.js Enter the following lines: exports . sum = function ( a , b ) { return a + b ; } Create a new file that will play the role of the service module node_service.js Enter the following lines: var node_lib = require ( \"../complex_node/node_lib\" ); var sum = node_lib . sum ( 2 , 3 ); console . log ( \"The Sum is: \" + sum ); Now, create the command file with name run.command and with the following content: { \"description\" : \"command description\" , \"contentType\" : \"text/plain\" , \"commands\" : [ { \"os\" : \"linux\" , \"command\" : \"node complex_node/node_service.js\" }, { \"os\" : \"mac\" , \"command\" : \"node complex_node/node_service.js\" }, { \"os\" : \"windows\" , \"command\" : \"node complex_node/node_service.js\" } ] } Select the file in the Workspace Explorer and see the result of the execution in the Preview window: The Sum is: 5","title":"What about dependencies?"},{"location":"2018/11/24/blogs-node-in-dirigible/#nodejs-is-cool-but-it-is-still-javascript-what-about-go","text":"Well, let's try a simple Go program following the same approach. If you do not have Go installed on your machine or container, just follow these instructions . Create a project called hello_go Create a file with name hello.go with the following content: package main import \"fmt\" func main () { fmt . Println ( \"hello world\" ) } Create the run.command with the following content: { \"description\" : \"command description\" , \"contentType\" : \"text/plain\" , \"commands\" : [ { \"os\" : \"linux\" , \"command\" : \"go run ./hello_go/hello.go\" }, { \"os\" : \"mac\" , \"command\" : \"go run ./hello_go/hello.go\" }, { \"os\" : \"windows\" , \"command\" : \"go run ./hello_go/hello.go\" } ] } After you publish and select the run.command file in the Workspace Explorer, you should see the following result in the Preview window: hello world","title":"Node.js is cool, but it is still JavaScript. What about Go?"},{"location":"2018/11/24/blogs-node-in-dirigible/#that-was-nice-can-i-run-java-maybe","text":"Let's assume that for some reason you would like to write a \"function\" module in the Java programming language (e.g. with the newest GraalVM). In this case we will need a preliminary step to compile the Java class before the execution. Create a project named hello_java Create a file named Hello.java with the following content: package hello_java ; public class Hello { public static void main ( String [] args ) { System . out . println ( \"Hello World!\" ); } } Create a shell command script run.sh to first compile and then execute the class: javac ./hello_java/Hello.java java hello_java.Hello Finally create the command file run.command with the following content: { \"description\" : \"command description\" , \"contentType\" : \"text/plain\" , \"commands\" : [ { \"os\" : \"linux\" , \"command\" : \"sh hello_java/run.sh\" }, { \"os\" : \"mac\" , \"command\" : \"sh hello_java/run.sh\" }, { \"os\" : \"windows\" , \"command\" : \"hello_java/run.bat\" } ] } Save all, publish and voila - you have a running Java program as a service in Dirigible!","title":"That was nice! Can I run Java maybe?"},{"location":"2018/11/24/blogs-node-in-dirigible/#whats-next","text":"We have to admit that the introduction of the new Command engine is just the first step in the direction of the native multi-language support in Dirigible. There is still a lot to be done to adapt the Language Server Protocol extensions for the different languages. Fortunately, there are already very good examples available on how to do this at the backend like Eclipse standalone IDE, and in the editors - Monaco and Orion. But still, this is not a trivial task and it will take some time to implement in Dirigible. Another idea is to have a pool of running native servers, e.g Express nodes, to push the code to and to use them as external executors. In this case, we can pipe the network Socket streams instead of the standard in/out streams. This approach can lay the foundation for many more scenarios as well. Integration with the actual \"Function as a Service\" offerings by the Cloud platforms is also something that is yet to come. For instance, a tool for building Docker images based on these native modules and Cloud platform Registry integration would be a very useful feature of the Dirigible Web IDE.","title":"What's next?"},{"location":"2018/11/24/blogs-node-in-dirigible/#summary","text":"Well, for those of you who have read the blog post too fast, it explains how to write native modules in Node.js , Go , and Java and use them as integrated services in Dirigible environment.","title":"Summary?"},{"location":"2018/11/24/blogs-node-in-dirigible/#enjoy","text":"","title":"Enjoy!"},{"location":"2018/12/05/you-dont-need-abs-to-model-apps/","text":"Two of the coolest additions in version 3.x of Eclipse Dirigible are without a doubt the Entity Data Modeler (EDM) and the Business Process Modeler (BPM). They take the concept of model-driven architecture to the next level. What the \u201chack\u201d does that mean, you\u2019d probably say? With the fast pace of technological evolution (Cloud Computing, AI, IoT, etc.), application developers face a common problem \u2013 how to develop proof of concepts as quickly as possible, and to receive early feedback from customers. Dirigible provides you with the opportunity to use In-System and Rapid application development techniques such as code generation, EDM, BPM, and Enterprise JavaScript APIs. The Entity Data Modeler plays a huge role in model-driven development. It allows developers to create application prototypes based on linked entities. These entities can be Books, Stores, Currencies, Categories, among others. Everything that your customers may ever need. Every single one of these entities has its own unique identifier (ID), along with any other set of unique properties you\u2019d like to assign to these entities. They are stored in database tables and you can interconnect them to make your business application come true. To develop business applications, you need persistency, RESTful services, and a user interface with CRUD operations to create, edit, delete, and list entities. There are predefined templates that help you generate your application from the model, so you don\u2019t have to start the development process from scratch. Based on the model you\u2019ve come up with, you can then generate your full-stack application: Awesome, isn\u2019t it? But\u2026 that\u2019s just a set of well-organized CRUD views integrated into an Admin UI, which was generated from the EDM. Sure, the Entity Data Modeler can help me join the dots between all app entities, but how can I develop a more complex feature in the app itself? What about building a marketplace, or a feedback system, or a service request system for customers to use? Of course, you can build applications with separate data, logic, and presentation layers. These layers can reuse parts of the admin functionalities. For example, here is a bookshop marketplace that consumes several services from the admin application: That sounds great, but behind every enterprise solution, there must be a set of very complex business processes and models. Is there a way to develop this type of processes with Dirigible or even reuse already existing Activiti/Flowable processes? Sure, you can! All this is possible thanks to the Business Process Modeler. Eclipse Dirigible has an integrated BPM engine based on Flowable. This engine lets you define or reuse your business processes by adding service tasks and flows. These tasks enhance your business solution and will truly make your application enterprise-ready. What does that mean exactly, you'd say? Let's take a brief look at the following example: This is what the flow of a Print on Demand business process looks like at the background in the Eclipse Dirigible Web IDE. Meanwhile, here is an example for a custom-built view that lets users purchase their books on demand: By the way, this Print on Demand view is integrated in the marketplace thanks to the lightweight extensibility concept built in Dirigible. See Dirigible - Extend, Embed, Reuse . Right now, if there was a book on developing business applications with the Eclipse Dirigible Web IDE, I\u2019d be the first one to purchase it. While I\u2019m waiting though, I\u2019ll keep on exploring the ins and outs of this cool basket of modern tools for developers of business applications. The EDM and the BPM tools are just the beginning, I\u2019m sure of it! Resources Experiment with the single-click deployment of the following demos from EclipseCon 2018 : ( GitHub ) ( GitHub ) ( GitHub )","title":"You Dont Need Abs to Model Apps"},{"location":"2018/12/05/you-dont-need-abs-to-model-apps/#resources","text":"Experiment with the single-click deployment of the following demos from EclipseCon 2018 : ( GitHub ) ( GitHub ) ( GitHub )","title":"Resources"},{"location":"2019/09/25/how-does-eclipse-dirigible-contribute-to-eclipse-che-7/","text":"Eclipse Che unites a wide range of different frameworks, programming languages, and development tools, and helps developers design and create next-level services on the Cloud. Eclipse Che provides you with a default Web IDE. However, Eclipse Che also allows you to plug in other IDEs, because the default IDE may not be able to cover your use case. For example, we work on Eclipse Dirigible \u2013 an open-source cloud development platform that comes with its own Web IDE. You can directly integrate the Eclipse Dirigible Web IDE in Eclipse Che. This means that you can create a workspace in Eclipse Ch\u0435 using the Eclipse Dirigible Web IDE instead of the default Eclipse Theia IDE. What is so cool about that? Developers can run Eclipse Dirigible on whatever platform Eclipse Che 7 is deployed. The most notable example is the OpenShift platform offered by Red Hat. That way, the Eclipse Dirigible portfolio of services and features for business application development becomes available to the entire Che community. What does Eclipse Dirigible have to offer? Now, let us take a look at what this portfolio currently consists of. Eclipse Dirigible puts an emphasis on low-code/no-code tools for developing business applications. As of version 3.4, Eclipse Dirigible provides the following tools: In-system development with server-side JavaScript You can develop backend applications using Enterprise JavaScript. Enhanced RESTful frameworks - RS and RS data Entity data modeler You can generate an application using predefined application templates. Business process modeler You can model process flows and implement in-system and Java tasks. Database modeler You can design your own database schema with tables, views, and their relations. Job scheduler You can define declaratively and schedule jobs that run regularly. Message listener You can create topics and queues and subscribe for events. Kubernetes support For productive use cases, we recommend that you use Kubernetes, Keycloak, and PostgreSQL. Why use JavaScript as a business application language? At Dirigible, we have decided to focus on JavaScript, because it has a small learning curve, and it is a well-known programming language that has proven itself in the context of web development throughout the years. For business application development, which is our case, JavaScript is just a tool, which lets you consume the standardized set of Enterprise APIs that we provide. Additionally, Dirigible allows you to set the default server-side JavaScript execution as synchronous, so you could develop your service in a callback-free way. For example, some of the most popular Enterprise APIs that you can use are: Database / Database DAO HTTP Client / HTTP Client Async CMIS HTTP Request / HTTP Response / HTTP RS BPM Process Are there any alternatives to Eclipse Dirigible? There are alternatives to Eclipse Dirigible and these are platforms such as Mendix and Force.com by Salesforce. However, you have to purchase the corresponding licenses to start using them. In the open-source world though, there are no alternatives to this day. There are other open-source platforms such as Eclipse Theia and Jupyter, but they are not competing directly with Eclipse Dirigible, because they specialize in other areas. For example, Eclipse Theia focuses on general-purpose code editing using the VSCode platform while Jupyter, on the other hand, is the right choice when it comes to big data analysis and data mining. However, none of these platforms provide what Eclipse Dirigible has to offer in terms of low-code/no-code tools for developing business applications. At least for now. Conclusions Thanks to the great collaboration with the Eclipse Che team, Eclipse Dirigible is on the right way of achieving its ultimate goal, which is to provide developers of business applications with the fastest turnaround time in the Cloud and a unique user experience at the same time. So why don\u2019t you give it a try ? If there is something that you don't like, or you think it can be improved, don't hesitate to share your feedback . The Eclipse Dirigible team will definitely appreciate it. That is one of the best things about the open-source community that we are all part of! Resources How to Run Eclipse Dirigible on Eclipse Che 7","title":"How Does Eclipse Dirigible Contribute to Eclipse Che 7?"},{"location":"2019/09/25/how-does-eclipse-dirigible-contribute-to-eclipse-che-7/#what-is-so-cool-about-that","text":"Developers can run Eclipse Dirigible on whatever platform Eclipse Che 7 is deployed. The most notable example is the OpenShift platform offered by Red Hat. That way, the Eclipse Dirigible portfolio of services and features for business application development becomes available to the entire Che community.","title":"What is so cool about that?"},{"location":"2019/09/25/how-does-eclipse-dirigible-contribute-to-eclipse-che-7/#what-does-eclipse-dirigible-have-to-offer","text":"Now, let us take a look at what this portfolio currently consists of. Eclipse Dirigible puts an emphasis on low-code/no-code tools for developing business applications. As of version 3.4, Eclipse Dirigible provides the following tools: In-system development with server-side JavaScript You can develop backend applications using Enterprise JavaScript. Enhanced RESTful frameworks - RS and RS data Entity data modeler You can generate an application using predefined application templates. Business process modeler You can model process flows and implement in-system and Java tasks. Database modeler You can design your own database schema with tables, views, and their relations. Job scheduler You can define declaratively and schedule jobs that run regularly. Message listener You can create topics and queues and subscribe for events. Kubernetes support For productive use cases, we recommend that you use Kubernetes, Keycloak, and PostgreSQL.","title":"What does Eclipse Dirigible have to offer?"},{"location":"2019/09/25/how-does-eclipse-dirigible-contribute-to-eclipse-che-7/#why-use-javascript-as-a-business-application-language","text":"At Dirigible, we have decided to focus on JavaScript, because it has a small learning curve, and it is a well-known programming language that has proven itself in the context of web development throughout the years. For business application development, which is our case, JavaScript is just a tool, which lets you consume the standardized set of Enterprise APIs that we provide. Additionally, Dirigible allows you to set the default server-side JavaScript execution as synchronous, so you could develop your service in a callback-free way. For example, some of the most popular Enterprise APIs that you can use are: Database / Database DAO HTTP Client / HTTP Client Async CMIS HTTP Request / HTTP Response / HTTP RS BPM Process","title":"Why use JavaScript as a business application language?"},{"location":"2019/09/25/how-does-eclipse-dirigible-contribute-to-eclipse-che-7/#are-there-any-alternatives-to-eclipse-dirigible","text":"There are alternatives to Eclipse Dirigible and these are platforms such as Mendix and Force.com by Salesforce. However, you have to purchase the corresponding licenses to start using them. In the open-source world though, there are no alternatives to this day. There are other open-source platforms such as Eclipse Theia and Jupyter, but they are not competing directly with Eclipse Dirigible, because they specialize in other areas. For example, Eclipse Theia focuses on general-purpose code editing using the VSCode platform while Jupyter, on the other hand, is the right choice when it comes to big data analysis and data mining. However, none of these platforms provide what Eclipse Dirigible has to offer in terms of low-code/no-code tools for developing business applications. At least for now.","title":"Are there any alternatives to Eclipse Dirigible?"},{"location":"2019/09/25/how-does-eclipse-dirigible-contribute-to-eclipse-che-7/#conclusions","text":"Thanks to the great collaboration with the Eclipse Che team, Eclipse Dirigible is on the right way of achieving its ultimate goal, which is to provide developers of business applications with the fastest turnaround time in the Cloud and a unique user experience at the same time. So why don\u2019t you give it a try ? If there is something that you don't like, or you think it can be improved, don't hesitate to share your feedback . The Eclipse Dirigible team will definitely appreciate it. That is one of the best things about the open-source community that we are all part of!","title":"Conclusions"},{"location":"2019/09/25/how-does-eclipse-dirigible-contribute-to-eclipse-che-7/#resources","text":"How to Run Eclipse Dirigible on Eclipse Che 7","title":"Resources"},{"location":"2020/01/20/managing-documents-with-eclipse-dirigible-in-the-sap-cloud-platform-neo-environment/","text":"This blog presents the Document Explorer \u2013 a feature that you can use as part of the Eclipse Dirigible Web IDE. The Explorer allows you to upload, overwrite, download, delete, and search for pictures, spreadsheets, PDF files, and videos, among other artifacts. The Document Explorer of Dirigible is compatible with SAP Cloud Platform Document service in the Neo environment ... To read the whole article go to: Managing Documents with Eclipse Dirigible in the SAP Cloud Platform Neo Environment","title":"Managing Documents with Eclipse Dirigible in the SAP Cloud Platform Neo Environment"},{"location":"2020/02/05/sending-emails-with-the-eclipse-dirigible-mail-api/","text":"In this blog, we\u2019ll take a look at one of the newest Eclipse Dirigible features. It allows you to send e-mails from Eclipse Dirigible with the Mail Client API . For the purpose of this demo, I\u2019ll be using my Eclipse Dirigible (SAP Ephemeral edition) setup in the Neo environment of SAP Cloud Platform along with a Gmail account used for testing this use case ... To read the whole article go to: Sending E-Mails with the Eclipse Dirigible Mail API","title":"Sending E-Mails with the Eclipse Dirigible Mail API"},{"location":"2020/02/15/how-to-deploy-eclipse-dirigible-in-the-sap-cloud-platform-cloud-foundry-environment/","text":"Eclipse Dirigible is an open-source cloud development platform, that provides capabilities for end-to-end development process from database modeling and management, through RESTful services using server-side JavaScript, to pattern-based user interface generation, role based security, external services integration, testing, debugging, operations and monitoring ... To read the whole article go to: How to deploy Eclipse Dirigible in the SAP Cloud Platform Cloud Foundry environment","title":"How to deploy Eclipse Dirigible in the SAP Cloud Platform Cloud Foundry environment"},{"location":"2020/02/15/how-to-deploy-eclipse-dirigible-in-the-sap-cloud-platform-neo-environment/","text":"Eclipse Dirigible is an open-source cloud development platform, that provides capabilities for end-to-end development process from database modeling and management, through RESTful services using server-side JavaScript, to pattern-based user interface generation, role based security, external services integration, testing, debugging, operations and monitoring ... To read the whole article go to: How to deploy Eclipse Dirigible in the SAP Cloud Platform Neo environment","title":"How to deploy Eclipse Dirigible in the SAP Cloud Platform Neo environment"},{"location":"2020/05/07/what-is-eclipse-dirigible/","text":"Eclipse Dirigible is an open source project that provides development tools (Web IDE) and runtime environment (Java based) for building and running Business Applications in the Cloud. Dirigible provides the shortest possible turnaround time, during application development and boosts the developers\u2019 productivity with modeling tools, application templates and In-System development experience ... To read the whole article go to: What is Eclipse Dirigible?","title":"What is Eclipse Dirigible?"},{"location":"2020/06/29/celebrating-5-years-in-open-source/","text":"Eclipse Dirigible just turned five years in open source. Five years in Eclipse Foundation within the Eclipse Cloud Development group. Five years of innovations with many friends around the globe, lots of realised dreams, first-class happiness. GraalVM 's GraalJS Fast, easy-to-use, easy-to-upgrade from Nashorn or Rhino, ECMA 2020 compliant, deeply integrated with the host JVM, fast interoperability with JVM languages like Scala and Kotlin, embeddable, JVM agnostic, stable, robust... just perfect for our needs. We were quite happy so far by using Mozilla Rhino as the default scripting engine, but its slow adoption of the most recent ECMA specs was definitely an issue. Hence, we were kind of forced to look for another option for the future development of the stack. The biggest surprise was about the time and efforts it took to adapt our API layer to use GraalJS instead of Rhino - literally zero. How many projects or products support straigthforward and compatible migration from one major version to another? The fact that GraalJS is even a totally different project, driven by different people and still provides a smooth migration path from Rhino, deserves admirations. Chrome DevTools Another invaluable gift coming along with GraalJS is the Chrome DevTools debug protocol support. A few years ago we tried to adapt Rhino and V8 debug API and to expose them to the Chrome DevTools, but it was quite unstable due to the lack of public specification of the debug protocol itself and on the other hand not so trivial behavior of the tools themselves. So, you can imagine how speechless we were left once we tried to connect the dots and it just worked. The decision to replace our Debug Perspective's own tools with the well-known yet very powerful Chrome DevTools came naturally. Xterm.js We were also happy to add one more jewel in the most recent release was the Xterm.js terminal interface written in JavaScript and run entirely in the browser. It is adopted quite widely by many tools already as most prominent ones are the VSCode itslef and Eclipse Theia/Che projects. It connects to the server-side endpoint via websocket, so no need to open the port 22 on the server as it was a security-related requirement from our side. It integrates nicely with the ttyd terminal server, which we have embedded in the stack as well. Monaco Major advantage of VSCode is its editor - Monaco. During the past few years it became the most advanced open source code editor, that\u2019s easy to embed and enhance. The investment and support by Microsoft in VSCode and Monaco in particular, gives a good perspective and confidence of the project's future. We were quite happy by using Orion, but recently we decided to bet on Monaco as the default code editor for version 5.0 and above of Dirigible. All the innovations and integrations related to writing source code assumed to go to Monaco now. Another benefit of using Monaco is its diff editor, which became a part of the last but not least major feature of 5.0 release. Git Support Git support in Dirigible was available, since the very beginning. You could clone, push pull or share projects. So far, the supported operations were over-simplified due to the fact that the file system (workspaces, projects, files) was abstracted. It was possible to have workspaces stored in a RDBMS for instance. This had its advantage when you had to run Dirigible on a platform with limited functionalities or by some other reasons. Of course, the drawback was the very limited support of Git integrations, which in fact is more important for developers than having an abstract file system. In 5.0 we decided to first stick to the native file system only, then it was possible to implement a full-fledged Git perspective with listing and changes of branches, low-level operations on files for staging, diff editor, etc. Conclusion With the latest release we set the future direction of the Dirigible project from a technology perspective. We fixed the problematic dependencies by betting on new and emerging projects as well as reverted some of the questionable architectural decisions from the past the future of Dirigible looks quite bright. What's next? Now, we can safely focus on what Eclipse Dirigible always was supposed to give to developers - high-productivity application development platform. Many improvements to the MDA tools are planned already related to the built-in extensibility of the entities, distributed models, security validations and role-based access management in generation templates. The unified inbox for process events as well as better integration of user tasks are examples of what we see in the BPM related tools as next steps. Do you want to join forces with us in this endeavor? Graduation One more good news came along with this release - completed graduation review! We are mature - ayeeeee! \ud83d\udc83 \ud83d\udd7a \ud83e\uddd1\u200d\ud83c\udf93\ud83d\udc68\u200d\ud83c\udf93\ud83e\uddd1\u200d\ud83c\udfa4\ud83d\udc68\u200d\ud83c\udfa4\ud83d\udc69\u200d\ud83c\udfa4\ud83e\udd35\ud83d\udc70\ud83d\udc68\u200d\ud83e\uddb3\ud83e\uddd1\u200d\ud83e\uddb3\ud83d\udc69\u200d\ud83d\ude80\ud83e\uddd1\u200d\ud83d\ude92\ud83e\uddd1\u200d\ud83c\udf3e OMG! \ud83e\udd26\u200d\u2642\ufe0f","title":"Eclipse Dirigible 5.0 - celebrating 5 years in open source with 5 killer features"},{"location":"2020/06/29/celebrating-5-years-in-open-source/#graalvms-graaljs","text":"Fast, easy-to-use, easy-to-upgrade from Nashorn or Rhino, ECMA 2020 compliant, deeply integrated with the host JVM, fast interoperability with JVM languages like Scala and Kotlin, embeddable, JVM agnostic, stable, robust... just perfect for our needs. We were quite happy so far by using Mozilla Rhino as the default scripting engine, but its slow adoption of the most recent ECMA specs was definitely an issue. Hence, we were kind of forced to look for another option for the future development of the stack. The biggest surprise was about the time and efforts it took to adapt our API layer to use GraalJS instead of Rhino - literally zero. How many projects or products support straigthforward and compatible migration from one major version to another? The fact that GraalJS is even a totally different project, driven by different people and still provides a smooth migration path from Rhino, deserves admirations.","title":"GraalVM's GraalJS"},{"location":"2020/06/29/celebrating-5-years-in-open-source/#chrome-devtools","text":"Another invaluable gift coming along with GraalJS is the Chrome DevTools debug protocol support. A few years ago we tried to adapt Rhino and V8 debug API and to expose them to the Chrome DevTools, but it was quite unstable due to the lack of public specification of the debug protocol itself and on the other hand not so trivial behavior of the tools themselves. So, you can imagine how speechless we were left once we tried to connect the dots and it just worked. The decision to replace our Debug Perspective's own tools with the well-known yet very powerful Chrome DevTools came naturally.","title":"Chrome DevTools"},{"location":"2020/06/29/celebrating-5-years-in-open-source/#xtermjs","text":"We were also happy to add one more jewel in the most recent release was the Xterm.js terminal interface written in JavaScript and run entirely in the browser. It is adopted quite widely by many tools already as most prominent ones are the VSCode itslef and Eclipse Theia/Che projects. It connects to the server-side endpoint via websocket, so no need to open the port 22 on the server as it was a security-related requirement from our side. It integrates nicely with the ttyd terminal server, which we have embedded in the stack as well.","title":"Xterm.js"},{"location":"2020/06/29/celebrating-5-years-in-open-source/#monaco","text":"Major advantage of VSCode is its editor - Monaco. During the past few years it became the most advanced open source code editor, that\u2019s easy to embed and enhance. The investment and support by Microsoft in VSCode and Monaco in particular, gives a good perspective and confidence of the project's future. We were quite happy by using Orion, but recently we decided to bet on Monaco as the default code editor for version 5.0 and above of Dirigible. All the innovations and integrations related to writing source code assumed to go to Monaco now. Another benefit of using Monaco is its diff editor, which became a part of the last but not least major feature of 5.0 release.","title":"Monaco"},{"location":"2020/06/29/celebrating-5-years-in-open-source/#git-support","text":"Git support in Dirigible was available, since the very beginning. You could clone, push pull or share projects. So far, the supported operations were over-simplified due to the fact that the file system (workspaces, projects, files) was abstracted. It was possible to have workspaces stored in a RDBMS for instance. This had its advantage when you had to run Dirigible on a platform with limited functionalities or by some other reasons. Of course, the drawback was the very limited support of Git integrations, which in fact is more important for developers than having an abstract file system. In 5.0 we decided to first stick to the native file system only, then it was possible to implement a full-fledged Git perspective with listing and changes of branches, low-level operations on files for staging, diff editor, etc.","title":"Git Support"},{"location":"2020/06/29/celebrating-5-years-in-open-source/#conclusion","text":"With the latest release we set the future direction of the Dirigible project from a technology perspective. We fixed the problematic dependencies by betting on new and emerging projects as well as reverted some of the questionable architectural decisions from the past the future of Dirigible looks quite bright. What's next? Now, we can safely focus on what Eclipse Dirigible always was supposed to give to developers - high-productivity application development platform. Many improvements to the MDA tools are planned already related to the built-in extensibility of the entities, distributed models, security validations and role-based access management in generation templates. The unified inbox for process events as well as better integration of user tasks are examples of what we see in the BPM related tools as next steps. Do you want to join forces with us in this endeavor?","title":"Conclusion"},{"location":"2020/06/29/celebrating-5-years-in-open-source/#graduation","text":"One more good news came along with this release - completed graduation review! We are mature - ayeeeee! \ud83d\udc83 \ud83d\udd7a \ud83e\uddd1\u200d\ud83c\udf93\ud83d\udc68\u200d\ud83c\udf93\ud83e\uddd1\u200d\ud83c\udfa4\ud83d\udc68\u200d\ud83c\udfa4\ud83d\udc69\u200d\ud83c\udfa4\ud83e\udd35\ud83d\udc70\ud83d\udc68\u200d\ud83e\uddb3\ud83e\uddd1\u200d\ud83e\uddb3\ud83d\udc69\u200d\ud83d\ude80\ud83e\uddd1\u200d\ud83d\ude92\ud83e\uddd1\u200d\ud83c\udf3e OMG! \ud83e\udd26\u200d\u2642\ufe0f","title":"Graduation"},{"location":"2020/10/23/how-to-deploy-eclipse-dirigible-in-the-sap-cloud-platform-kyma-environment/","text":"Eclipse Dirigible is an open-source cloud development platform, that provides capabilities for end-to-end development process from database modeling and management, through RESTful services using server-side JavaScript, to pattern-based user interface generation, role based security, external services integration, testing, debugging, operations and monitoring ... To read the whole article go to: How to deploy Eclipse Dirigible in the SAP Cloud Platform Kyma environment","title":"How to deploy Eclipse Dirigible in the SAP Cloud Platform Kyma environment"},{"location":"2020/11/19/eclipse-dirigible-building-application-docker-image/","text":"Starting with Eclipse Dirigible 5.0, there are built-in integrations with the SAP Cloud Platform Cloud Foundry and Kyma environments. To deploy Eclipse Dirigible on either one of these environments, you need to use a Docker image ... To read the whole article go to: Eclipse Dirigible \u2013 Building a Docker Image for Your Application","title":"Eclipse Dirigible \u2013 Building a Docker Image for Your Application"},{"location":"2020/11/20/how-to-deploy-eclipse-dirigible-private-docker-image-in-the-sap-cloud-platform-cloud-foundry-environment/","text":"Starting with Eclipse Dirigible 5.0, there are built-in integrations with the SAP Cloud Platform Cloud Foundry and Kyma environments. To deploy Eclipse Dirigible on either one of these environments, you need to use a Docker image. In this tutorial, I\u2019m going to show you how to deploy Eclipse Dirigible based Docker image from a private Docker repository ... To read the whole article go to: How to deploy Eclipse Dirigible private Docker image in the SAP Cloud Platform Cloud Foundry environment","title":"How to deploy Eclipse Dirigible private Docker image in the SAP Cloud Platform Cloud Foundry environment"},{"location":"2020/11/24/eclipse-dirigible-sap-identity-and-authentication-service-integration-on-the-sap-cloud-platform/","text":"Starting with Eclipse Dirigible 5.0, there are built-in integrations with the SAP Cloud Platform Cloud Foundry and Kyma environments. Beyond the most basic configuration with the default Identity Provider, there are plenty of use cases where integration with the SAP Identity and Authentication Service is needed ... To read the whole article go to: Eclipse Dirigible \u2013 SAP Identity and Authentication Service Integration on the SAP Cloud Platform","title":"Eclipse Dirigible \u2013 SAP Identity and Authentication Service Integration on the SAP Cloud Platform"},{"location":"2020/11/24/how-to-deploy-eclipse-dirigible-private-docker-image-in-the-sap-cloud-platform-kyma-environment/","text":"Starting with Eclipse Dirigible 5.0, there are built-in integrations with the SAP Cloud Platform Cloud Foundry and Kyma environments. To deploy Eclipse Dirigible on either one of these environments, you need to use a Docker image. In this tutorial, I\u2019m going to show you how to deploy Eclipse Dirigible based Docker image from a private Docker repository ... To read the whole article go to: How to deploy Eclipse Dirigible private Docker image in the SAP Cloud Platform Kyma environment","title":"How to deploy Eclipse Dirigible private Docker image in the SAP Cloud Platform Kyma environment"},{"location":"2020/12/16/2020-what-a-year-for-eclipse-dirigible/","text":"It has been a challenging, but in the same time an incredible year for Eclipse Dirigible in terms of progress, contribution and adoption. 2020 in numbers: 12 releases from 4.2 to 5.6 with 1 major 5.0 138 issues fixed 11 blog posts 10 new API - Content, Template Engine, Execute, Lifecycle, SOAP, Websocket, Kafka Consumer, Kafka Producer, MongoDB Client, MongoDB DAO 11K+ users from 143 countries for 2020 412 repositories in DirigibleLabs till date Notable new features Packages for SAP Cloud Platfom Neo , Cloud Foundry and Kyma OData v2 support based on Apache Olingo Entity Modeler improvements - calculated properties, security roles, pattern based validation, projection entity type, extension entity type Form Builder based on AngularJS and Bootstrap introduced OpenUI5 starter pack JWT token and OAuth2 flow Renovation Terminal replaced with xterm.js Monaco (VSCode Editor) set as default editor GraalVM engine introduced and set as default Debug View replaced with Chrome Dev Tools Git functionality re-architecture License License updated to EPL 2.0 REUSE compliant Conferences & Social Media What is Eclipse Dirigible? What is Low Code Development? at EclipseCon Pan-European Matchathon New videos in the channel Slack channel with 49 participants Derivative work Project \"XSK\" - compatible environment for SAP HANA Extended Application Services (XS) based applications Stay safe and healthy! See you all in 2021! \ud83e\udd73","title":"2020 - What a year for Eclipse Dirigible!"},{"location":"2020/12/16/2020-what-a-year-for-eclipse-dirigible/#2020-in-numbers","text":"12 releases from 4.2 to 5.6 with 1 major 5.0 138 issues fixed 11 blog posts 10 new API - Content, Template Engine, Execute, Lifecycle, SOAP, Websocket, Kafka Consumer, Kafka Producer, MongoDB Client, MongoDB DAO 11K+ users from 143 countries for 2020 412 repositories in DirigibleLabs till date","title":"2020 in numbers:"},{"location":"2020/12/16/2020-what-a-year-for-eclipse-dirigible/#notable-new-features","text":"Packages for SAP Cloud Platfom Neo , Cloud Foundry and Kyma OData v2 support based on Apache Olingo Entity Modeler improvements - calculated properties, security roles, pattern based validation, projection entity type, extension entity type Form Builder based on AngularJS and Bootstrap introduced OpenUI5 starter pack JWT token and OAuth2 flow","title":"Notable new features"},{"location":"2020/12/16/2020-what-a-year-for-eclipse-dirigible/#renovation","text":"Terminal replaced with xterm.js Monaco (VSCode Editor) set as default editor GraalVM engine introduced and set as default Debug View replaced with Chrome Dev Tools Git functionality re-architecture","title":"Renovation"},{"location":"2020/12/16/2020-what-a-year-for-eclipse-dirigible/#license","text":"License updated to EPL 2.0 REUSE compliant","title":"License"},{"location":"2020/12/16/2020-what-a-year-for-eclipse-dirigible/#conferences-social-media","text":"What is Eclipse Dirigible? What is Low Code Development? at EclipseCon Pan-European Matchathon New videos in the channel Slack channel with 49 participants","title":"Conferences &amp; Social Media"},{"location":"2020/12/16/2020-what-a-year-for-eclipse-dirigible/#derivative-work","text":"Project \"XSK\" - compatible environment for SAP HANA Extended Application Services (XS) based applications Stay safe and healthy! See you all in 2021! \ud83e\udd73","title":"Derivative work"},{"location":"2021/10/11/low-code-mobile-apps-with-dirigible-and-ns/","text":"WebViews Sometimes Work Nowadays, modern browsers allow web developers to access more and more native APIs and thus making them a platform good enough too meet more and more needs. Sometimes though you have a good reason to ask users to install your app on a device without native UI being necessary. And even more - what if you just reuse your web app code? Of course, I am talking about WebViews. WebView If you are happy with how your app looks in browsers, but you need to use platform APIs that are limited for PWAs (like Notifications , Bluetooth , Face ID , Offline Storage , etc.), a WebView application with JS-to-native messages would do the trick. And it's fairly simple to produce - create a single-view application with a WebView and implement some callbacks in both JavaScript and native code ( here is how you can do it in iOS). If it's simple, why don't you automate it? Step 1: Generate a WebView Mobile App from Dirigible First, let's create a WebView app for iOS from scratch. Create a new Xcode project, choose Single View Application and add a WKWebView to your one-and-only UIViewController. You need to add some boilerplate code to make your WKWebview open a URL. import SwiftUI import WebKit struct ContentView : View { var body : some View { Webview ( url : URL ( string : \"https://www.dirigible.io/\" ) ! ) } } struct Webview : UIViewRepresentable { let url : URL func makeUIView ( context : UIViewRepresentableContext < Webview > ) -> WKWebView { let config = WKWebViewConfiguration () let webview = WKWebView ( frame : CGRect . zero , configuration : config ) let request = URLRequest ( url : self . url ) webview . load ( request ) return webview } func updateUIView ( _ webview : WKWebView , context : UIViewRepresentableContext < Webview > ) { let request = URLRequest ( url : self . url , cachePolicy : . returnCacheDataElseLoad ) webview . load ( request ) } } Therefore, this works with the public URL of your deployed app. Now you can go ahead and publish your app in the App Store. If we change just the WKWebView URL, it should be really simple to automate it. Let's go through the steps that need to be automated: Create a Xcode project template to build the app from. In this project, we update the URL in the WebView configuration code to match the public index URL of our app. Replace the URL in the WebView configuration code with the public index URL of our app. Build and archive the iOS application. Send the archived application to a user via the Dirigible UI. Step 1. We already did that, but let's add a placeholder for the app URL, which we will be updating using a regex. For steps 2. and 3. I created a Node.js script that you can get from here and play with it. For Step 4. I created an endpoint in TransportProjectRestService.java : @GET @Path ( \"/project/{workspace}/{project}/ios\" ) @ApiOperation ( \"Generate ipa file\" ) @ApiResponses ({ @ApiResponse ( code = 200 , message = \"Project Exported\" ) }) public void exportProjectIos ( @Suspended AsyncResponse asyncResponse , @ApiParam ( value = \"Name of the Workspace\" , required = true ) @PathParam ( \"workspace\" ) String workspace , @ApiParam ( value = \"Name of the Project\" , required = true ) @PathParam ( \"project\" ) String project , @QueryParam ( \"previewUrl\" ) String previewUrl ) throws RepositoryExportException { String user = UserFacade . getName (); if ( user == null ) { asyncResponse . resume ( createErrorResponseForbidden ( NO_LOGGED_IN_USER )); } String appUrl = previewUrl + project + \"/index.html\" ; ProcessBuilder pb = new ProcessBuilder ( \"node\" , APP_GENERATOR_SCRIPT_PATH , \"generate\" , appUrl ); pb . inheritIO (); try { Process p = pb . start (); CompletableFuture onProcessExit = p . onExit (); onProcessExit . get (); onProcessExit . thenAccept ( ph -> { ByteArrayOutputStream baos = null ; baos = new ByteArrayOutputStream (); ZipOutputStream zipOut = new ZipOutputStream ( baos ); File fileToZip = new File ( GENERATED_APP_BUILD_PATH ); try { zipFile ( fileToZip , fileToZip . getName (), zipOut ); } catch ( IOException e ) { e . printStackTrace (); } try { zipOut . close (); } catch ( IOException e ) { e . printStackTrace (); } asyncResponse . resume ( Response . ok (). header ( \"Content-Disposition\" , \"attachment; filename=\\\"\" + project + \"-\" + \"build.zip\\\"\" ). entity ( baos . toByteArray ()). build ()); }); } catch ( IOException e ) { asyncResponse . resume ( Response . noContent (). build ()); } catch ( ExecutionException e ) { e . printStackTrace (); asyncResponse . resume ( Response . noContent (). build ()); } catch ( InterruptedException e ) { e . printStackTrace (); asyncResponse . resume ( Response . noContent (). build ()); } } It does the following: Gets the public URL of the Dirigible app from the request parameters. Calls the Node.js script with the URL as a parameter. Archives the contents of the build folder (archived iOS artifacts). Sends the .zip in the response. Now we need some front-end stuff. I went for the simplest way possible - added a new Export iOS app in the project right-button menu ( workspace.js ). The result: And when we load the app in the iOS Simulator: Step 2: Call Native APIs from the Dirigible App At the beginning of this post, I talked about messages between JS and native code, but this would require a bunch of code for handling different scenarios. Fortunately, there is a better way. NativeScript - Native Calls NativeScript's runtime allows native calls from JavaScript while keeping the exact same class, methods and property names as you are writing native code. This practically eliminates any need for learning bridge-specific APIs and if you want to do a native call, you can just refer to the corresponding docs. For example, this is how we initialize a UIViewController in Objective-C: UIViewController * vc = [ UIViewController alloc ] init ]; Using NativeScript it becomes: let vc = UIViewController . alloc (). init (); Since the NativeScript runtime works in a separate thread, we can't share context between it and our web app. That's why it provides worker -like interface. Keep in mind that this interface is still an experimental feature. For example, this is how you can get the model of the device from your Dirigible application: let worker = new NSWorker ( \"postmessage(UIDevice.currentDevice.localizedModel)\" ); onNativeMessage = function ( msg ) { console . log ( \"Message from native - \" + JSON . stringify ( msg )); $ ( '#model' ). text ( JSON . stringify ( msg . data )); } To make our Xcode project template project support this some changes are necessary - add the NativeScript framework and some other build settings in order to build and link the project properly. And this is the final result: Note What we reviewed in this article is a research topic rather than a fully implemented feature in Dirigible. The generation of mobile apps is certainly coming to Dirigible at some point, but there is a lot of work left to make it production-ready. That being said, any feedback, ideas and, of course, contribution will be appreciated. Link to the code sample on GitHub can be found here .","title":"Low-code mobile apps with Dirigible and NativeScript"},{"location":"2021/10/11/low-code-mobile-apps-with-dirigible-and-ns/#webviews-sometimes-work","text":"Nowadays, modern browsers allow web developers to access more and more native APIs and thus making them a platform good enough too meet more and more needs. Sometimes though you have a good reason to ask users to install your app on a device without native UI being necessary. And even more - what if you just reuse your web app code? Of course, I am talking about WebViews. WebView If you are happy with how your app looks in browsers, but you need to use platform APIs that are limited for PWAs (like Notifications , Bluetooth , Face ID , Offline Storage , etc.), a WebView application with JS-to-native messages would do the trick. And it's fairly simple to produce - create a single-view application with a WebView and implement some callbacks in both JavaScript and native code ( here is how you can do it in iOS). If it's simple, why don't you automate it?","title":"WebViews Sometimes Work"},{"location":"2021/10/11/low-code-mobile-apps-with-dirigible-and-ns/#step-1-generate-a-webview-mobile-app-from-dirigible","text":"First, let's create a WebView app for iOS from scratch. Create a new Xcode project, choose Single View Application and add a WKWebView to your one-and-only UIViewController. You need to add some boilerplate code to make your WKWebview open a URL. import SwiftUI import WebKit struct ContentView : View { var body : some View { Webview ( url : URL ( string : \"https://www.dirigible.io/\" ) ! ) } } struct Webview : UIViewRepresentable { let url : URL func makeUIView ( context : UIViewRepresentableContext < Webview > ) -> WKWebView { let config = WKWebViewConfiguration () let webview = WKWebView ( frame : CGRect . zero , configuration : config ) let request = URLRequest ( url : self . url ) webview . load ( request ) return webview } func updateUIView ( _ webview : WKWebView , context : UIViewRepresentableContext < Webview > ) { let request = URLRequest ( url : self . url , cachePolicy : . returnCacheDataElseLoad ) webview . load ( request ) } } Therefore, this works with the public URL of your deployed app. Now you can go ahead and publish your app in the App Store. If we change just the WKWebView URL, it should be really simple to automate it. Let's go through the steps that need to be automated: Create a Xcode project template to build the app from. In this project, we update the URL in the WebView configuration code to match the public index URL of our app. Replace the URL in the WebView configuration code with the public index URL of our app. Build and archive the iOS application. Send the archived application to a user via the Dirigible UI. Step 1. We already did that, but let's add a placeholder for the app URL, which we will be updating using a regex. For steps 2. and 3. I created a Node.js script that you can get from here and play with it. For Step 4. I created an endpoint in TransportProjectRestService.java : @GET @Path ( \"/project/{workspace}/{project}/ios\" ) @ApiOperation ( \"Generate ipa file\" ) @ApiResponses ({ @ApiResponse ( code = 200 , message = \"Project Exported\" ) }) public void exportProjectIos ( @Suspended AsyncResponse asyncResponse , @ApiParam ( value = \"Name of the Workspace\" , required = true ) @PathParam ( \"workspace\" ) String workspace , @ApiParam ( value = \"Name of the Project\" , required = true ) @PathParam ( \"project\" ) String project , @QueryParam ( \"previewUrl\" ) String previewUrl ) throws RepositoryExportException { String user = UserFacade . getName (); if ( user == null ) { asyncResponse . resume ( createErrorResponseForbidden ( NO_LOGGED_IN_USER )); } String appUrl = previewUrl + project + \"/index.html\" ; ProcessBuilder pb = new ProcessBuilder ( \"node\" , APP_GENERATOR_SCRIPT_PATH , \"generate\" , appUrl ); pb . inheritIO (); try { Process p = pb . start (); CompletableFuture onProcessExit = p . onExit (); onProcessExit . get (); onProcessExit . thenAccept ( ph -> { ByteArrayOutputStream baos = null ; baos = new ByteArrayOutputStream (); ZipOutputStream zipOut = new ZipOutputStream ( baos ); File fileToZip = new File ( GENERATED_APP_BUILD_PATH ); try { zipFile ( fileToZip , fileToZip . getName (), zipOut ); } catch ( IOException e ) { e . printStackTrace (); } try { zipOut . close (); } catch ( IOException e ) { e . printStackTrace (); } asyncResponse . resume ( Response . ok (). header ( \"Content-Disposition\" , \"attachment; filename=\\\"\" + project + \"-\" + \"build.zip\\\"\" ). entity ( baos . toByteArray ()). build ()); }); } catch ( IOException e ) { asyncResponse . resume ( Response . noContent (). build ()); } catch ( ExecutionException e ) { e . printStackTrace (); asyncResponse . resume ( Response . noContent (). build ()); } catch ( InterruptedException e ) { e . printStackTrace (); asyncResponse . resume ( Response . noContent (). build ()); } } It does the following: Gets the public URL of the Dirigible app from the request parameters. Calls the Node.js script with the URL as a parameter. Archives the contents of the build folder (archived iOS artifacts). Sends the .zip in the response. Now we need some front-end stuff. I went for the simplest way possible - added a new Export iOS app in the project right-button menu ( workspace.js ). The result: And when we load the app in the iOS Simulator:","title":"Step 1: Generate a WebView Mobile App from Dirigible"},{"location":"2021/10/11/low-code-mobile-apps-with-dirigible-and-ns/#step-2-call-native-apis-from-the-dirigible-app","text":"At the beginning of this post, I talked about messages between JS and native code, but this would require a bunch of code for handling different scenarios. Fortunately, there is a better way. NativeScript - Native Calls NativeScript's runtime allows native calls from JavaScript while keeping the exact same class, methods and property names as you are writing native code. This practically eliminates any need for learning bridge-specific APIs and if you want to do a native call, you can just refer to the corresponding docs. For example, this is how we initialize a UIViewController in Objective-C: UIViewController * vc = [ UIViewController alloc ] init ]; Using NativeScript it becomes: let vc = UIViewController . alloc (). init (); Since the NativeScript runtime works in a separate thread, we can't share context between it and our web app. That's why it provides worker -like interface. Keep in mind that this interface is still an experimental feature. For example, this is how you can get the model of the device from your Dirigible application: let worker = new NSWorker ( \"postmessage(UIDevice.currentDevice.localizedModel)\" ); onNativeMessage = function ( msg ) { console . log ( \"Message from native - \" + JSON . stringify ( msg )); $ ( '#model' ). text ( JSON . stringify ( msg . data )); } To make our Xcode project template project support this some changes are necessary - add the NativeScript framework and some other build settings in order to build and link the project properly. And this is the final result: Note What we reviewed in this article is a research topic rather than a fully implemented feature in Dirigible. The generation of mobile apps is certainly coming to Dirigible at some point, but there is a lot of work left to make it production-ready. That being said, any feedback, ideas and, of course, contribution will be appreciated. Link to the code sample on GitHub can be found here .","title":"Step 2: Call Native APIs from the Dirigible App"},{"location":"2021/10/14/kubernetes-aws-gardener-istio-letsencrypt-dirigible/","text":"Overview In this article we are going to setup custom domain for Dirigible application in Kubernes cluster with Gardener , AWS Route 53 , Istio , Let's encrypt . The target Kubernetes deployment is shown bellow: Kubernetes AWS Route 53 Gardener Istio Overview Kubernetes is an open source system for automating deployment, scaling, and management of containerized applications in a cluster environment. You can read more about Kubernetes here . Overview Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. here . Overview Deliver fully-managed clusters at scale everywhere with your own Kubernetes-as-a-Service. Kubernetes-native system managing the full lifecycle of conformant Kubernetes clusters as a service on Alicloud, AWS, Azure, GCP, OpenStack, EquinixMetal, vSphere, MetalStack, and Kubevirt with minimal TCO. here . Overview Istio is an open source service mesh that layers transparently onto existing distributed applications. Istio\u2019s powerful features provide a uniform and more efficient way to secure, connect, and monitor services. here . Prerequisites In this article we assume that you have already running productive Kubernetes Cluster on Gardener and configured kubectl for it. If you don't have such, you can create one by using the the open-source Gardener project. Also you will need AWS account or AWS Free Tier , you need to install Istio and Dirigible AWS Route 53 Configuration Create hosted zone - when you create hosted zone choose type Public hosted zone see the image below. After you create your hosted zone you can delegate your subdomain to AWS, if you don't host your parent domain in AWS. You can take the name servers which you can see in the image bellow and add ns records to your domain. But if you host your domain in AWS you don't need to delegate. Create new user - which will provide to Gardener dns provider. When you create user select credential type to be Access key - Programmatic access ( you see the image below). Create group - add user to group, but for this scenario we need to create new group which will be using only for this purpose. That's why, click on Create group and it will open new tab to create the group. Create Policy - before you create the new group click on Create policy it will open new tab to create the policy. On the first step click on the JSON see the image below: Leave this tab open and find your Hosted zone ID see the image below: Add your hosted zone id to the JSON you can use this example json { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : \"route53:ListResourceRecordSets\" , \"Resource\" : \"arn:aws:route53:::hostedzone/*\" }, { \"Sid\" : \"VisualEditor1\" , \"Effect\" : \"Allow\" , \"Action\" : \"route53:GetHostedZone\" , \"Resource\" : \"arn:aws:route53:::hostedzone/Z2XXXXXXXXXXXX\" }, { \"Sid\" : \"VisualEditor2\" , \"Effect\" : \"Allow\" , \"Action\" : \"route53:ListHostedZones\" , \"Resource\" : \"*\" }, { \"Sid\" : \"VisualEditor3\" , \"Effect\" : \"Allow\" , \"Action\" : \"route53:ChangeResourceRecordSets\" , \"Resource\" : \"arn:aws:route53:::hostedzone/Z2XXXXXXXXXXXX\" } ] } Note It's very important to add your correct hosted zone id We don't need tags, see image below: \u0422ype policy name, see image below: Add new policy to the group, see image below: Add new group to the user . We don't need tags: Note Before you click on create user check Permissions summary that consist your new group. Download your access key . Gardener Configuration Provide AWS Route 53 credentials - we need to provide our AWS Route 53 credentials from the previous step: Add gardener extensions to the Shoot cluster - configure dns and Let's Encrypt certificate. Open Gardener Shoot yaml file and add DNS providers - shoot-dns-service , shoot-cert-service : spec : dns : providers : - secretName : my-aws-route53-secret type : aws-route53 extensions : - type : shoot-dns-service providerConfig : apiVersion : service.dns.extensions.gardener.cloud/v1alpha1 kind : DNSConfig dnsProviderReplication : enabled : true - type : shoot-cert-service providerConfig : apiVersion : service.cert.extensions.gardener.cloud/v1alpha1 issuers : - email : <your-email-here> name : <type-name-for-the-issue> server : 'https://acme-v02.api.letsencrypt.org/directory' Create subdomain for your application - we can use DNSEntry for Gardener, because in Gardener Shoot yaml file we configured: dnsProviderReplication : enabled : true Find Istio ingress gateway external ip: kubectl get services istio-ingressgateway -n istio-system \\ --output jsonpath='{.status.loadBalancer.ingress[0].hostname}' apiVersion : dns.gardener.cloud/v1alpha1 kind : DNSEntry metadata : annotations : dns.gardener.cloud/class : garden name : dns-entry namespace : default spec : dnsName : \"app.demo.dirigible.io\" ttl : 600 targets : - <type-here-the-result-from-previous-command> Istio Configuration We need to configure our istio ingress gateway to accept our new sub domain app.demo.dirigible.io and the certificate. Apply the dns configuration and certificate - in this article we will configure istio ingress gateway to accept wildcard certificate. kubectl edit svc istio-ingressgateway -n istio-system # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion : v1 kind : Service metadata : annotations : cert.gardener.cloud/issuer : app.demo.dirigible.io cert.gardener.cloud/secretname : wildcard-tls dns.gardener.cloud/class : garden dns.gardener.cloud/dnsnames : '*.demo.dirigible.io' dns.gardener.cloud/ttl : \"120\" Gateway configuration - configure Gateway and VirtualService for Dirigible application or any. apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : name : dirigible spec : selector : istio : ingressgateway servers : - port : number : 80 name : http protocol : HTTP tls : httpsRedirect : true hosts : - \"app.demo.dirigible.io\" - port : number : 443 name : https protocol : HTTPS tls : mode : SIMPLE credentialName : wildcard-tls hosts : - \"app.demo.dirigible.io\" --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : dirigible spec : hosts : - \"app.demo.dirigible.io\" gateways : - dirigible http : - match : - uri : regex : /.* route : - destination : host : dirigible port : number : 8080","title":"Custom Domain in Kubernetes with AWS Route 53, Gardener, Istio & Let's encrypt"},{"location":"2021/10/14/kubernetes-aws-gardener-istio-letsencrypt-dirigible/#overview","text":"In this article we are going to setup custom domain for Dirigible application in Kubernes cluster with Gardener , AWS Route 53 , Istio , Let's encrypt . The target Kubernetes deployment is shown bellow: Kubernetes AWS Route 53 Gardener Istio Overview Kubernetes is an open source system for automating deployment, scaling, and management of containerized applications in a cluster environment. You can read more about Kubernetes here . Overview Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. here . Overview Deliver fully-managed clusters at scale everywhere with your own Kubernetes-as-a-Service. Kubernetes-native system managing the full lifecycle of conformant Kubernetes clusters as a service on Alicloud, AWS, Azure, GCP, OpenStack, EquinixMetal, vSphere, MetalStack, and Kubevirt with minimal TCO. here . Overview Istio is an open source service mesh that layers transparently onto existing distributed applications. Istio\u2019s powerful features provide a uniform and more efficient way to secure, connect, and monitor services. here .","title":"Overview"},{"location":"2021/10/14/kubernetes-aws-gardener-istio-letsencrypt-dirigible/#prerequisites","text":"In this article we assume that you have already running productive Kubernetes Cluster on Gardener and configured kubectl for it. If you don't have such, you can create one by using the the open-source Gardener project. Also you will need AWS account or AWS Free Tier , you need to install Istio and Dirigible","title":"Prerequisites"},{"location":"2021/10/14/kubernetes-aws-gardener-istio-letsencrypt-dirigible/#aws-route-53-configuration","text":"Create hosted zone - when you create hosted zone choose type Public hosted zone see the image below. After you create your hosted zone you can delegate your subdomain to AWS, if you don't host your parent domain in AWS. You can take the name servers which you can see in the image bellow and add ns records to your domain. But if you host your domain in AWS you don't need to delegate. Create new user - which will provide to Gardener dns provider. When you create user select credential type to be Access key - Programmatic access ( you see the image below). Create group - add user to group, but for this scenario we need to create new group which will be using only for this purpose. That's why, click on Create group and it will open new tab to create the group. Create Policy - before you create the new group click on Create policy it will open new tab to create the policy. On the first step click on the JSON see the image below: Leave this tab open and find your Hosted zone ID see the image below: Add your hosted zone id to the JSON you can use this example json { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : \"route53:ListResourceRecordSets\" , \"Resource\" : \"arn:aws:route53:::hostedzone/*\" }, { \"Sid\" : \"VisualEditor1\" , \"Effect\" : \"Allow\" , \"Action\" : \"route53:GetHostedZone\" , \"Resource\" : \"arn:aws:route53:::hostedzone/Z2XXXXXXXXXXXX\" }, { \"Sid\" : \"VisualEditor2\" , \"Effect\" : \"Allow\" , \"Action\" : \"route53:ListHostedZones\" , \"Resource\" : \"*\" }, { \"Sid\" : \"VisualEditor3\" , \"Effect\" : \"Allow\" , \"Action\" : \"route53:ChangeResourceRecordSets\" , \"Resource\" : \"arn:aws:route53:::hostedzone/Z2XXXXXXXXXXXX\" } ] } Note It's very important to add your correct hosted zone id We don't need tags, see image below: \u0422ype policy name, see image below: Add new policy to the group, see image below: Add new group to the user . We don't need tags: Note Before you click on create user check Permissions summary that consist your new group. Download your access key .","title":"AWS Route 53 Configuration"},{"location":"2021/10/14/kubernetes-aws-gardener-istio-letsencrypt-dirigible/#gardener-configuration","text":"Provide AWS Route 53 credentials - we need to provide our AWS Route 53 credentials from the previous step: Add gardener extensions to the Shoot cluster - configure dns and Let's Encrypt certificate. Open Gardener Shoot yaml file and add DNS providers - shoot-dns-service , shoot-cert-service : spec : dns : providers : - secretName : my-aws-route53-secret type : aws-route53 extensions : - type : shoot-dns-service providerConfig : apiVersion : service.dns.extensions.gardener.cloud/v1alpha1 kind : DNSConfig dnsProviderReplication : enabled : true - type : shoot-cert-service providerConfig : apiVersion : service.cert.extensions.gardener.cloud/v1alpha1 issuers : - email : <your-email-here> name : <type-name-for-the-issue> server : 'https://acme-v02.api.letsencrypt.org/directory' Create subdomain for your application - we can use DNSEntry for Gardener, because in Gardener Shoot yaml file we configured: dnsProviderReplication : enabled : true Find Istio ingress gateway external ip: kubectl get services istio-ingressgateway -n istio-system \\ --output jsonpath='{.status.loadBalancer.ingress[0].hostname}' apiVersion : dns.gardener.cloud/v1alpha1 kind : DNSEntry metadata : annotations : dns.gardener.cloud/class : garden name : dns-entry namespace : default spec : dnsName : \"app.demo.dirigible.io\" ttl : 600 targets : - <type-here-the-result-from-previous-command>","title":"Gardener Configuration"},{"location":"2021/10/14/kubernetes-aws-gardener-istio-letsencrypt-dirigible/#istio-configuration","text":"We need to configure our istio ingress gateway to accept our new sub domain app.demo.dirigible.io and the certificate. Apply the dns configuration and certificate - in this article we will configure istio ingress gateway to accept wildcard certificate. kubectl edit svc istio-ingressgateway -n istio-system # Please edit the object below. Lines beginning with a '#' will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion : v1 kind : Service metadata : annotations : cert.gardener.cloud/issuer : app.demo.dirigible.io cert.gardener.cloud/secretname : wildcard-tls dns.gardener.cloud/class : garden dns.gardener.cloud/dnsnames : '*.demo.dirigible.io' dns.gardener.cloud/ttl : \"120\" Gateway configuration - configure Gateway and VirtualService for Dirigible application or any. apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : name : dirigible spec : selector : istio : ingressgateway servers : - port : number : 80 name : http protocol : HTTP tls : httpsRedirect : true hosts : - \"app.demo.dirigible.io\" - port : number : 443 name : https protocol : HTTPS tls : mode : SIMPLE credentialName : wildcard-tls hosts : - \"app.demo.dirigible.io\" --- apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : dirigible spec : hosts : - \"app.demo.dirigible.io\" gateways : - dirigible http : - match : - uri : regex : /.* route : - destination : host : dirigible port : number : 8080","title":"Istio Configuration"},{"location":"2021/11/1/dirigible-runs-material/","text":"Intro Until recently, the Eclipse Dirigible website was powered by Jekyll . As part of a comprehensive update of the website's look-and-feel, we've switched entirely to Material for MkDocs . In addition, we added our favorite Dirigible gold into the website color pallette. We're quite excited by the result so we decided to sum everything up in this post. Read on for more details as well as a couple of before/after comparisons. Main Site Before Here's the old look of the Eclipse Dirigible website: After And here's how it looks now: Eclipse Dirigible Documentation Before The old documentation (i.e., help) page represented a long table of contents: After Currently, there's a landing page with a built-in search and tiles that give you hints when you hover over them: Dirigiballs Everywhere By now, you've probably noticed the habitual presence of meatball-looking creatures all over the site. Meet our out-of-this-world friends, the \"dirigiballs\". They have generously accepted our invitation to pose for a couple of photos for the site. We hope everyone has as much fun looking at them as we had during the shootout. If you take a look at the rest of the Dirigible sites, you'll notice that the dirigiballs have invaded our whole landscape: dirigible.io/blogs dirigible.io/news dirigible.io/releases dirigible.io/api dirigible.io/samples On a serious note, credit goes to unDraw for the great illustrations.","title":"Dirigible now runs Material for MkDocs"},{"location":"2021/11/1/dirigible-runs-material/#intro","text":"Until recently, the Eclipse Dirigible website was powered by Jekyll . As part of a comprehensive update of the website's look-and-feel, we've switched entirely to Material for MkDocs . In addition, we added our favorite Dirigible gold into the website color pallette. We're quite excited by the result so we decided to sum everything up in this post. Read on for more details as well as a couple of before/after comparisons.","title":"Intro"},{"location":"2021/11/1/dirigible-runs-material/#main-site","text":"","title":"Main Site"},{"location":"2021/11/1/dirigible-runs-material/#before","text":"Here's the old look of the Eclipse Dirigible website:","title":"Before"},{"location":"2021/11/1/dirigible-runs-material/#after","text":"And here's how it looks now:","title":"After"},{"location":"2021/11/1/dirigible-runs-material/#eclipse-dirigible-documentation","text":"","title":"Eclipse Dirigible Documentation"},{"location":"2021/11/1/dirigible-runs-material/#before_1","text":"The old documentation (i.e., help) page represented a long table of contents:","title":"Before"},{"location":"2021/11/1/dirigible-runs-material/#after_1","text":"Currently, there's a landing page with a built-in search and tiles that give you hints when you hover over them:","title":"After"},{"location":"2021/11/1/dirigible-runs-material/#dirigiballs-everywhere","text":"By now, you've probably noticed the habitual presence of meatball-looking creatures all over the site. Meet our out-of-this-world friends, the \"dirigiballs\". They have generously accepted our invitation to pose for a couple of photos for the site. We hope everyone has as much fun looking at them as we had during the shootout. If you take a look at the rest of the Dirigible sites, you'll notice that the dirigiballs have invaded our whole landscape: dirigible.io/blogs dirigible.io/news dirigible.io/releases dirigible.io/api dirigible.io/samples On a serious note, credit goes to unDraw for the great illustrations.","title":"Dirigiballs Everywhere"},{"location":"2021/11/2/material-blogging-capabilities/","text":"Overview In this blog, we're going to discuss how to add blogging capabilities to Material for MkDocs . Here's what we refer to as blogging capabilities: author's GitHub avatar, name, and link to GitHub profile a calendar icon with a publishing date next to it a clock icon with a value for time to read next to it You can also see the above mentioned capabilities under the title of this blog. In particular, we'll look at: What kind of metadata key-value pairs you need to define in your .md file and how. How to override a specific block in the main.html file that MkDocs uses as a template when building the HTML output from your .md source, so that the metadata values provided in your .md file are reflected in the HTML output. As a result, a blog author will just have to provide sufficient metadata in the frontmatter of their .md file and data about author, publishing date, and reading time will be displayed as part of their blog. Cool, right? This can also be done manually by including a piece of HTML code in each .md file but we wanted to have everything set up for blog authors in a way that they can focus on the writing itself. Adding Metadata to Your Blog Metadata about an .md file (also called frontmatter) is declared within a specific block in the beginning of the .md file itself and is denoted by the triple dashes at the start and end of the block. Usually, the metadata is not processed when generating HTML output from the .md file. With MkDocs, metadata can be displayed on the page or used to control the page rendering, but only if this is supported by the theme you're using with MkDocs. For more details, checkout the MkDocs documentation . We have to determine what metadata key-value pairs are needed for the blogging capabilities. As mentioned above, we want to have name, GitHub avatar, and link to GitHub profile of the author, as well as publishing date and reading time. Open your .md file and add the following lines: --- author : <Your Name> author_gh_user : <Your GitHub User> read_time : <Reading Time> publish_date : <Date of Publishing> --- It's quite self-explanatory which metadata key-value pairs are responsible for which of the blogging capabilities. You can find more details in step 5 of Overriding the Content Block . The more interesting one is author_gh_user . This value will be used for both getting the author's GitHub avatar and creating a hyperlink to their GitHub profile. Make sure the title is also set in the metadata. Setting the title in the metadata will help position the blogging capabilities in the right place - under the title and before the rest of your blog. --- title : <Your Blog Title> author : <Your Name> author_gh_user : <Your GitHub User> read_time : <Reading Time> publish_date : <Date of Publishing> --- When the title is set in the metadata, use Heading 2 level ( ## This is a heading 2 ) as the highest heading level in your blog. Otherwise, the first Heading 1 you use will overwrite the title from the frontmatter and cause formatting issues. Overriding the Content Block MkDocs supports theme extension out-of-the-box. Material for MkDocs leverages this feature and provides the possibility to override a partial (such as the default header or footer) or a template block. The process is described in detail in the Extending the theme section of the Material for MkDocs documentation. Moreover, Material for MkDocs provides a ready-made content template blog, among others. Overriding Blocks provides the full details about template blogs that are provided by the theme. Since we want to add the blogging capabilities above the content, but just under the title of our blog, we'll have to override the content block. Follow the steps below: Create an overrides directory on the same level where your mkdocs.yml resides. Open your mkdocs.yml and add a reference to the overrides directory using the custom_dir parameter: theme : name : material custom_dir : overrides In the overrides directory, create a new file main.html . Copy the original code of the content block from the source files of Material for MkDocs and paste it in your main.html . Here's the code you need: <!-- Content --> {% block content %} <!-- Edit button --> {% if page.edit_url %} < a href = \"{{ page.edit_url }}\" title = \"{{ lang.t('edit.link.title') }}\" class = \"md-content__button md-icon\" > {% include \".icons/material/pencil.svg\" %} </ a > {% endif %} <!-- Hack: check whether the content contains a h1 headline. If it doesn't, the page title (or respectively site name) is used as the main headline. --> {% if not \"\\x3ch1\" in page.content %} < h1 > {{ page.title | d(config.site_name, true)}} </ h1 > {% endif %} <!-- Markdown content --> {{ page.content }} <!-- Last update of source file --> {% if page and page.meta %} {% if page.meta.git_revision_date_localized or page.meta.revision_date %} {% include \"partials/source-file.html\" %} {% endif %} {% endif %} {% endblock %} Although we're overriding a template block in main.html , the actual code resides in the base.html file that main.html extends. Open base.html in your browser and scroll down to the content block. Under Markdown content , add the custom HTML code we need for the blogging capabilities: <!-- Markdown content --> {% if page and page.meta and page.meta.author_gh_user %} < aside class = \"mdx-author\" > < p > < img src = \"https://avatars.githubusercontent.com/{{ page.meta.author_gh_user }}\" alt = \"@{{ page.meta.author_gh_user }}\" > </ p > < p > < span > < b > {{ page.meta.author }} </ b > \u00b7 < a href = \"https://github.com/{{ page.meta.author_gh_user }}\" > @{{ page.meta.author_gh_user }} </ a > </ span > < span > < svg xmlns = \"http://www.w3.org/2000/svg\" width = \"16\" height = \"16\" fill = \"currentColor\" class = \"bi bi-calendar2\" viewBox = \"0 0 16 16\" > < path d = \"M3.5 0a.5.5 0 0 1 .5.5V1h8V.5a.5.5 0 0 1 1 0V1h1a2 2 0 0 1 2 2v11a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V3a2 2 0 0 1 2-2h1V.5a.5.5 0 0 1 .5-.5zM2 2a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h12a1 1 0 0 0 1-1V3a1 1 0 0 0-1-1H2z\" /> < path d = \"M2.5 4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5H3a.5.5 0 0 1-.5-.5V4z\" /> </ svg > {{ page.meta.publish_date }} \u00b7 < svg xmlns = \"http://www.w3.org/2000/svg\" width = \"16\" height = \"16\" fill = \"currentColor\" class = \"bi bi-clock\" viewBox = \"0 0 16 16\" > < path d = \"M8 3.5a.5.5 0 0 0-1 0V9a.5.5 0 0 0 .252.434l3.5 2a.5.5 0 0 0 .496-.868L8 8.71V3.5z\" /> < path d = \"M8 16A8 8 0 1 0 8 0a8 8 0 0 0 0 16zm7-8A7 7 0 1 1 1 8a7 7 0 0 1 14 0z\" /> </ svg > {{ page.meta.read_time }} read </ span > </ p > </ aside > {% endif %} {{ page.content }} Kudos to squidfunk for providing the raw code of his own implementation as part of this discussion . By following the steps above, you've already overriden and extended the content block with information about the blog author, publishing date, and reading time. Let's have a closer look at some of the settings: {% if page and page.meta and page.meta.author_gh_user %} ensures that the blogging capabilities will be shown on the page only if the metadata key author_gh_user is defined in the .md file metadata. https://avatars.githubusercontent.com/{{ page.meta.author_gh_user }} is the URL from which the author's avatar is extracted. It uses the author_gh_user value provided in the .md metadata to get the avatar from the author's GitHub profile. {{ page.meta.author }} uses the author value provided in the .md metadata. <a href=\"https://github.com/{{ page.meta.author_gh_user }}\">@{{ page.meta.author_gh_user }}</a> uses the author_gh_user value provided in the .md metadata to create a hyperlink to the author's GitHub profile. the <svg>...</svg> elements provide a calendar and a clock Bootstrap Icons , while {{ page.meta.publish_date }} and {{ page.meta.read_time }} use the publish_date and read_time values, respectively, from the .md metadata. Create a custom stylesheet file ( custom.css ) in the docs/stylesheets directory and add a reference to it in the mkdocs.yml : extra_css : - stylesheets/custom.css Any additional stylesheet files should be placed in a stylesheets directory within your docs folder. For more details, you can refer to Additional CSS . Add the following styles to custom.css : /* Styles for blog-like features - author avatar & name, posting date, min to read, etc. */ . md-typeset . mdx-author img { border-radius : 100 % ; height : 2 rem ; } . md-typeset . mdx-author { display : flex ; font-size : .68 rem ; } . md-typeset . mdx-author p > span { display : block ; } p { display : block ; margin-block-start : 1 em ; margin-block-end : 1 em ; margin-inline-start : 0 px ; margin-inline-end : 0 px ; } . md-typeset . mdx-author p : first-child { flex-shrink : 0 ; margin-right : .8 rem ; } This will provide the desired styling of our blogging capabilities. You can always play with the styles to make them more appealing. That's it. You have enabled blogging capabilities for your Material for MkDocs website. Go ahead and try it out!","title":"Enable Blogging Capabilities with Material for MkDocs"},{"location":"2021/11/2/material-blogging-capabilities/#overview","text":"In this blog, we're going to discuss how to add blogging capabilities to Material for MkDocs . Here's what we refer to as blogging capabilities: author's GitHub avatar, name, and link to GitHub profile a calendar icon with a publishing date next to it a clock icon with a value for time to read next to it You can also see the above mentioned capabilities under the title of this blog. In particular, we'll look at: What kind of metadata key-value pairs you need to define in your .md file and how. How to override a specific block in the main.html file that MkDocs uses as a template when building the HTML output from your .md source, so that the metadata values provided in your .md file are reflected in the HTML output. As a result, a blog author will just have to provide sufficient metadata in the frontmatter of their .md file and data about author, publishing date, and reading time will be displayed as part of their blog. Cool, right? This can also be done manually by including a piece of HTML code in each .md file but we wanted to have everything set up for blog authors in a way that they can focus on the writing itself.","title":"Overview"},{"location":"2021/11/2/material-blogging-capabilities/#adding-metadata-to-your-blog","text":"Metadata about an .md file (also called frontmatter) is declared within a specific block in the beginning of the .md file itself and is denoted by the triple dashes at the start and end of the block. Usually, the metadata is not processed when generating HTML output from the .md file. With MkDocs, metadata can be displayed on the page or used to control the page rendering, but only if this is supported by the theme you're using with MkDocs. For more details, checkout the MkDocs documentation . We have to determine what metadata key-value pairs are needed for the blogging capabilities. As mentioned above, we want to have name, GitHub avatar, and link to GitHub profile of the author, as well as publishing date and reading time. Open your .md file and add the following lines: --- author : <Your Name> author_gh_user : <Your GitHub User> read_time : <Reading Time> publish_date : <Date of Publishing> --- It's quite self-explanatory which metadata key-value pairs are responsible for which of the blogging capabilities. You can find more details in step 5 of Overriding the Content Block . The more interesting one is author_gh_user . This value will be used for both getting the author's GitHub avatar and creating a hyperlink to their GitHub profile. Make sure the title is also set in the metadata. Setting the title in the metadata will help position the blogging capabilities in the right place - under the title and before the rest of your blog. --- title : <Your Blog Title> author : <Your Name> author_gh_user : <Your GitHub User> read_time : <Reading Time> publish_date : <Date of Publishing> --- When the title is set in the metadata, use Heading 2 level ( ## This is a heading 2 ) as the highest heading level in your blog. Otherwise, the first Heading 1 you use will overwrite the title from the frontmatter and cause formatting issues.","title":"Adding Metadata to Your Blog"},{"location":"2021/11/2/material-blogging-capabilities/#overriding-the-content-block","text":"MkDocs supports theme extension out-of-the-box. Material for MkDocs leverages this feature and provides the possibility to override a partial (such as the default header or footer) or a template block. The process is described in detail in the Extending the theme section of the Material for MkDocs documentation. Moreover, Material for MkDocs provides a ready-made content template blog, among others. Overriding Blocks provides the full details about template blogs that are provided by the theme. Since we want to add the blogging capabilities above the content, but just under the title of our blog, we'll have to override the content block. Follow the steps below: Create an overrides directory on the same level where your mkdocs.yml resides. Open your mkdocs.yml and add a reference to the overrides directory using the custom_dir parameter: theme : name : material custom_dir : overrides In the overrides directory, create a new file main.html . Copy the original code of the content block from the source files of Material for MkDocs and paste it in your main.html . Here's the code you need: <!-- Content --> {% block content %} <!-- Edit button --> {% if page.edit_url %} < a href = \"{{ page.edit_url }}\" title = \"{{ lang.t('edit.link.title') }}\" class = \"md-content__button md-icon\" > {% include \".icons/material/pencil.svg\" %} </ a > {% endif %} <!-- Hack: check whether the content contains a h1 headline. If it doesn't, the page title (or respectively site name) is used as the main headline. --> {% if not \"\\x3ch1\" in page.content %} < h1 > {{ page.title | d(config.site_name, true)}} </ h1 > {% endif %} <!-- Markdown content --> {{ page.content }} <!-- Last update of source file --> {% if page and page.meta %} {% if page.meta.git_revision_date_localized or page.meta.revision_date %} {% include \"partials/source-file.html\" %} {% endif %} {% endif %} {% endblock %} Although we're overriding a template block in main.html , the actual code resides in the base.html file that main.html extends. Open base.html in your browser and scroll down to the content block. Under Markdown content , add the custom HTML code we need for the blogging capabilities: <!-- Markdown content --> {% if page and page.meta and page.meta.author_gh_user %} < aside class = \"mdx-author\" > < p > < img src = \"https://avatars.githubusercontent.com/{{ page.meta.author_gh_user }}\" alt = \"@{{ page.meta.author_gh_user }}\" > </ p > < p > < span > < b > {{ page.meta.author }} </ b > \u00b7 < a href = \"https://github.com/{{ page.meta.author_gh_user }}\" > @{{ page.meta.author_gh_user }} </ a > </ span > < span > < svg xmlns = \"http://www.w3.org/2000/svg\" width = \"16\" height = \"16\" fill = \"currentColor\" class = \"bi bi-calendar2\" viewBox = \"0 0 16 16\" > < path d = \"M3.5 0a.5.5 0 0 1 .5.5V1h8V.5a.5.5 0 0 1 1 0V1h1a2 2 0 0 1 2 2v11a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V3a2 2 0 0 1 2-2h1V.5a.5.5 0 0 1 .5-.5zM2 2a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h12a1 1 0 0 0 1-1V3a1 1 0 0 0-1-1H2z\" /> < path d = \"M2.5 4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5H3a.5.5 0 0 1-.5-.5V4z\" /> </ svg > {{ page.meta.publish_date }} \u00b7 < svg xmlns = \"http://www.w3.org/2000/svg\" width = \"16\" height = \"16\" fill = \"currentColor\" class = \"bi bi-clock\" viewBox = \"0 0 16 16\" > < path d = \"M8 3.5a.5.5 0 0 0-1 0V9a.5.5 0 0 0 .252.434l3.5 2a.5.5 0 0 0 .496-.868L8 8.71V3.5z\" /> < path d = \"M8 16A8 8 0 1 0 8 0a8 8 0 0 0 0 16zm7-8A7 7 0 1 1 1 8a7 7 0 0 1 14 0z\" /> </ svg > {{ page.meta.read_time }} read </ span > </ p > </ aside > {% endif %} {{ page.content }} Kudos to squidfunk for providing the raw code of his own implementation as part of this discussion . By following the steps above, you've already overriden and extended the content block with information about the blog author, publishing date, and reading time. Let's have a closer look at some of the settings: {% if page and page.meta and page.meta.author_gh_user %} ensures that the blogging capabilities will be shown on the page only if the metadata key author_gh_user is defined in the .md file metadata. https://avatars.githubusercontent.com/{{ page.meta.author_gh_user }} is the URL from which the author's avatar is extracted. It uses the author_gh_user value provided in the .md metadata to get the avatar from the author's GitHub profile. {{ page.meta.author }} uses the author value provided in the .md metadata. <a href=\"https://github.com/{{ page.meta.author_gh_user }}\">@{{ page.meta.author_gh_user }}</a> uses the author_gh_user value provided in the .md metadata to create a hyperlink to the author's GitHub profile. the <svg>...</svg> elements provide a calendar and a clock Bootstrap Icons , while {{ page.meta.publish_date }} and {{ page.meta.read_time }} use the publish_date and read_time values, respectively, from the .md metadata. Create a custom stylesheet file ( custom.css ) in the docs/stylesheets directory and add a reference to it in the mkdocs.yml : extra_css : - stylesheets/custom.css Any additional stylesheet files should be placed in a stylesheets directory within your docs folder. For more details, you can refer to Additional CSS . Add the following styles to custom.css : /* Styles for blog-like features - author avatar & name, posting date, min to read, etc. */ . md-typeset . mdx-author img { border-radius : 100 % ; height : 2 rem ; } . md-typeset . mdx-author { display : flex ; font-size : .68 rem ; } . md-typeset . mdx-author p > span { display : block ; } p { display : block ; margin-block-start : 1 em ; margin-block-end : 1 em ; margin-inline-start : 0 px ; margin-inline-end : 0 px ; } . md-typeset . mdx-author p : first-child { flex-shrink : 0 ; margin-right : .8 rem ; } This will provide the desired styling of our blogging capabilities. You can always play with the styles to make them more appealing. That's it. You have enabled blogging capabilities for your Material for MkDocs website. Go ahead and try it out!","title":"Overriding the Content Block"},{"location":"2022/01/14/dirigible-vscode-theia-custom-editors/","text":"Recently, I was tasked with porting an extension from Eclipse Dirigible to VS Code and Eclipse Theia. During that process, I found some interesting things about how those products work and their design choices, which inspired this short blog post. I will try to explain the main differences between them from the point of view of an extension developer. This will not be a tutorial on how to make extensions, instead it will be more focused on overall design differences between the IDEs (Integrated Development Environment). What are they? First, let me give you a brief introduction to what each of those products are and what they are best suited for. Eclipse Dirigible This is a cloud development environment, featuring a low-code platform for rapid development of business applications, while also providing a runtime. It is best suited for people who need a cloud IDE (Integrated Development Environment), which focuses on JavaScript development with the ability to connect to databases such as SAP HANA and PostgreSQL. Visual Studio Code (VS Code) This one is quite popular and does not need much of an introduction. It is a desktop-focused code editor, which can also run in the cloud. It is best suited for people who need a general-purpose IDE with a clean and simple UI (User Interface). Eclipse Theia Eclipse Theia is a cloud and desktop IDE framework. Its primary aim is to be a modular, vendor-agnostic version of VS Code. It is not a fork or a one-to-one copy of VS Code but it is API (Application Programming Interface) compatible with it and shares a lot of its components. While it can be used as an IDE, it is an IDE framework which means that it is used as a base for creating other IDEs. It is best suited for people who need to develop their own custom IDE, without reinventing the wheel. One extension to rule them all \u201cExtension\u201d is the official name used by VS Code. In Dirigible, the same thing is referred to as a \u201cmodule\u201d. The equivalent in Theia is called \u201cplug-in\u201d. All three platforms are built using web technologies such as HTML, CSS and JavaScript, so my initial reaction was that I could take the Dirigible module and make it run in the other two by making some minor changes. I tested that hypothesis, and while it can be achieved by making some sacrifices, I would not recommend it. Here is a screenshot of my attempt at running an unmodified Dirigible module inside a VS Code extension serving as a container: It does run but the first and most obvious problem is that the user interface does not react to the theme selected in VS Code. This makes the extension feel very out of place. Another problem I encountered is that input fields created by Dirigible\u2019s UI framework disable the save action in VS Code, when focused. This is probably caused by the input field model in AngularJS, but I did not spend too much time trying to figure it out. After seeing those problems, I decided that the best way to go about this is to have two separate extensions. One for Dirigible and one for the other two. Theia and other editors based on Theia are, as I said, API compatible with VS Code, so from an extension point of view they are the same. This reduces the maintenance burden significantly. Something to note here is that Theia has something called \u201cextensions\u201d, but they are not the same thing as the extensions found in VS Code and I will not be talking about them as they serve a different use case. As mentioned earlier, in Theia, VS Code extensions are referred to as \u201cplug-ins\u201d. Dirigible view modules and theming Every view module in Dirigible has a few basic components. Each view is contained in its own iframe window. For the view itself, Dirigible provides its own version of Bootstrap as a CSS framework combined with AngularJS as a front-end framework. This means that Dirigible provides ready-to-use widgets and a developer can quickly create a view module. Those components, however, are not mandatory. If developers want to, they can use their own framework and make the UI however they like. Everything is permitted as long as it is contained inside the iframe of the view. Theming in Dirigible is done by having each theme implement its own colors, fonts and widget sizes, in its own CSS file, packaged inside a theme module. The only rule is that CSS classes and IDs must have the same names as the default theme . When a view requests the IDE stylesheet, the back end responds with the CSS file of the currently selected theme. This means that a view is never aware of what theme is being used . Currently, this is being reworked as we move away from Bootstrap and adopt the \u201cfundamental-styles\u201d open-source project initiated by SAP. Modules can communicate with other modules by making use of something called MessageHub. It is an abstraction layer for the \u2018postMessage\u2019 publisher-subscriber system found in every major browser. When communicating with the back end , modules use the Dirigible RESTful API. View modules are stateful , meaning that if they are created and active, they will not lose their state even if the user moves their focus to a different view in another tab and they will not have to be redrawn when focus is returned to them. Developing an extension in Dirigible In Dirigible, modules are dynamically loaded. You just create a normal project, then create a project.json file which points to the folder containing the module and Dirigible will detect and load it based on its content. No special development host is needed. When you make a change, all you must do is refresh the view. This allows for rapid development but depending on what view you are working on, it could also mean that it can stop Dirigible from rendering its UI when there is a significant problem with the view. If you want to see examples of Dirigible views and more detailed information, you can take a look at the following GitHub repository: https://github.com/dirigiblelabs/sample-ide-perspective . Custom editors in Dirigible When you create a custom editor in Dirigible, your communication with the back end must be kept to a minimum . After all, Dirigible is a cloud IDE and frequent requests can lead to a bad user experience. When a user opens a file, the editor receives the contents of the file from the back end and all editing and unsaved changes are done and saved on the client machine. When editing is done and the user saves the file, its contents are sent to the back end, where the old file will be replaced. Dirigible does not provide a universal API to create a custom editor. At least not yet. This means that functionalities such as undo and redo have to be implemented by the custom editor itself. Theia/VS Code extensions and theming In VS Code, and therefore Theia, extensions are done in a very different way . In fact, it\u2019s almost the exact opposite of Dirigible. Most extensions do not create their own UI. Instead, they use the VS Code API which creates the UI for them, which limits where the extension can be shown and what the UI can do. Most extensions are small add-ons to the status, activity, or side bar. If your extension is more advanced, like a custom file editor, or you need more visual space and a custom UI, your only option is to use something called WebViews . This splits the extension into two parts. Front end and back end. Like in Dirigible, WebViews are iframes but have some strict content policies applied to them. At the time of writing, there is no UI framework provided by VS Code that you can use, so it is only plain HTML and CSS. The only thing you will have as a helping hand are the default VS Code colors in the form of CSS variables, which will be injected into your WebView. Technically speaking, you can use Angular or React but as I said, WebViews have some strict content policies, so they do not fully work out-of-the-box . This makes WebView extensions inconsistent and hard to implement, especially if you want your extensions to look at home inside the IDE. There has been some recent work on developing an official UI Toolkit but at the time of writing, it is just a public preview. The back-end part of the extension communicates with the front end using the WebView API which looks and feels very similar to MessageHub. WebViews are also stateless . This means that once a user focuses on another editor and the WebView is no longer visible, its UI is destroyed. When a user focuses back, the UI is created again. This happens even if there are unsaved changes. To not lose those changes, you can use the WebView state API which stores a JavaScript object describing the last known state of the WebView. There is an option which you can enable, that will make your WebView stateful like in Dirigible, but it is not recommended as there are some performance penalties and should be used only if there is no other option. Finally, WebViews are aware of themes. The colors from the current theme are injected into the WebView and your body element will have a special CSS class assigned to it, which will tell you if the theme is dark or light. Most of the time, your UI will work without problems regardless of themes, but in some small edge cases, you may need to implement special rules based on the theme selected. Developing an extension in Theia/VS Code Extensions in VS Code and Theia are not loaded dynamically , at least not in the same way they are in Dirigible. When developing an extension, before you could test it, you must compile it first, then start something called an \u201cExtension Development Host\u201d. It is a special instance of the IDE that runs your extension. Every time you make a change, you must recompile and restart the development host . It does not take a lot of time, but it is something that slows you down. On the plus side, if there is something very wrong with your extension, it will not affect the non-development host. Custom editors in Theia/VS Code In the case of a custom editor, when a user opens a file, the back end sends the file contents to the front end and every change, no matter small or big, must be send back. Once the file is saved, it is again sent to the front end, which is redrawn. This means that unlike Dirigible, actual editing and all unsaved changes take place in the back end back-end front end just displays its current state . VS Code also provides a custom editor API, simplifying the process. Functionalities such as save, undo, and redo are handled by the IDE. That being said, the developer (or developers) can create their own editor API from scratch if they need to. Summary To summarize everything, VS Code and Theia have a different design from Dirigible. VS Code can be a bit more intimidating in the beginning and has a steep learning curve but once you get the hang of it, it is quite easy to develop extensions for it. The biggest problem it has is that it does not have a proper UI framework. Dirigible is easier to start with, loads modules dynamically and has a proper UI framework, but lacks more advanced APIs and you may end up having to write your own functionality for certain things. As I said in the beginning of this post, you can make Dirigible modules run inside VS Code and Theia, but you will be making some sacrifices. The maintenance burden will be bigger, and you may encounter some unexpected bugs. If you need to make an extension which supports all three, make one for Dirigible and one for the other two. It will take more time when starting out, but it will prove to be the better choice in the long run. The main question, of course, is do you really need to support all three? In fact, VS Code and Theia target efficient coding experience, while Dirigible is focused on RAD/LCNC tooling, so depending on your scenario you can choose one of these complementary frameworks as a basis for your tooling.","title":"Eclipse Dirigible vs Visual Studio Code vs Eclipse Theia - Custom Editors"},{"location":"2022/01/14/dirigible-vscode-theia-custom-editors/#what-are-they","text":"First, let me give you a brief introduction to what each of those products are and what they are best suited for.","title":"What are they?"},{"location":"2022/01/14/dirigible-vscode-theia-custom-editors/#eclipse-dirigible","text":"This is a cloud development environment, featuring a low-code platform for rapid development of business applications, while also providing a runtime. It is best suited for people who need a cloud IDE (Integrated Development Environment), which focuses on JavaScript development with the ability to connect to databases such as SAP HANA and PostgreSQL.","title":"Eclipse Dirigible"},{"location":"2022/01/14/dirigible-vscode-theia-custom-editors/#visual-studio-code-vs-code","text":"This one is quite popular and does not need much of an introduction. It is a desktop-focused code editor, which can also run in the cloud. It is best suited for people who need a general-purpose IDE with a clean and simple UI (User Interface).","title":"Visual Studio Code (VS Code)"},{"location":"2022/01/14/dirigible-vscode-theia-custom-editors/#eclipse-theia","text":"Eclipse Theia is a cloud and desktop IDE framework. Its primary aim is to be a modular, vendor-agnostic version of VS Code. It is not a fork or a one-to-one copy of VS Code but it is API (Application Programming Interface) compatible with it and shares a lot of its components. While it can be used as an IDE, it is an IDE framework which means that it is used as a base for creating other IDEs. It is best suited for people who need to develop their own custom IDE, without reinventing the wheel.","title":"Eclipse Theia"},{"location":"2022/01/14/dirigible-vscode-theia-custom-editors/#one-extension-to-rule-them-all","text":"\u201cExtension\u201d is the official name used by VS Code. In Dirigible, the same thing is referred to as a \u201cmodule\u201d. The equivalent in Theia is called \u201cplug-in\u201d. All three platforms are built using web technologies such as HTML, CSS and JavaScript, so my initial reaction was that I could take the Dirigible module and make it run in the other two by making some minor changes. I tested that hypothesis, and while it can be achieved by making some sacrifices, I would not recommend it. Here is a screenshot of my attempt at running an unmodified Dirigible module inside a VS Code extension serving as a container: It does run but the first and most obvious problem is that the user interface does not react to the theme selected in VS Code. This makes the extension feel very out of place. Another problem I encountered is that input fields created by Dirigible\u2019s UI framework disable the save action in VS Code, when focused. This is probably caused by the input field model in AngularJS, but I did not spend too much time trying to figure it out. After seeing those problems, I decided that the best way to go about this is to have two separate extensions. One for Dirigible and one for the other two. Theia and other editors based on Theia are, as I said, API compatible with VS Code, so from an extension point of view they are the same. This reduces the maintenance burden significantly. Something to note here is that Theia has something called \u201cextensions\u201d, but they are not the same thing as the extensions found in VS Code and I will not be talking about them as they serve a different use case. As mentioned earlier, in Theia, VS Code extensions are referred to as \u201cplug-ins\u201d.","title":"One extension to rule them all"},{"location":"2022/01/14/dirigible-vscode-theia-custom-editors/#dirigible-view-modules-and-theming","text":"Every view module in Dirigible has a few basic components. Each view is contained in its own iframe window. For the view itself, Dirigible provides its own version of Bootstrap as a CSS framework combined with AngularJS as a front-end framework. This means that Dirigible provides ready-to-use widgets and a developer can quickly create a view module. Those components, however, are not mandatory. If developers want to, they can use their own framework and make the UI however they like. Everything is permitted as long as it is contained inside the iframe of the view. Theming in Dirigible is done by having each theme implement its own colors, fonts and widget sizes, in its own CSS file, packaged inside a theme module. The only rule is that CSS classes and IDs must have the same names as the default theme . When a view requests the IDE stylesheet, the back end responds with the CSS file of the currently selected theme. This means that a view is never aware of what theme is being used . Currently, this is being reworked as we move away from Bootstrap and adopt the \u201cfundamental-styles\u201d open-source project initiated by SAP. Modules can communicate with other modules by making use of something called MessageHub. It is an abstraction layer for the \u2018postMessage\u2019 publisher-subscriber system found in every major browser. When communicating with the back end , modules use the Dirigible RESTful API. View modules are stateful , meaning that if they are created and active, they will not lose their state even if the user moves their focus to a different view in another tab and they will not have to be redrawn when focus is returned to them.","title":"Dirigible view modules and theming"},{"location":"2022/01/14/dirigible-vscode-theia-custom-editors/#developing-an-extension-in-dirigible","text":"In Dirigible, modules are dynamically loaded. You just create a normal project, then create a project.json file which points to the folder containing the module and Dirigible will detect and load it based on its content. No special development host is needed. When you make a change, all you must do is refresh the view. This allows for rapid development but depending on what view you are working on, it could also mean that it can stop Dirigible from rendering its UI when there is a significant problem with the view. If you want to see examples of Dirigible views and more detailed information, you can take a look at the following GitHub repository: https://github.com/dirigiblelabs/sample-ide-perspective .","title":"Developing an extension in Dirigible"},{"location":"2022/01/14/dirigible-vscode-theia-custom-editors/#custom-editors-in-dirigible","text":"When you create a custom editor in Dirigible, your communication with the back end must be kept to a minimum . After all, Dirigible is a cloud IDE and frequent requests can lead to a bad user experience. When a user opens a file, the editor receives the contents of the file from the back end and all editing and unsaved changes are done and saved on the client machine. When editing is done and the user saves the file, its contents are sent to the back end, where the old file will be replaced. Dirigible does not provide a universal API to create a custom editor. At least not yet. This means that functionalities such as undo and redo have to be implemented by the custom editor itself.","title":"Custom editors in Dirigible"},{"location":"2022/01/14/dirigible-vscode-theia-custom-editors/#theiavs-code-extensions-and-theming","text":"In VS Code, and therefore Theia, extensions are done in a very different way . In fact, it\u2019s almost the exact opposite of Dirigible. Most extensions do not create their own UI. Instead, they use the VS Code API which creates the UI for them, which limits where the extension can be shown and what the UI can do. Most extensions are small add-ons to the status, activity, or side bar. If your extension is more advanced, like a custom file editor, or you need more visual space and a custom UI, your only option is to use something called WebViews . This splits the extension into two parts. Front end and back end. Like in Dirigible, WebViews are iframes but have some strict content policies applied to them. At the time of writing, there is no UI framework provided by VS Code that you can use, so it is only plain HTML and CSS. The only thing you will have as a helping hand are the default VS Code colors in the form of CSS variables, which will be injected into your WebView. Technically speaking, you can use Angular or React but as I said, WebViews have some strict content policies, so they do not fully work out-of-the-box . This makes WebView extensions inconsistent and hard to implement, especially if you want your extensions to look at home inside the IDE. There has been some recent work on developing an official UI Toolkit but at the time of writing, it is just a public preview. The back-end part of the extension communicates with the front end using the WebView API which looks and feels very similar to MessageHub. WebViews are also stateless . This means that once a user focuses on another editor and the WebView is no longer visible, its UI is destroyed. When a user focuses back, the UI is created again. This happens even if there are unsaved changes. To not lose those changes, you can use the WebView state API which stores a JavaScript object describing the last known state of the WebView. There is an option which you can enable, that will make your WebView stateful like in Dirigible, but it is not recommended as there are some performance penalties and should be used only if there is no other option. Finally, WebViews are aware of themes. The colors from the current theme are injected into the WebView and your body element will have a special CSS class assigned to it, which will tell you if the theme is dark or light. Most of the time, your UI will work without problems regardless of themes, but in some small edge cases, you may need to implement special rules based on the theme selected.","title":"Theia/VS Code extensions and theming"},{"location":"2022/01/14/dirigible-vscode-theia-custom-editors/#developing-an-extension-in-theiavs-code","text":"Extensions in VS Code and Theia are not loaded dynamically , at least not in the same way they are in Dirigible. When developing an extension, before you could test it, you must compile it first, then start something called an \u201cExtension Development Host\u201d. It is a special instance of the IDE that runs your extension. Every time you make a change, you must recompile and restart the development host . It does not take a lot of time, but it is something that slows you down. On the plus side, if there is something very wrong with your extension, it will not affect the non-development host.","title":"Developing an extension in Theia/VS Code"},{"location":"2022/01/14/dirigible-vscode-theia-custom-editors/#custom-editors-in-theiavs-code","text":"In the case of a custom editor, when a user opens a file, the back end sends the file contents to the front end and every change, no matter small or big, must be send back. Once the file is saved, it is again sent to the front end, which is redrawn. This means that unlike Dirigible, actual editing and all unsaved changes take place in the back end back-end front end just displays its current state . VS Code also provides a custom editor API, simplifying the process. Functionalities such as save, undo, and redo are handled by the IDE. That being said, the developer (or developers) can create their own editor API from scratch if they need to.","title":"Custom editors in Theia/VS Code"},{"location":"2022/01/14/dirigible-vscode-theia-custom-editors/#summary","text":"To summarize everything, VS Code and Theia have a different design from Dirigible. VS Code can be a bit more intimidating in the beginning and has a steep learning curve but once you get the hang of it, it is quite easy to develop extensions for it. The biggest problem it has is that it does not have a proper UI framework. Dirigible is easier to start with, loads modules dynamically and has a proper UI framework, but lacks more advanced APIs and you may end up having to write your own functionality for certain things. As I said in the beginning of this post, you can make Dirigible modules run inside VS Code and Theia, but you will be making some sacrifices. The maintenance burden will be bigger, and you may encounter some unexpected bugs. If you need to make an extension which supports all three, make one for Dirigible and one for the other two. It will take more time when starting out, but it will prove to be the better choice in the long run. The main question, of course, is do you really need to support all three? In fact, VS Code and Theia target efficient coding experience, while Dirigible is focused on RAD/LCNC tooling, so depending on your scenario you can choose one of these complementary frameworks as a basis for your tooling.","title":"Summary"},{"location":"2022/08/10/gcp-gke-dns-istio-letsencrypt-postgresql-keycloak/","text":"Overview In this article we are going to setup a custom domain for a Dirigible application in a GKE cluster with GCP Cloud DNS , GCP Cloud SQL Postgre , Istio , Let's Encrypt , Keycloak . Components: Kubernetes GCP Cloud DNS GCP Cloud SQL Postgre Istio Let's Encrypt Cert-manager Keycloak Overview Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications in a cluster environment. You can read more about Kubernetes at www.kubernetes.io . Overview Reliable, resilient, low-latency DNS serving from Google's worldwide network provides you with everything you need to register, manage, and serve your domains. For more information, see Cloud DNS . Overview Fully managed relational database service for MySQL, PostgreSQL, and SQL Server with rich extension collections, configuration flags, and developer ecosystems. For more information, see GCP Cloud SQL Postgreere . Overview Istio is an open-source service mesh that layers transparently onto existing distributed applications. Istio\u2019s powerful features provide a uniform and more efficient way to secure, connect, and monitor services. For more information, see Istio . Overview Istio is an open-source service mesh that layers transparently onto existing distributed applications. Istio\u2019s powerful features provide a uniform and more efficient way to secure, connect, and monitor services. For more information, see Let's Encrypt . Overview Cert-manager is a powerful and extensible X.509 certificate controller for Kubernetes and OpenShift workloads. It will obtain certificates from a variety of Issuers, both popular public Issuers as well as private Issuers, and ensure the certificates are valid and up-to-date, and will attempt to renew certificates at a configured time before expiry. For more information, see Cert-manager . Overview Keycloak is an open source Identity and Access Management system for applications and services.For more information, see Keycloak . Prerequisites In this article, we assume that you already have a GCP account and added billing account. GKE Cluster Configuration Prerequisites Install the gcloud CLI Install kubectl and configure cluster access Install Helm Create project Go to create your project . Set name, organization and billing account. Enable Engine API To be able to create a cluster, we need to enable Kubernetes Api. Go to enable Kubernetes API Create GKE cluster You can create standard and an autopilot cluster. In ths article, we will create the standard cluster. At this time you have two options to create a cluster: manually or by Use a setup guide . Manually Use a setup guide In this article we will use a setup guide and cost-optimized cluster . Choose a suitable location and name your cluster. Set a release channel. We are going to set Regular channel . This version have passed internal validation and are considered production-quality. Choose a cluster size. In this article we are going to keep the default size. Verify the machine type. Advanced settings: In this article we will use Optimize utilization . Prioritize optimizing utilization over keeping spare resources in the cluster. When selected, the cluster autoscaler scales down the cluster more aggressively: it can remove more nodes, and remove nodes faster. For cluster autoscaler you can configure how many maximum nodes to scale. It depends on your requirements. For this article we are going to keep this configuration maximum nodes 3 . Vertical Pod Autoscaling will ensure that the pods will be deployed on the right node. Configure usage metering. In this article we are not using metering. Enable Workload Identity We need a workload identity to allow our Dirigible pod to access PostgreSQL. Cluster Workload Identity Node Workload Identity Istio Configuration Note In this article we will configure istioctl to use the configmaps from the 1-14-3 revision. We can run multiple versions of Istio concurrently and can specify exactly which revision gets applied in the tooling. Initialize or reinitialize gcloud - check this url for more information. gcloud init Enable the specific GKE cluster as the default cluster to be used for the remaining commands. Note You need to replace dirigible with your cluster name and europe-north1-a with your region . gcloud container clusters get-credentials dirigible \\ --region europe-north1-a Create istio-system namespace and add label istio-injection. kubectl create namespace istio-system kubectl label namespace istio-system istio-injection=enabled --overwrite Create Istio control plane service istiod. kubectl apply -f - <<EOF apiVersion : v1 kind : Service metadata : labels : app : istiod istio : pilot release : istio name : istiod namespace : istio-system spec : type : ClusterIP ports : - name : grpc-xds port : 15010 - name : https-dns port : 15012 - name : https-webhook port : 443 targetPort : 15017 - name : http-monitoring port : 15014 selector : app : istiod EOF Install minimal control plane. istioctl install -y -n istio-system --revision 1-14-3 -f - <<EOF apiVersion : install.istio.io/v1alpha1 kind : IstioOperator metadata : name : control-plane spec : profile : minimal values : gateways : istio-ingressgateway : autoscaleEnabled : true components : pilot : k8s : env : - name : PILOT_FILTER_GATEWAY_CLUSTER_CONFIG value : \"true\" meshConfig : defaultConfig : proxyMetadata : ISTIO_META_DNS_CAPTURE : \"true\" enablePrometheusMerge : true EOF Enable the istio-ingressgateway component. Install the istio-ingress gateway in a namespace that is different from istiod and add the istio-injection tag. kubectl create namespace istio-ingress kubectl label namespace istio-ingress istio-injection=enabled --overwrite Install it with a revision that matches the control plane in the istio-system namespace. istioctl install -y -n istio-ingress --revision 1-14-3 -f - <<EOF apiVersion : install.istio.io/v1alpha1 kind : IstioOperator metadata : name : istio-ingress-gw-install spec : profile : empty values : gateways : istio-ingressgateway : autoscaleEnabled : true components : ingressGateways : - name : istio-ingressgateway namespace : istio-ingress enabled : true k8s : overlays : - apiVersion : apps/v1 kind : Deployment name : istio-ingressgateway patches : - path : spec.template.spec.containers[name:istio-proxy].lifecycle value : preStop : exec : command : [ \"sh\" , \"-c\" , \"sleep 5\" ] EOF Apply Strict mTLS Encrypt the traffic between services in the mesh with mutual TLS. kubectl apply --namespace istio-system -f - <<EOF apiVersion : security.istio.io/v1beta1 kind : PeerAuthentication metadata : name : default spec : mtls : mode : STRICT EOF GCP Cloud DNS Configuration Enable Cloud DNS API. Go to page and choose Enable to enable the API. Create Cloud DNS Zone. Go to page Copy the generated DNS name servers for your zone and add them to your domain, if your control is not in Google Clod DNS. The generated DNS name servers should look like this: ns-cloud-e1.googledomains.com. ns-cloud-e2.googledomains.com. ns-cloud-e3.googledomains.com. ns-cloud-e4.googledomains.com. Create an A record for Dirigible and Keycloak. We need Istio Gateway IP . kubectl get svc -n istio-ingress istio-ingressgateway -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\" A static external IP address is the IP address that is reserved for your project until you decide to release it. You need to create the IP address in your region. gcloud compute addresses create demo --addresses=<YOUR-GATEWAY-IP> \\ --region=europe-north1 Set the IP for Dirigible. Set the IP for Keycloak. Let's Encrypt Configuration Install Cert-manager. Check for last version Add cert-manager helm repo. helm repo add jetstack https://charts.jetstack.io helm repo update Install Cert-Manager. helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.9.1 \\ --set installCRDs=true Create ClusterIssuer. Note You need to replace <YOUR-EMAIL-ADDRESS> with your valid email address. kubectl apply -f - <<EOF apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt namespace : cert-manager spec : acme : server : https://acme-v02.api.letsencrypt.org/directory email : <YOUR-EMAIL-ADDRESS> privateKeySecretRef : name : letsencrypt solvers : - selector : {} http01 : ingress : class : istio EOF GCP Cloud SQL Postgre Configuration Enable Cloud SQL API and create an instance. Go to page Enable Cloud SQL Admin API. Go to page Choose PostgreSQL database engine. Note For this article we will create separate instances for Dirigible and Keycloak. You can follow these steps for Dirigible and Keycloak. Create an instance. Set instance info, production. Set region, machine type, storage. Set connections. We need to Enable Service Networking API Set automatically allocated IP range. Set data protection and maintenance. After you create the instance update the configuration for connections to allow only SSL connection. Set Up a Dirigible Database Note Create the same way a database and a user for Keycloak. Create a database Go to your PostgreSQL instance and create the database. Create a user Go to your PostgreSQL instance and create the user. Create a service account for Dirigible and Keycloak. Add a role for the service account. Create a Dirigible and Keycloak Kubernetes Service Account Create the namespace dirigible-demo . kubectl create namespace dirigible-demo Add an Istio injection. kubectl label namespace dirigible-demo istio-injection=enabled --overwrite Configure a Kubernetes service account binding to the Google Cloud service account using Workload Identity. kubectl create sa dirigible-sa -n dirigible-demo kubectl create sa keycloak-sa -n dirigible-demo Add a new binding between your gcp service account and kubernetes service account Note You need to replace dirigible-gke-demo with your project id . Dirigible gcloud iam service-accounts add-iam-policy-binding \\ --role=\"roles/iam.workloadIdentityUser\" \\ --member=\"serviceAccount:dirigible-gke-demo.svc.id.goog[dirigible-demo/dirigible-sa]\" \\ dirigible-gcp-sa@dirigible-gke-demo.iam.gserviceaccount.com Keycloak gcloud iam service-accounts add-iam-policy-binding \\ --role=\"roles/iam.workloadIdentityUser\" \\ --member=\"serviceAccount:dirigible-gke-demo.svc.id.goog[dirigible-demo/keycloak-sa]\" \\ keycloak-gcp-sa@dirigible-gke-demo.iam.gserviceaccount.com Annotate the Kubernetes Service Account with the new binding. Note You need to replace dirigible-gke-demo with your project id . Dirigible kubectl annotate serviceaccount -n dirigible-demo \\ dirigible-sa \\ iam.gke.io/gcp-service-account=dirigible-gcp-sa@dirigible-gke-demo.iam.gserviceaccount.com Keycloak kubectl annotate serviceaccount -n dirigible-demo \\ keycloak-sa \\ iam.gke.io/gcp-service-account=keycloak-gcp-sa@dirigible-gke-demo.iam.gserviceaccount.com Create secrets for Kubernetes service accounts. Dirigible kubectl create secret generic dirigible-db -n dirigible-demo \\ --from-literal=username=dirigible_user \\ --from-literal=password=<your-password> \\ --from-literal=database=dirigible \\ --from-literal=postgre_url=jdbc:postgresql://127.0.0.1:5432/dirigible Keycloak kubectl create secret generic keycloak-db -n dirigible-demo \\ --from-literal=username=keycloak_user \\ --from-literal=password=<your-password> \\ --from-literal=database=keycloak \\ --from-literal=postgre_url=jdbc:postgresql://127.0.0.1:5432/keycloak Dirigible Deployment When you run this Dirigible helm chart with these sets, it will create volume , enable https , install Keycloak , create Istio gateway and virtualservice , enable usages for GCP Cloud SQL . You don't need to create a service account for Dirigible annd Keycloak, because it was already created in the previous steps. You need to provide gke.projectId , gke.region , ingress.host . helm repo add dirigible https://eclipse.github.io/dirigible helm repo update Note You need to replace: dirigible-gke-demo with your-project-id . europe-north1 with your-region . demo.apps.dirigible.io with your domain . dirigible-demo with your namespace . helm upgrade --install dirigible dirigible/dirigible -n dirigible-demo \\ --set volume.enabled=true \\ --set serviceAccount.create=false \\ --set keycloak.serviceAccountCreate=false \\ --set ingress.tls=true \\ --set keycloak.enabled=true \\ --set keycloak.install=true \\ --set istio.enabled=true \\ --set istio.enableHttps=true \\ --set gke.cloudSQL=true \\ --set gke.projectId=dirigible-gke-demo \\ --set gke.region=europe-north1 \\ --set ingress.host=demo.apps.dirigible.io \\ --set dirigible.image=dirigiblelabs/dirigible-keycloak:latest Check the Logs on the Cert-Manager Pod Wait for 3-5 minutes and check the logs. kubectl logs -n cert-manager -lapp=cert-manager | grep -i \"READY\" You should see something telling you that the certificate is ready and you can redirect http traffic to https. I0809 13:58:22.750528 1 conditions.go:190] Found status change for Certificate \"keycloak\" condition \"Ready\": \"False\" -> \"True\"; setting lastTransitionTime to 2022-08-09 13:58:22.750516762 +0000 UTC m=+18939.377721330 I0809 13:58:22.781247 1 conditions.go:190] Found status change for Certificate \"keycloak\" condition \"Ready\": \"False\" -> \"True\"; setting lastTransitionTime to 2022-08-09 13:58:22.781230149 +0000 UTC m=+18939.408434691 I0809 13:58:23.193897 1 conditions.go:250] Found status change for CertificateRequest \"dirigible-spnjm\" condition \"Ready\": \"False\" -> \"True\"; setting lastTransitionTime to 2022-08-09 13:58:23.193882791 +0000 UTC m=+18939.821087330 I0809 13:58:23.347574 1 conditions.go:190] Found status change for Certificate \"dirigible\" condition \"Ready\": \"False\" -> \"True\"; setting lastTransitionTime to 2022-08-09 13:58:23.347561322 +0000 UTC m=+18939.974765874 Now you can set httpsRedirect: true to redirect HTTP traffic to HTTPS . --set istio.httpsRedirect=true Keycloak Configuration Note You need to replace demo.apps.dirigible.io with your domain. When you first open https://dirigible.demo.apps.dirigible.io , you will see . We need to create clientId dirigible that's why go to https://keycloak.demo.apps.dirigible.io/auth/admin/master/console/#/realms/master/clients and login with username admin and password admin . Add Role \u2013 Open a new client and add the new roles Developer, Operator, Everyone. Add Default Roles \u2013 Roles->Default Roles add all roles from the previous step. Add User \u2013 By default, the new user should have the default roles assigned to it. Go to page to create the new user. Add a password. Set a valid redirect URL. Finally access Dirigible at https://dirigible.demo.apps.dirigible.io and log in with the password that we used for our user password. We can see that we have database connection to Cloud SQL PostgreSQL and we have assigned a certificate.","title":"Custom Domain in Google Kubernetes Engine with GCP Cloud DNS, Cloud SQL, Istio, Lets Encrypt, PostgreSQL and Keycloak"},{"location":"2022/08/10/gcp-gke-dns-istio-letsencrypt-postgresql-keycloak/#overview","text":"In this article we are going to setup a custom domain for a Dirigible application in a GKE cluster with GCP Cloud DNS , GCP Cloud SQL Postgre , Istio , Let's Encrypt , Keycloak . Components: Kubernetes GCP Cloud DNS GCP Cloud SQL Postgre Istio Let's Encrypt Cert-manager Keycloak Overview Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications in a cluster environment. You can read more about Kubernetes at www.kubernetes.io . Overview Reliable, resilient, low-latency DNS serving from Google's worldwide network provides you with everything you need to register, manage, and serve your domains. For more information, see Cloud DNS . Overview Fully managed relational database service for MySQL, PostgreSQL, and SQL Server with rich extension collections, configuration flags, and developer ecosystems. For more information, see GCP Cloud SQL Postgreere . Overview Istio is an open-source service mesh that layers transparently onto existing distributed applications. Istio\u2019s powerful features provide a uniform and more efficient way to secure, connect, and monitor services. For more information, see Istio . Overview Istio is an open-source service mesh that layers transparently onto existing distributed applications. Istio\u2019s powerful features provide a uniform and more efficient way to secure, connect, and monitor services. For more information, see Let's Encrypt . Overview Cert-manager is a powerful and extensible X.509 certificate controller for Kubernetes and OpenShift workloads. It will obtain certificates from a variety of Issuers, both popular public Issuers as well as private Issuers, and ensure the certificates are valid and up-to-date, and will attempt to renew certificates at a configured time before expiry. For more information, see Cert-manager . Overview Keycloak is an open source Identity and Access Management system for applications and services.For more information, see Keycloak .","title":"Overview"},{"location":"2022/08/10/gcp-gke-dns-istio-letsencrypt-postgresql-keycloak/#prerequisites","text":"In this article, we assume that you already have a GCP account and added billing account.","title":"Prerequisites"},{"location":"2022/08/10/gcp-gke-dns-istio-letsencrypt-postgresql-keycloak/#gke-cluster-configuration","text":"Prerequisites Install the gcloud CLI Install kubectl and configure cluster access Install Helm Create project Go to create your project . Set name, organization and billing account. Enable Engine API To be able to create a cluster, we need to enable Kubernetes Api. Go to enable Kubernetes API Create GKE cluster You can create standard and an autopilot cluster. In ths article, we will create the standard cluster. At this time you have two options to create a cluster: manually or by Use a setup guide . Manually Use a setup guide In this article we will use a setup guide and cost-optimized cluster . Choose a suitable location and name your cluster. Set a release channel. We are going to set Regular channel . This version have passed internal validation and are considered production-quality. Choose a cluster size. In this article we are going to keep the default size. Verify the machine type. Advanced settings: In this article we will use Optimize utilization . Prioritize optimizing utilization over keeping spare resources in the cluster. When selected, the cluster autoscaler scales down the cluster more aggressively: it can remove more nodes, and remove nodes faster. For cluster autoscaler you can configure how many maximum nodes to scale. It depends on your requirements. For this article we are going to keep this configuration maximum nodes 3 . Vertical Pod Autoscaling will ensure that the pods will be deployed on the right node. Configure usage metering. In this article we are not using metering.","title":"GKE Cluster Configuration"},{"location":"2022/08/10/gcp-gke-dns-istio-letsencrypt-postgresql-keycloak/#enable-workload-identity","text":"We need a workload identity to allow our Dirigible pod to access PostgreSQL. Cluster Workload Identity Node Workload Identity","title":"Enable Workload Identity"},{"location":"2022/08/10/gcp-gke-dns-istio-letsencrypt-postgresql-keycloak/#istio-configuration","text":"Note In this article we will configure istioctl to use the configmaps from the 1-14-3 revision. We can run multiple versions of Istio concurrently and can specify exactly which revision gets applied in the tooling. Initialize or reinitialize gcloud - check this url for more information. gcloud init Enable the specific GKE cluster as the default cluster to be used for the remaining commands. Note You need to replace dirigible with your cluster name and europe-north1-a with your region . gcloud container clusters get-credentials dirigible \\ --region europe-north1-a Create istio-system namespace and add label istio-injection. kubectl create namespace istio-system kubectl label namespace istio-system istio-injection=enabled --overwrite Create Istio control plane service istiod. kubectl apply -f - <<EOF apiVersion : v1 kind : Service metadata : labels : app : istiod istio : pilot release : istio name : istiod namespace : istio-system spec : type : ClusterIP ports : - name : grpc-xds port : 15010 - name : https-dns port : 15012 - name : https-webhook port : 443 targetPort : 15017 - name : http-monitoring port : 15014 selector : app : istiod EOF Install minimal control plane. istioctl install -y -n istio-system --revision 1-14-3 -f - <<EOF apiVersion : install.istio.io/v1alpha1 kind : IstioOperator metadata : name : control-plane spec : profile : minimal values : gateways : istio-ingressgateway : autoscaleEnabled : true components : pilot : k8s : env : - name : PILOT_FILTER_GATEWAY_CLUSTER_CONFIG value : \"true\" meshConfig : defaultConfig : proxyMetadata : ISTIO_META_DNS_CAPTURE : \"true\" enablePrometheusMerge : true EOF Enable the istio-ingressgateway component. Install the istio-ingress gateway in a namespace that is different from istiod and add the istio-injection tag. kubectl create namespace istio-ingress kubectl label namespace istio-ingress istio-injection=enabled --overwrite Install it with a revision that matches the control plane in the istio-system namespace. istioctl install -y -n istio-ingress --revision 1-14-3 -f - <<EOF apiVersion : install.istio.io/v1alpha1 kind : IstioOperator metadata : name : istio-ingress-gw-install spec : profile : empty values : gateways : istio-ingressgateway : autoscaleEnabled : true components : ingressGateways : - name : istio-ingressgateway namespace : istio-ingress enabled : true k8s : overlays : - apiVersion : apps/v1 kind : Deployment name : istio-ingressgateway patches : - path : spec.template.spec.containers[name:istio-proxy].lifecycle value : preStop : exec : command : [ \"sh\" , \"-c\" , \"sleep 5\" ] EOF Apply Strict mTLS Encrypt the traffic between services in the mesh with mutual TLS. kubectl apply --namespace istio-system -f - <<EOF apiVersion : security.istio.io/v1beta1 kind : PeerAuthentication metadata : name : default spec : mtls : mode : STRICT EOF","title":"Istio Configuration"},{"location":"2022/08/10/gcp-gke-dns-istio-letsencrypt-postgresql-keycloak/#gcp-cloud-dns-configuration","text":"Enable Cloud DNS API. Go to page and choose Enable to enable the API. Create Cloud DNS Zone. Go to page Copy the generated DNS name servers for your zone and add them to your domain, if your control is not in Google Clod DNS. The generated DNS name servers should look like this: ns-cloud-e1.googledomains.com. ns-cloud-e2.googledomains.com. ns-cloud-e3.googledomains.com. ns-cloud-e4.googledomains.com. Create an A record for Dirigible and Keycloak. We need Istio Gateway IP . kubectl get svc -n istio-ingress istio-ingressgateway -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\" A static external IP address is the IP address that is reserved for your project until you decide to release it. You need to create the IP address in your region. gcloud compute addresses create demo --addresses=<YOUR-GATEWAY-IP> \\ --region=europe-north1 Set the IP for Dirigible. Set the IP for Keycloak.","title":"GCP Cloud DNS Configuration"},{"location":"2022/08/10/gcp-gke-dns-istio-letsencrypt-postgresql-keycloak/#lets-encrypt-configuration","text":"Install Cert-manager. Check for last version Add cert-manager helm repo. helm repo add jetstack https://charts.jetstack.io helm repo update Install Cert-Manager. helm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.9.1 \\ --set installCRDs=true Create ClusterIssuer. Note You need to replace <YOUR-EMAIL-ADDRESS> with your valid email address. kubectl apply -f - <<EOF apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt namespace : cert-manager spec : acme : server : https://acme-v02.api.letsencrypt.org/directory email : <YOUR-EMAIL-ADDRESS> privateKeySecretRef : name : letsencrypt solvers : - selector : {} http01 : ingress : class : istio EOF","title":"Let's Encrypt Configuration"},{"location":"2022/08/10/gcp-gke-dns-istio-letsencrypt-postgresql-keycloak/#gcp-cloud-sql-postgre-configuration","text":"Enable Cloud SQL API and create an instance. Go to page Enable Cloud SQL Admin API. Go to page Choose PostgreSQL database engine. Note For this article we will create separate instances for Dirigible and Keycloak. You can follow these steps for Dirigible and Keycloak. Create an instance. Set instance info, production. Set region, machine type, storage. Set connections. We need to Enable Service Networking API Set automatically allocated IP range. Set data protection and maintenance. After you create the instance update the configuration for connections to allow only SSL connection.","title":"GCP Cloud SQL Postgre Configuration"},{"location":"2022/08/10/gcp-gke-dns-istio-letsencrypt-postgresql-keycloak/#set-up-a-dirigible-database","text":"Note Create the same way a database and a user for Keycloak. Create a database Go to your PostgreSQL instance and create the database. Create a user Go to your PostgreSQL instance and create the user. Create a service account for Dirigible and Keycloak. Add a role for the service account.","title":"Set Up a Dirigible Database"},{"location":"2022/08/10/gcp-gke-dns-istio-letsencrypt-postgresql-keycloak/#create-a-dirigible-and-keycloak-kubernetes-service-account","text":"Create the namespace dirigible-demo . kubectl create namespace dirigible-demo Add an Istio injection. kubectl label namespace dirigible-demo istio-injection=enabled --overwrite Configure a Kubernetes service account binding to the Google Cloud service account using Workload Identity. kubectl create sa dirigible-sa -n dirigible-demo kubectl create sa keycloak-sa -n dirigible-demo Add a new binding between your gcp service account and kubernetes service account Note You need to replace dirigible-gke-demo with your project id . Dirigible gcloud iam service-accounts add-iam-policy-binding \\ --role=\"roles/iam.workloadIdentityUser\" \\ --member=\"serviceAccount:dirigible-gke-demo.svc.id.goog[dirigible-demo/dirigible-sa]\" \\ dirigible-gcp-sa@dirigible-gke-demo.iam.gserviceaccount.com Keycloak gcloud iam service-accounts add-iam-policy-binding \\ --role=\"roles/iam.workloadIdentityUser\" \\ --member=\"serviceAccount:dirigible-gke-demo.svc.id.goog[dirigible-demo/keycloak-sa]\" \\ keycloak-gcp-sa@dirigible-gke-demo.iam.gserviceaccount.com Annotate the Kubernetes Service Account with the new binding. Note You need to replace dirigible-gke-demo with your project id . Dirigible kubectl annotate serviceaccount -n dirigible-demo \\ dirigible-sa \\ iam.gke.io/gcp-service-account=dirigible-gcp-sa@dirigible-gke-demo.iam.gserviceaccount.com Keycloak kubectl annotate serviceaccount -n dirigible-demo \\ keycloak-sa \\ iam.gke.io/gcp-service-account=keycloak-gcp-sa@dirigible-gke-demo.iam.gserviceaccount.com Create secrets for Kubernetes service accounts. Dirigible kubectl create secret generic dirigible-db -n dirigible-demo \\ --from-literal=username=dirigible_user \\ --from-literal=password=<your-password> \\ --from-literal=database=dirigible \\ --from-literal=postgre_url=jdbc:postgresql://127.0.0.1:5432/dirigible Keycloak kubectl create secret generic keycloak-db -n dirigible-demo \\ --from-literal=username=keycloak_user \\ --from-literal=password=<your-password> \\ --from-literal=database=keycloak \\ --from-literal=postgre_url=jdbc:postgresql://127.0.0.1:5432/keycloak","title":"Create a Dirigible and Keycloak Kubernetes Service Account"},{"location":"2022/08/10/gcp-gke-dns-istio-letsencrypt-postgresql-keycloak/#dirigible-deployment","text":"When you run this Dirigible helm chart with these sets, it will create volume , enable https , install Keycloak , create Istio gateway and virtualservice , enable usages for GCP Cloud SQL . You don't need to create a service account for Dirigible annd Keycloak, because it was already created in the previous steps. You need to provide gke.projectId , gke.region , ingress.host . helm repo add dirigible https://eclipse.github.io/dirigible helm repo update Note You need to replace: dirigible-gke-demo with your-project-id . europe-north1 with your-region . demo.apps.dirigible.io with your domain . dirigible-demo with your namespace . helm upgrade --install dirigible dirigible/dirigible -n dirigible-demo \\ --set volume.enabled=true \\ --set serviceAccount.create=false \\ --set keycloak.serviceAccountCreate=false \\ --set ingress.tls=true \\ --set keycloak.enabled=true \\ --set keycloak.install=true \\ --set istio.enabled=true \\ --set istio.enableHttps=true \\ --set gke.cloudSQL=true \\ --set gke.projectId=dirigible-gke-demo \\ --set gke.region=europe-north1 \\ --set ingress.host=demo.apps.dirigible.io \\ --set dirigible.image=dirigiblelabs/dirigible-keycloak:latest","title":"Dirigible Deployment"},{"location":"2022/08/10/gcp-gke-dns-istio-letsencrypt-postgresql-keycloak/#check-the-logs-on-the-cert-manager-pod","text":"Wait for 3-5 minutes and check the logs. kubectl logs -n cert-manager -lapp=cert-manager | grep -i \"READY\" You should see something telling you that the certificate is ready and you can redirect http traffic to https. I0809 13:58:22.750528 1 conditions.go:190] Found status change for Certificate \"keycloak\" condition \"Ready\": \"False\" -> \"True\"; setting lastTransitionTime to 2022-08-09 13:58:22.750516762 +0000 UTC m=+18939.377721330 I0809 13:58:22.781247 1 conditions.go:190] Found status change for Certificate \"keycloak\" condition \"Ready\": \"False\" -> \"True\"; setting lastTransitionTime to 2022-08-09 13:58:22.781230149 +0000 UTC m=+18939.408434691 I0809 13:58:23.193897 1 conditions.go:250] Found status change for CertificateRequest \"dirigible-spnjm\" condition \"Ready\": \"False\" -> \"True\"; setting lastTransitionTime to 2022-08-09 13:58:23.193882791 +0000 UTC m=+18939.821087330 I0809 13:58:23.347574 1 conditions.go:190] Found status change for Certificate \"dirigible\" condition \"Ready\": \"False\" -> \"True\"; setting lastTransitionTime to 2022-08-09 13:58:23.347561322 +0000 UTC m=+18939.974765874 Now you can set httpsRedirect: true to redirect HTTP traffic to HTTPS . --set istio.httpsRedirect=true","title":"Check the Logs on the Cert-Manager Pod"},{"location":"2022/08/10/gcp-gke-dns-istio-letsencrypt-postgresql-keycloak/#keycloak-configuration","text":"Note You need to replace demo.apps.dirigible.io with your domain. When you first open https://dirigible.demo.apps.dirigible.io , you will see . We need to create clientId dirigible that's why go to https://keycloak.demo.apps.dirigible.io/auth/admin/master/console/#/realms/master/clients and login with username admin and password admin . Add Role \u2013 Open a new client and add the new roles Developer, Operator, Everyone. Add Default Roles \u2013 Roles->Default Roles add all roles from the previous step. Add User \u2013 By default, the new user should have the default roles assigned to it. Go to page to create the new user. Add a password. Set a valid redirect URL. Finally access Dirigible at https://dirigible.demo.apps.dirigible.io and log in with the password that we used for our user password. We can see that we have database connection to Cloud SQL PostgreSQL and we have assigned a certificate.","title":"Keycloak Configuration"},{"location":"2022/09/12/sendgrid-smtp-relay-setup/","text":"Recent changes in Gmail 's policies allows Eclipse Dirigible users to send emails only via Workspace enabled accounts in G Suite and require workspace administration privileges. By connecting Eclipse Dirigible with Twilio SendGrid's SMTP Relay feature you can send emails with your personal gmail account. Note SendGrid is a 3rd party service and charges/limits apply. Here you can find more information about their Email API Plans . Setup SendGrid Account Create a SendGrid account at https://sendgrid.com . Login at https://app.sendgrid.com . Verify a single sender email: Click Settings \u2192 Sender Authentication \u2192 Verify a Single Sender . Enter the details of the email address that Eclipse Dirigible mails will be sent from. Setup SMTP Relay: Click Email API \u2192 Integration Guide \u2192 SMTP Relay . Enter an API Key Name and click Create Key to get an API Key for your SendGrid SMTP Relay. Notice the Configure your application section, the credentials from it will be used to configure the mail client. Setup Eclipse Dirigible Prerequisites You can follow the Setup guide on how to deploy Eclipse Dirigible locally or in the cloud. Add the following environment variables to your deployment: DIRIGIBLE_MAIL_USERNAME=apikey DIRIGIBLE_MAIL_PASSWORD=<YOUR_API_KEY_HERE> DIRIGIBLE_MAIL_TRANSPORT_PROTOCOL=smtps DIRIGIBLE_MAIL_SMTPS_HOST=smtp.sendgrid.net DIRIGIBLE_MAIL_SMTPS_PORT=465 Note Replace the <YOUR_API_KEY_HERE> placeholder with the SendGrid SMTP Relay API key. Restart the Eclipse Dirigible instance in order to apply the new environment variables. Environment Variables To get a complete list of all environment variables navigate to the Environment Variables page. Send an Email with SendGrid SMTP Relay Login to your Eclipse Dirigible instance. Create new project. Create new Javascript ESM Service . Use the following sinippet to send emails: import { client as mail } from \"@dirigible/mail\" ; const from = \"<YOUR_VERIFIED_SENDER_EMAIL_HERE>\" ; const to = \"<YOUR_RECIPIENT_EMAIL_HERE>\" ; const subject = \"Subject\" ; const content = \"Hello World!\" ; const subType = \"html\" ; mail . send ( from , to , subject , content , subType ); Note Replace the <YOUR_VERIFIED_SENDER_EMAIL_HERE> and the <YOUR_RECIPIENT_EMAIL_HERE> placeholders with valid email addresses.","title":"Connecting Eclipse Dirigible with SendGrid SMTP Relay"},{"location":"2022/09/12/sendgrid-smtp-relay-setup/#setup-sendgrid-account","text":"Create a SendGrid account at https://sendgrid.com . Login at https://app.sendgrid.com . Verify a single sender email: Click Settings \u2192 Sender Authentication \u2192 Verify a Single Sender . Enter the details of the email address that Eclipse Dirigible mails will be sent from. Setup SMTP Relay: Click Email API \u2192 Integration Guide \u2192 SMTP Relay . Enter an API Key Name and click Create Key to get an API Key for your SendGrid SMTP Relay. Notice the Configure your application section, the credentials from it will be used to configure the mail client.","title":"Setup SendGrid Account"},{"location":"2022/09/12/sendgrid-smtp-relay-setup/#setup-eclipse-dirigible","text":"Prerequisites You can follow the Setup guide on how to deploy Eclipse Dirigible locally or in the cloud. Add the following environment variables to your deployment: DIRIGIBLE_MAIL_USERNAME=apikey DIRIGIBLE_MAIL_PASSWORD=<YOUR_API_KEY_HERE> DIRIGIBLE_MAIL_TRANSPORT_PROTOCOL=smtps DIRIGIBLE_MAIL_SMTPS_HOST=smtp.sendgrid.net DIRIGIBLE_MAIL_SMTPS_PORT=465 Note Replace the <YOUR_API_KEY_HERE> placeholder with the SendGrid SMTP Relay API key. Restart the Eclipse Dirigible instance in order to apply the new environment variables. Environment Variables To get a complete list of all environment variables navigate to the Environment Variables page.","title":"Setup Eclipse Dirigible"},{"location":"2022/09/12/sendgrid-smtp-relay-setup/#send-an-email-with-sendgrid-smtp-relay","text":"Login to your Eclipse Dirigible instance. Create new project. Create new Javascript ESM Service . Use the following sinippet to send emails: import { client as mail } from \"@dirigible/mail\" ; const from = \"<YOUR_VERIFIED_SENDER_EMAIL_HERE>\" ; const to = \"<YOUR_RECIPIENT_EMAIL_HERE>\" ; const subject = \"Subject\" ; const content = \"Hello World!\" ; const subType = \"html\" ; mail . send ( from , to , subject , content , subType ); Note Replace the <YOUR_VERIFIED_SENDER_EMAIL_HERE> and the <YOUR_RECIPIENT_EMAIL_HERE> placeholders with valid email addresses.","title":"Send an Email with SendGrid SMTP Relay"},{"location":"2022/09/26/qldb-repository-api-guide/","text":"In this blog you will learn how to use Amazon Quantum Ledger Database within Eclipse Dirigible projects. What is QLDB? Amazon Quantum Ledger Database (QLDB) is a fully managed database running over a ledger that provides a transparent, immutable, and cryptographically verifiable transaction log. What is the QLDB Repository API and why is it useful for Eclipse Dirigible projects? The QLDB Repository API in Eclipse Dirigible adds a clean and low code JavaScript interface for work with AWS QLDB. It simplifies it's use and allows Eclipse Dirigible projects to use QLDB's secure transaction log to store many different kind of critical data. Such as, financial transactions or in supply chain systems to store transactions and details of every batch manufactured, shipped, stored, and sold from facility to store. See more use cases at AWS QLDB and AWS QLDB FAQs . What are the limitations when using QLDB? Although QLDB automatically scales to support the demands of your application and you don't need to worry about provisioning capacity or configuring read and write limits, there are some limits that you might want to read Quotas and limits in Amazon QLDB and Amazon QLDB endpoints and quotas . Setup AWS Account with QLDB Enabled Sign up for AWS or use an existing account. Create AWS IAM User To create AWS Identity and Access Management (IAM) user, sign in to the IAM console as the account owner by choosing Root user and entering your AWS account email address. On the next page, enter your password: In the navigation pane, choose Users and then choose Add users. For User name, enter dirigible_qldb_user . Select the check box next to AWS Management Console access. Then select Password - AWS Management Console access and then enter your new user password in the text box. (Optional) By default, AWS requires the new user to create a new password when first signing in. You can clear the check box next to User must create a new password at next sign-in to allow the new user to reset their password after they sign in. Choose Next: Permissions. Under Set permissions , choose Add user to group . Choose Create group . In the Create group dialog box, for Group name enter dirigible_qldb_group . Choose in the Filter policies input, search for the term qldb . Put checkboxes on AmazonQLDBReadOnly , AmazonQLDBFullAccess and AmazonQLDBConsoleFullAccess . Then click on Create group . Back in the list of groups, select the check box for your new group. Choose Refresh if necessary to see the group in the list. Choose Next: Tags . (Optional) Add metadata to the user by attaching tags as key-value pairs. For more information about using tags in IAM, see Tagging IAM entities in the IAM User Guide. Choose Next: Review to see the list of group memberships to be added to the new user. When you are ready to proceed, choose Create user . Note You can read more in the AWS documentation How to Create an IAM User . Get an IAM Access Key The Access Key is used by the QLDB Driver in Eclipse Dirigible to establish connections securely. Go back to the IAM console . In the navigation pane, choose Users . Click on the name of the user you created in the previous step. Choose the Security credentials tab. In the Access keys section, choose Create access key . To view the new access key pair, choose Show. You will not have access to the secret access key again after this dialog box closes. Copy and remember the Access key ID and Secret access key they will be used in the next step. Note Your credentials will look something like this: Access key ID: AKIAIOSFODNN7EXAMPLE Secret access key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY You can read more in the AWS documentation How to Get an IAM Access Key . Credentials and Region Setup Setup the credentials and region on the machine you are running Eclipse Dirigible. Note Create a file ~/.aws/credentials , where the tilde character (~) represents your home directory: [default] region = eu-west-2 aws_access_key_id = <your_access_key_id> aws_secret_access_key = <your_secret_access_key> Replace ( eu-west-2 , your_access_key_id , your_secret_access_key ) with your credentials from the last step. Setup a Ledger To create a ledger, sign in to the AWS Management Console, and open the Amazon QLDB console : In the navigation pane, choose Getting started . On the Create your first ledger card, choose Create Ledger. For Ledger information \u2013 The Ledger name should be pre-populated with vehicle-registration , change that to myTestLedger . For Permission mode - choose Standard \u2013 (Recommended) (A permissions mode that enables access control with finer granularity for ledgers, tables, and PartiQL commands.) For Encrypt data at rest \u2013 choose Use AWS owned KMS key (Use a KMS key that is owned and managed by AWS on your behalf. This is the default option and requires no additional setup.) Tags \u2013 (Optional) Add metadata to the ledger by attaching tags as key-value pairs. You can add tags to your ledger to help organize and identify them. For more information, see Tagging Amazon QLDB resources . Choose Create ledger . In the list of Ledgers, locate myTestLedger and wait for the ledger's status to become Active. Note You can read more in the AWS QLDB documentation How to Setup a Ledger . Build Custom Eclipse Dirigible stack with AWS QLDB support To create a Custom Stack follow the steps here Custom Stack documentation . After that replace the content of the releng/pom.xml file (described in the first step of the Custom Stack documentation ) with: <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\" > <modelVersion> 4.0.0 </modelVersion> <parent> <groupId> io.dirigible.custom.stack </groupId> <artifactId> custom-stack-parent </artifactId> <version> 1.0.0-SNAPSHOT </version> <relativePath> ../pom.xml </relativePath> </parent> <name> Custom Stack - Releng - Spring Boot </name> <artifactId> custom-stack-spring-boot </artifactId> <version> 1.0.0-SNAPSHOT </version> <packaging> jar </packaging> <build> <plugins> <plugin> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-maven-plugin </artifactId> <version> ${spring.boot.version} </version> <configuration> <mainClass> io.dirigible.custom.platform.CustomPlatformApplication </mainClass> </configuration> <executions> <execution> <goals> <goal> repackage </goal> </goals> </execution> </executions> </plugin> </plugins> <resources> <resource> <directory> src/main/resources </directory> <filtering> true </filtering> </resource> </resources> </build> <dependencies> <!-- Dirigible --> <dependency> <groupId> org.eclipse.dirigible </groupId> <artifactId> dirigible-server-spring </artifactId> <version> ${dirigible.version} </version> </dependency> <!-- Dirigible \u0415XT --> <dependency> <groupId> org.eclipse.dirigible </groupId> <artifactId> dirigible-ext </artifactId> <version> 7.0.0-SNAPSHOT </version> </dependency> <!-- Platform --> <dependency> <groupId> org.slf4j </groupId> <artifactId> slf4j-api </artifactId> <version> ${slf4j.version} </version> <scope> compile </scope> </dependency> <dependency> <groupId> ch.qos.logback </groupId> <artifactId> logback-core </artifactId> <version> ${logback.version} </version> <scope> compile </scope> </dependency> <dependency> <groupId> ch.qos.logback </groupId> <artifactId> logback-classic </artifactId> <version> ${logback.version} </version> <scope> compile </scope> </dependency> <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-configuration-processor </artifactId> <optional> true </optional> <version> ${spring.boot.version} </version> </dependency> </dependencies> </project> Create Eclipse Dirigible Project with AWS QLDB Create a new Project. Add a qldb-sample.js or qldb-sample.mjs file to the project. Add the following content with an example usage of the QLDBRepository API to the file: qldb-sample.js qldb-sample.mjs const QLDBRepository = require ( \"qldb/QLDBRepository\" ); // 1. Create a repository for the ledger 'myTestLedger' that works with a table 'tableName' const qldb = new QLDBRepository ( \"myTestLedger\" , \"tableName\" ); // 2. [OPTIONAL] Create the table as it doesn't exist in your ledger, // on the next execution of this script - comment out this line or it will fail. qldb . createTable (); // 3. Insert a JS Object as a record. let inserted = qldb . insert ({ email : \"test@mail.com\" , number : 123 , status : false }); // Notice: The inserted object now has a 'documentId' property, // which is the id of the record generated by QLDB console . log ( \"Inserted entry: \" + inserted ); // 4.1 Update a record inserted . email = \"q@mail.com\" ; inserted . number = 5 ; inserted . status = false ; let updated = qldb . update ( inserted ); console . log ( \"Updated entry: \" + updated ); // 4.2 [OPTIONAL] Update a record with an object // let updatedRaw = qldb.update({ // email: \"text@mail.com\", // number: 50000, // status: true, // documentId: \"7ekJBB1FEm1EmhJBqH0WLX\" // }); // Notice: Unlike insertion where the 'documentId' is generated by QLDB, // in 'update' the object must have a 'documentId' property defined // with value - a valid documentId of an entry in your table. // 5. Get all current records in the repository. let allRecords = qldb . getAll (); console . log ( \"allRecords: \" + allRecords ); // 6.1 Delete a record let deletedId = qldb . delete ( updated ); console . log ( \"Deleted entry with id: \" + deletedId ); // 6.2 [OPTIONAL] Delete a record by ID // deletedId = qldb.delete(updated.documentId); // console.log(\"Deleted entry with id: \" + deletedId); // 7. Get array with all transactions for the table let transactionHistory = qldb . getHistory (); console . log ( \"Transaction History:\" + transactionHistory ); // 8. [OPTIONAL] Drop the table // qldb.dropTable(); // Notice: In QLDB dropping a table simply inactivates it, // you can reactivate a table that you have dropped by running // an SQL UNDROP statement in PartiQL import { QLDBRepository } from \"@dirigible/qldb\" // 1. Create a repository for the ledger 'myTestLedger' that works with a table 'tableName' const qldb = new QLDBRepository ( \"myTestLedger\" , \"tableName\" ); // 2. [OPTIONAL] Create the table as it doesn't exist in your ledger, // on the next execution of this script - comment out this line or it will fail. qldb . createTable (); // 3. Insert a JS Object as a record. let inserted = qldb . insert ({ email : \"test@mail.com\" , number : 123 , status : false }); // Notice: The inserted object now has a 'documentId' property, // which is the id of the record generated by QLDB console . log ( \"Inserted entry: \" + inserted ); // 4.1 Update a record inserted . email = \"q@mail.com\" ; inserted . number = 5 ; inserted . status = false ; let updated = qldb . update ( inserted ); console . log ( \"Updated entry: \" + updated ); // 4.2 [OPTIONAL] Update a record with an object // let updatedRaw = qldb.update({ // email: \"text@mail.com\", // number: 50000, // status: true, // documentId: \"7ekJBB1FEm1EmhJBqH0WLX\" // }); // Notice: Unlike insertion where the 'documentId' is generated by QLDB, // in 'update' the object must have a 'documentId' property defined // with value - a valid documentId of an entry in your table. // 5. Get all current records in the repository. let allRecords = qldb . getAll (); console . log ( \"allRecords: \" + allRecords ); // 6.1 Delete a record let deletedId = qldb . delete ( updated ); console . log ( \"Deleted entry with id: \" + deletedId ); // 6.2 [OPTIONAL] Delete a record by ID // deletedId = qldb.delete(updated.documentId); // console.log(\"Deleted entry with id: \" + deletedId); // 7. Get array with all transactions for the table let transactionHistory = qldb . getHistory (); console . log ( \"Transaction History:\" + transactionHistory ); // 8. [OPTIONAL] Drop the table // qldb.dropTable(); // Notice: In QLDB dropping a table simply inactivates it, // you can reactivate a table that you have dropped by running // an SQL UNDROP statement in PartiQL (Optional) Manually Run PartiQL queries against your ledger Sign in to the AWS Management Console, and open the Amazon QLDB console . In the navigation pane choose PartiQL editor . In the editor's Choose a ledger dropdown select myTestLedger . You can now write and execute queries to your database manually. Note This can be useful if you want to create, delete or restore deleted tables. Read more at Getting started with PartiQL for DynamoDB .","title":"Amazon Quantum Ledger Database with Eclipse Dirigible"},{"location":"2022/09/26/qldb-repository-api-guide/#setup-aws-account-with-qldb-enabled","text":"Sign up for AWS or use an existing account.","title":"Setup AWS Account with QLDB Enabled"},{"location":"2022/09/26/qldb-repository-api-guide/#create-aws-iam-user","text":"To create AWS Identity and Access Management (IAM) user, sign in to the IAM console as the account owner by choosing Root user and entering your AWS account email address. On the next page, enter your password: In the navigation pane, choose Users and then choose Add users. For User name, enter dirigible_qldb_user . Select the check box next to AWS Management Console access. Then select Password - AWS Management Console access and then enter your new user password in the text box. (Optional) By default, AWS requires the new user to create a new password when first signing in. You can clear the check box next to User must create a new password at next sign-in to allow the new user to reset their password after they sign in. Choose Next: Permissions. Under Set permissions , choose Add user to group . Choose Create group . In the Create group dialog box, for Group name enter dirigible_qldb_group . Choose in the Filter policies input, search for the term qldb . Put checkboxes on AmazonQLDBReadOnly , AmazonQLDBFullAccess and AmazonQLDBConsoleFullAccess . Then click on Create group . Back in the list of groups, select the check box for your new group. Choose Refresh if necessary to see the group in the list. Choose Next: Tags . (Optional) Add metadata to the user by attaching tags as key-value pairs. For more information about using tags in IAM, see Tagging IAM entities in the IAM User Guide. Choose Next: Review to see the list of group memberships to be added to the new user. When you are ready to proceed, choose Create user . Note You can read more in the AWS documentation How to Create an IAM User .","title":"Create AWS IAM User"},{"location":"2022/09/26/qldb-repository-api-guide/#get-an-iam-access-key","text":"The Access Key is used by the QLDB Driver in Eclipse Dirigible to establish connections securely. Go back to the IAM console . In the navigation pane, choose Users . Click on the name of the user you created in the previous step. Choose the Security credentials tab. In the Access keys section, choose Create access key . To view the new access key pair, choose Show. You will not have access to the secret access key again after this dialog box closes. Copy and remember the Access key ID and Secret access key they will be used in the next step. Note Your credentials will look something like this: Access key ID: AKIAIOSFODNN7EXAMPLE Secret access key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY You can read more in the AWS documentation How to Get an IAM Access Key .","title":"Get an IAM Access Key"},{"location":"2022/09/26/qldb-repository-api-guide/#credentials-and-region-setup","text":"Setup the credentials and region on the machine you are running Eclipse Dirigible. Note Create a file ~/.aws/credentials , where the tilde character (~) represents your home directory: [default] region = eu-west-2 aws_access_key_id = <your_access_key_id> aws_secret_access_key = <your_secret_access_key> Replace ( eu-west-2 , your_access_key_id , your_secret_access_key ) with your credentials from the last step.","title":"Credentials and Region Setup"},{"location":"2022/09/26/qldb-repository-api-guide/#setup-a-ledger","text":"To create a ledger, sign in to the AWS Management Console, and open the Amazon QLDB console : In the navigation pane, choose Getting started . On the Create your first ledger card, choose Create Ledger. For Ledger information \u2013 The Ledger name should be pre-populated with vehicle-registration , change that to myTestLedger . For Permission mode - choose Standard \u2013 (Recommended) (A permissions mode that enables access control with finer granularity for ledgers, tables, and PartiQL commands.) For Encrypt data at rest \u2013 choose Use AWS owned KMS key (Use a KMS key that is owned and managed by AWS on your behalf. This is the default option and requires no additional setup.) Tags \u2013 (Optional) Add metadata to the ledger by attaching tags as key-value pairs. You can add tags to your ledger to help organize and identify them. For more information, see Tagging Amazon QLDB resources . Choose Create ledger . In the list of Ledgers, locate myTestLedger and wait for the ledger's status to become Active. Note You can read more in the AWS QLDB documentation How to Setup a Ledger .","title":"Setup a Ledger"},{"location":"2022/09/26/qldb-repository-api-guide/#build-custom-eclipse-dirigible-stack-with-aws-qldb-support","text":"To create a Custom Stack follow the steps here Custom Stack documentation . After that replace the content of the releng/pom.xml file (described in the first step of the Custom Stack documentation ) with: <project xmlns= \"http://maven.apache.org/POM/4.0.0\" xmlns:xsi= \"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation= \"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\" > <modelVersion> 4.0.0 </modelVersion> <parent> <groupId> io.dirigible.custom.stack </groupId> <artifactId> custom-stack-parent </artifactId> <version> 1.0.0-SNAPSHOT </version> <relativePath> ../pom.xml </relativePath> </parent> <name> Custom Stack - Releng - Spring Boot </name> <artifactId> custom-stack-spring-boot </artifactId> <version> 1.0.0-SNAPSHOT </version> <packaging> jar </packaging> <build> <plugins> <plugin> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-maven-plugin </artifactId> <version> ${spring.boot.version} </version> <configuration> <mainClass> io.dirigible.custom.platform.CustomPlatformApplication </mainClass> </configuration> <executions> <execution> <goals> <goal> repackage </goal> </goals> </execution> </executions> </plugin> </plugins> <resources> <resource> <directory> src/main/resources </directory> <filtering> true </filtering> </resource> </resources> </build> <dependencies> <!-- Dirigible --> <dependency> <groupId> org.eclipse.dirigible </groupId> <artifactId> dirigible-server-spring </artifactId> <version> ${dirigible.version} </version> </dependency> <!-- Dirigible \u0415XT --> <dependency> <groupId> org.eclipse.dirigible </groupId> <artifactId> dirigible-ext </artifactId> <version> 7.0.0-SNAPSHOT </version> </dependency> <!-- Platform --> <dependency> <groupId> org.slf4j </groupId> <artifactId> slf4j-api </artifactId> <version> ${slf4j.version} </version> <scope> compile </scope> </dependency> <dependency> <groupId> ch.qos.logback </groupId> <artifactId> logback-core </artifactId> <version> ${logback.version} </version> <scope> compile </scope> </dependency> <dependency> <groupId> ch.qos.logback </groupId> <artifactId> logback-classic </artifactId> <version> ${logback.version} </version> <scope> compile </scope> </dependency> <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-configuration-processor </artifactId> <optional> true </optional> <version> ${spring.boot.version} </version> </dependency> </dependencies> </project>","title":"Build Custom Eclipse Dirigible stack with AWS QLDB support"},{"location":"2022/09/26/qldb-repository-api-guide/#create-eclipse-dirigible-project-with-aws-qldb","text":"Create a new Project. Add a qldb-sample.js or qldb-sample.mjs file to the project. Add the following content with an example usage of the QLDBRepository API to the file: qldb-sample.js qldb-sample.mjs const QLDBRepository = require ( \"qldb/QLDBRepository\" ); // 1. Create a repository for the ledger 'myTestLedger' that works with a table 'tableName' const qldb = new QLDBRepository ( \"myTestLedger\" , \"tableName\" ); // 2. [OPTIONAL] Create the table as it doesn't exist in your ledger, // on the next execution of this script - comment out this line or it will fail. qldb . createTable (); // 3. Insert a JS Object as a record. let inserted = qldb . insert ({ email : \"test@mail.com\" , number : 123 , status : false }); // Notice: The inserted object now has a 'documentId' property, // which is the id of the record generated by QLDB console . log ( \"Inserted entry: \" + inserted ); // 4.1 Update a record inserted . email = \"q@mail.com\" ; inserted . number = 5 ; inserted . status = false ; let updated = qldb . update ( inserted ); console . log ( \"Updated entry: \" + updated ); // 4.2 [OPTIONAL] Update a record with an object // let updatedRaw = qldb.update({ // email: \"text@mail.com\", // number: 50000, // status: true, // documentId: \"7ekJBB1FEm1EmhJBqH0WLX\" // }); // Notice: Unlike insertion where the 'documentId' is generated by QLDB, // in 'update' the object must have a 'documentId' property defined // with value - a valid documentId of an entry in your table. // 5. Get all current records in the repository. let allRecords = qldb . getAll (); console . log ( \"allRecords: \" + allRecords ); // 6.1 Delete a record let deletedId = qldb . delete ( updated ); console . log ( \"Deleted entry with id: \" + deletedId ); // 6.2 [OPTIONAL] Delete a record by ID // deletedId = qldb.delete(updated.documentId); // console.log(\"Deleted entry with id: \" + deletedId); // 7. Get array with all transactions for the table let transactionHistory = qldb . getHistory (); console . log ( \"Transaction History:\" + transactionHistory ); // 8. [OPTIONAL] Drop the table // qldb.dropTable(); // Notice: In QLDB dropping a table simply inactivates it, // you can reactivate a table that you have dropped by running // an SQL UNDROP statement in PartiQL import { QLDBRepository } from \"@dirigible/qldb\" // 1. Create a repository for the ledger 'myTestLedger' that works with a table 'tableName' const qldb = new QLDBRepository ( \"myTestLedger\" , \"tableName\" ); // 2. [OPTIONAL] Create the table as it doesn't exist in your ledger, // on the next execution of this script - comment out this line or it will fail. qldb . createTable (); // 3. Insert a JS Object as a record. let inserted = qldb . insert ({ email : \"test@mail.com\" , number : 123 , status : false }); // Notice: The inserted object now has a 'documentId' property, // which is the id of the record generated by QLDB console . log ( \"Inserted entry: \" + inserted ); // 4.1 Update a record inserted . email = \"q@mail.com\" ; inserted . number = 5 ; inserted . status = false ; let updated = qldb . update ( inserted ); console . log ( \"Updated entry: \" + updated ); // 4.2 [OPTIONAL] Update a record with an object // let updatedRaw = qldb.update({ // email: \"text@mail.com\", // number: 50000, // status: true, // documentId: \"7ekJBB1FEm1EmhJBqH0WLX\" // }); // Notice: Unlike insertion where the 'documentId' is generated by QLDB, // in 'update' the object must have a 'documentId' property defined // with value - a valid documentId of an entry in your table. // 5. Get all current records in the repository. let allRecords = qldb . getAll (); console . log ( \"allRecords: \" + allRecords ); // 6.1 Delete a record let deletedId = qldb . delete ( updated ); console . log ( \"Deleted entry with id: \" + deletedId ); // 6.2 [OPTIONAL] Delete a record by ID // deletedId = qldb.delete(updated.documentId); // console.log(\"Deleted entry with id: \" + deletedId); // 7. Get array with all transactions for the table let transactionHistory = qldb . getHistory (); console . log ( \"Transaction History:\" + transactionHistory ); // 8. [OPTIONAL] Drop the table // qldb.dropTable(); // Notice: In QLDB dropping a table simply inactivates it, // you can reactivate a table that you have dropped by running // an SQL UNDROP statement in PartiQL","title":"Create Eclipse Dirigible Project with AWS QLDB"},{"location":"2022/09/26/qldb-repository-api-guide/#optional-manually-run-partiql-queries-against-your-ledger","text":"Sign in to the AWS Management Console, and open the Amazon QLDB console . In the navigation pane choose PartiQL editor . In the editor's Choose a ledger dropdown select myTestLedger . You can now write and execute queries to your database manually. Note This can be useful if you want to create, delete or restore deleted tables. Read more at Getting started with PartiQL for DynamoDB .","title":"(Optional) Manually Run PartiQL queries against your ledger"},{"location":"2023/01/16/http-decorators/","text":"Eclipse Dirigible supports JavaScript decorators? Although decorators support is something new in Dirigible yet to be documented, we have plans to use them more and more. You may ask, how did we add support for them? Well, the secret ingredient is that internally GraalJS is used for executing JavaScript. If you have read my blog post about decorators you should have a basic idea of how we've done it. What do we use JavaScript decorators for? For some time now, we have been thinking of an easier-to-code and easier-to-read solution for writing REST APIs. The current way of writing RESTful services is via the rs module . At a first glance, this is pretty similar to the plain old NodeJS Express. We have a router and we define some routes: import { rs } from \"@dirigible/http\" // or if you use the CJS modules support // const rs = require(\"http/v4/rs\"); rs . service () . get ( \"/hello\" , ( ctx , req , res ) => res . println ( \"Hello there!\" );) . execute (); This approach has been working well but we've been hearing more and more about people wanting to write APIs in a way similar to Java Spring's and NestJS's annotation/decorator definitions. When the new GraalJS version supporting decorators was released, we decided to create a PoC for decorator-based APIs. The first thing we tried out was something like the following: import { Controller , Get } from \"@dirigible/http\" // or if you use the CJS modules support // const { Controller, Get } = require(\"http/v4/rs/decorators\"); @ Controller class MyApi { @ Get ( \"/test\" ) onGet ( req , res , ctx ) { res . println ( \"Hello there!\" ); } } While this was good enough for a PoC, it was still a bit more verbose than necessary for simple APIs. From what we have seen, people most often write an API that receives some request data and respond with some other data. So, we changed our design to the following: import { Controller , Get , Post } from \"@dirigible/http\" // or if you use the CJS modules support // const { Controller, Get, Post } = require(\"http/v4/rs/decorators\"); @ Controller class MyApi { @ Get ( \"/test\" ) onGet () { return \"Hello there!\" ; } @ Post ( \"/test\" ) onPost ( body ) { return { some : \"data\" } } } This change made writing simple APIs a lot easier - you just receive some data, and return some data. But what if you need to read some query parameters? Or headers? Or something that we have still not added support via just decorators? Well, you could still write your request handler like this: @ Post ( \"/test/:id\" ) onPost ( body , ctx ) { const id = ctx . req . params . id ; return { some : \"data\" , id : id } } By using the ctx argument of the request handler you can access the underlying request with ctx.req or the response object with ctx.res . Wrap up Whether using decorators for declaring REST APIs is the best way, of course, is debatable. Some people like using decorators, and some people like defining routes like in Express. Personally, I believe both solutions have their pros and cons and it's the developer's responsibility to choose the best approach for a given REST API. If you want to see all this for yourself, go ahead and try it out at Dirigible Trial .","title":"JavaScript Decorators in Eclipse Dirigible"},{"location":"2023/01/16/http-decorators/#eclipse-dirigible-supports-javascript-decorators","text":"Although decorators support is something new in Dirigible yet to be documented, we have plans to use them more and more. You may ask, how did we add support for them? Well, the secret ingredient is that internally GraalJS is used for executing JavaScript. If you have read my blog post about decorators you should have a basic idea of how we've done it.","title":"Eclipse Dirigible supports JavaScript decorators?"},{"location":"2023/01/16/http-decorators/#what-do-we-use-javascript-decorators-for","text":"For some time now, we have been thinking of an easier-to-code and easier-to-read solution for writing REST APIs. The current way of writing RESTful services is via the rs module . At a first glance, this is pretty similar to the plain old NodeJS Express. We have a router and we define some routes: import { rs } from \"@dirigible/http\" // or if you use the CJS modules support // const rs = require(\"http/v4/rs\"); rs . service () . get ( \"/hello\" , ( ctx , req , res ) => res . println ( \"Hello there!\" );) . execute (); This approach has been working well but we've been hearing more and more about people wanting to write APIs in a way similar to Java Spring's and NestJS's annotation/decorator definitions. When the new GraalJS version supporting decorators was released, we decided to create a PoC for decorator-based APIs. The first thing we tried out was something like the following: import { Controller , Get } from \"@dirigible/http\" // or if you use the CJS modules support // const { Controller, Get } = require(\"http/v4/rs/decorators\"); @ Controller class MyApi { @ Get ( \"/test\" ) onGet ( req , res , ctx ) { res . println ( \"Hello there!\" ); } } While this was good enough for a PoC, it was still a bit more verbose than necessary for simple APIs. From what we have seen, people most often write an API that receives some request data and respond with some other data. So, we changed our design to the following: import { Controller , Get , Post } from \"@dirigible/http\" // or if you use the CJS modules support // const { Controller, Get, Post } = require(\"http/v4/rs/decorators\"); @ Controller class MyApi { @ Get ( \"/test\" ) onGet () { return \"Hello there!\" ; } @ Post ( \"/test\" ) onPost ( body ) { return { some : \"data\" } } } This change made writing simple APIs a lot easier - you just receive some data, and return some data. But what if you need to read some query parameters? Or headers? Or something that we have still not added support via just decorators? Well, you could still write your request handler like this: @ Post ( \"/test/:id\" ) onPost ( body , ctx ) { const id = ctx . req . params . id ; return { some : \"data\" , id : id } } By using the ctx argument of the request handler you can access the underlying request with ctx.req or the response object with ctx.res .","title":"What do we use JavaScript decorators for?"},{"location":"2023/01/16/http-decorators/#wrap-up","text":"Whether using decorators for declaring REST APIs is the best way, of course, is debatable. Some people like using decorators, and some people like defining routes like in Express. Personally, I believe both solutions have their pros and cons and it's the developer's responsibility to choose the best approach for a given REST API. If you want to see all this for yourself, go ahead and try it out at Dirigible Trial .","title":"Wrap up"}]}